{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0+cu117)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (6.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement rogue-score (from versions: none)\n",
      "ERROR: No matching distribution found for rogue-score\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch psutil tqdm numpy rogue-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Running on: CUDA\n",
      "Hybrid model directory: ../../Hybrid_Dense_Layer_Freeze\n",
      "Test data path: ../../test.json\n",
      "Running performance test on 50 samples\n",
      "\n",
      "================================================================================\n",
      "RUNNING BENCHMARK ON CPU\n",
      "================================================================================\n",
      "\n",
      "Loading models on CPU...\n",
      "Loading from: ../../Hybrid_Dense_Layer_Freeze\\label_encoders.json\n",
      "Initializing model...\n",
      "All GPT-2 and DistilBERT layers remain frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load generation model from ../../text_generation_results_03-09-25/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   2%|▏         | 1/50 [00:17<14:06, 17.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1/50 (Device: CPU):\n",
      "Instruction: i do not know what i have to do to get my invoice #85632\n",
      "New Input: i do not know what i have to do to get my invoice #85632 [Classified: Intent is 'get_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about the steps to take in order to access your invoice with the number #{{client last name}}. let me guide you through the process. first, please log in to your account on our website. once logged in, navigate to the \"billing\" or \"invoices\" section. there, you should be able to find a list of your invoices. look for a tab or link that says \"view invoice.\" click on that tab, and you'll be directed to a page where you can view and download the invoice. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team. they will be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.6074                    | 16.6736                   | 17.2810                   | CPU        | 17.2461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   4%|▍         | 2/50 [00:28<11:09, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 2/50 (Device: CPU):\n",
      "Instruction: transfer to live agent\n",
      "New Input: transfer to live agent [Classified: Intent is 'switch_account', Category is 'contact', Entities are live (account_type)]\n",
      "Generated Response: i'm here to assist you in transferring to our {{account type}} agent. i understand that you would like to switch to a different account type, and i'll guide you through the process step by step. to get started, could you please provide me with your account details, such as your username or email address? once i have this information, i will be able to initiate the transfer and ensure a seamless experience for you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.3696                    | 11.2426                   | 11.6122                   | CPU        | 12.4062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   6%|▌         | 3/50 [00:39<09:54, 12.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 3/50 (Device: CPU):\n",
      "Instruction: i need help to see how long refunds take\n",
      "New Input: i need help to see how long refunds take [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the timeframe for refunds. the duration for processing refunds can vary depending on several factors, such as the payment method used and the specific policies of the vendor or service provider. to provide you with accurate information, could you please provide me with your order number or any other relevant details? this will enable me to check the status of your refund and give you an estimated timeframe. thank you for your cooperation, and i appreciate your patience while i gather the necessary information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4706                    | 10.6291                   | 11.0998                   | CPU        | 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   8%|▊         | 4/50 [00:50<09:02, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 4/50 (Device: CPU):\n",
      "Instruction: help to pseak to customer support\n",
      "New Input: help to pseak to customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are customer (account_type)]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to know how to reach our dedicated customer assistance team. to get in touch with us, you can reach out to us through various channels such as phone, email, or live chat on our website at {{website url}}. our team is ready to assist you with any questions or concerns you may have. feel free to let me know if there's anything else i can do to help you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4335                    | 10.0346                   | 10.4681                   | CPU        | 0.5391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  10%|█         | 5/50 [01:01<08:30, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 5/50 (Device: CPU):\n",
      "Instruction: i do not know what to do to swtch to the premium account\n",
      "New Input: i do not know what to do to swtch to the premium account [Classified: Intent is 'switch_account', Category is 'account', Entities are premium (account_type)]\n",
      "Generated Response: i appreciate your interest in switching to our {{account type}} account, and i'm here to guide you through the process. to initiate the switch, could you please provide me with your account details, such as your username or email address? once i have this information, i'll be able to assist you further and ensure a smooth transition. rest assured, your security and privacy are of utmost importance to us, so your information will be handled with the utmost care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4201                    | 10.1503                   | 10.5704                   | CPU        | 4.3594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  12%|█▏        | 6/50 [01:25<11:41, 15.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 6/50 (Device: CPU):\n",
      "Instruction: i need assistance resetting the key of my user account\n",
      "New Input: i need assistance resetting the key of my user account [Classified: Intent is 'recover_password', Category is 'account', Entities are user (account_type)]\n",
      "Generated Response: assuredly! i'm here to provide you with the assistance you need to reset your key and regain access to your account. let's work together to resolve this issue and ensure a seamless experience. here's what you can do: 1. begin by visiting our platform's \"{{login page url}}\" to initiate the password reset process step by step. it's a simple and user-friendly interface that allows you to enter your registered email address and password to access your user profile. 2. once you're logged in, navigate to the account settings or profile section, where you should find an option to update your password. click on that option and follow the on-screen instructions to complete the necessary steps. you'll be prompted to verify your identity through a security question or provide any additional information that may assist you in verifying your access key. please provide as much detail as possible so that i can guide you through the process accurately and efficiently. your security is of utmost importance to us, and we'll do everything we can to address your request promptly and effectively.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.3958                    | 24.4780                   | 24.8738                   | CPU        | 1.9766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  14%|█▍        | 7/50 [01:36<10:12, 14.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 7/50 (Device: CPU):\n",
      "Instruction: i want help taking a quick look at the bills from john robertson\n",
      "New Input: i want help taking a quick look at the bills from john robertson [Classified: Intent is 'check_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in quickly reviewing the invoices from {{salutation}} {{client last name}}. to provide you with the most accurate and up-to-date information, could you please provide me with your account number or any other relevant details? once i have these details, i'll be able to guide you through the process step by step. your satisfaction is our top priority, and we're committed to resolving any questions or concerns you may have.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4023                    | 10.3096                   | 10.7119                   | CPU        | 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  16%|█▌        | 8/50 [01:46<09:05, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 8/50 (Device: CPU):\n",
      "Instruction: i can't enter a delivery address\n",
      "New Input: i can't enter a delivery address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're having trouble entering a shipping address. let's work together to resolve this issue. could you please provide more details about the specific problem you are facing? are you encountering any error messages or is the system not accepting the address format? once i have a better understanding of the issue, i'll be able to assist you further in resolving it.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4216                    | 9.9044                   | 10.3260                   | CPU        | 0.5469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  18%|█▊        | 9/50 [01:55<07:52, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 9/50 (Device: CPU):\n",
      "Instruction: can you help me place an order?\n",
      "New Input: can you help me place an order? [Classified: Intent is 'delivery_options', Category is 'delivery', Entities are none]\n",
      "Generated Response: i'll take care of it! i'm here to assist you in placing your order. could you please provide me with the details of the items you would like to purchase? once i have the necessary information, i will guide you through the ordering process step by step. your satisfaction is our top priority, and we want to ensure a seamless experience for you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.3860                    | 7.9032                   | 8.2893                   | CPU        | 1.8320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  20%|██        | 10/50 [02:14<09:16, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 10/50 (Device: CPU):\n",
      "Instruction: i do not know how i can set up my shipping address\n",
      "New Input: i do not know how i can set up my shipping address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about the process of setting up your delivery address. allow me to guide you through it step by step: 1. log in to your account on our website or app by navigating to the \"my account\" or \"profile\" section, where you should find an option to manage your shipping addresses. you can usually find this option by clicking on your username or email address associated with the address you'd like to add to a shipping profile. NOTE: if you encounter any difficulties or have further questions, please don't hesitate to reach out to our customer support team for assistance. they are available {{customer support hours}} and will be more than happy to help you with any questions or concerns you may have. remember, your satisfaction is our top priority, and we're here to support you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4727                    | 18.7832                   | 19.2559                   | CPU        | 1.2891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  22%|██▏       | 11/50 [02:38<10:57, 16.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 11/50 (Device: CPU):\n",
      "Instruction: i need assistance to set my new delovery address up\n",
      "New Input: i need assistance to set my new delovery address up [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in setting up your new delivery address. to ensure a smooth process, please follow these steps: 1. log in to your account on our website/app using your username and password to access the account settings. 2. once you're logged in, navigate to the \"my account\" or \"profile\" section, where you should find an option to manage your shipping addresses. 3. click on that option, and you'll be presented with a list of your delivery addresses, including {{delivery address}}. 4. fill in the required fields with the necessary information, such as street name, city, state, postal code, contact details, or any other relevant details. 5. double-check the accuracy of the information provided, ensuring that all fields are filled correctly. if you encounter any issues or have further questions, feel free to reach out to our customer support team for further assistance. they will be more than happy to guide you through the process and address any concerns you may have.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4076                    | 23.1176                   | 23.5252                   | CPU        | 1.2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  24%|██▍       | 12/50 [02:45<08:57, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 12/50 (Device: CPU):\n",
      "Instruction: help me to check what payment methods are available\n",
      "New Input: help me to check what payment methods are available [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking our available payment options. to provide you with accurate information, could you please provide me with your full name, email address, and a preferred payment method? once i have these details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4394                    | 7.5257                   | 7.9650                   | CPU        | 1.8906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  26%|██▌       | 13/50 [02:53<07:23, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 13/50 (Device: CPU):\n",
      "Instruction: i call to leave my feedback for ur services\n",
      "New Input: i call to leave my feedback for ur services [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: thank you for taking the time to share your feedback about our services! your insights are incredibly valuable to us as they help us understand how we can better serve our customers. please let me know if you have any specific questions or if there's anything else i can assist you with.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4241                    | 6.6117                   | 7.0357                   | CPU        | 2.6484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  28%|██▊       | 14/50 [03:03<06:53, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 14/50 (Device: CPU):\n",
      "Instruction: open freemium account for wife\n",
      "New Input: open freemium account for wife [Classified: Intent is 'create_account', Category is 'account', Entities are ium (account_type), wife (account_type)]\n",
      "Generated Response: thank you for expressing your interest in opening a {{account type}} account with us! i'm here to assist you every step of the way. to get started, could you please provide me with your wife's full name, email address, and a preferred username? once i have these details, i'll guide you through the account creation process and ensure a seamless experience for both of you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4234                    | 9.8832                   | 10.3066                   | CPU        | 2.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  30%|███       | 15/50 [03:13<06:32, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 15/50 (Device: CPU):\n",
      "Instruction: i am trying to see the status of order ord-1135283\n",
      "New Input: i am trying to see the status of order ord-1135283 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-1135283 (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the current status and estimated time of arrival (eta) for your order with the order number {{order number}}. to provide you with accurate information, could you please provide me with your full name, email address, and a preferred username? once i have these details, i'll be able to guide you through the process step by step, ensuring you have a seamless experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4007                    | 10.1836                   | 10.5843                   | CPU        | 4.5273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  32%|███▏      | 16/50 [03:31<07:29, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 16/50 (Device: CPU):\n",
      "Instruction: wanna see ur payment modalities will yahelp me\n",
      "New Input: wanna see ur payment modalities will yahelp me [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking our available payment methods. we strive to provide a seamless and convenient payment experience for our valued customers. here is a comprehensive list of our accepted payment options: 1. credit/debit card: we accept major credit and debit cards such as visa, mastercard, and american express. you can securely make payments using your credit or debit card details to complete your purchase. our platform also supports popular online payment platforms like paypal, bank transfer, apple pay, or google wallet, allowing you to make secure and hassle-free transactions with your money right at your fingertips. if you have any specific questions or need further assistance, please don't hesitate to let me know. i'll be more than happy to help you every step of the way!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.3997                    | 17.4980                   | 17.8977                   | CPU        | 4.5273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  34%|███▍      | 17/50 [03:45<07:16, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 17/50 (Device: CPU):\n",
      "Instruction: what do i ned to do to change to the real estate account?\n",
      "New Input: what do i ned to do to change to the real estate account? [Classified: Intent is 'switch_account', Category is 'account', Entities are real estate (account_category)]\n",
      "Generated Response: i appreciate your interest in upgrading to our {{account category}} account. let me guide you through the process step by step. first, log in to your account on our website or app. once you're logged in, locate the account settings or profile section. look for an option that says \"upgrade\" or \"switch account type.\" click on that option, and you'll be directed to a page where you can make the necessary changes. if you encounter any difficulties or have further questions, please don't hesitate to reach out. i'm here to assist you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4011                    | 12.8633                   | 13.2643                   | CPU        | 12.1406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  36%|███▌      | 18/50 [03:53<06:18, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 18/50 (Device: CPU):\n",
      "Instruction: notify of errorw with online registration\n",
      "New Input: notify of errorw with online registration [Classified: Intent is 'registration_problems', Category is 'account', Entities are none]\n",
      "Generated Response: we understand that encountering errors during the registration process can be frustrating. rest assured, we are here to assist you in resolving this issue. could you please provide more details about the specific error you are facing? this will help us understand the situation better and provide you with the most appropriate solution. thank you for bringing this to our attention, and we appreciate your patience as we work towards a resolution.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4213                    | 8.1756                   | 8.5968                   | CPU        | 2.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  38%|███▊      | 19/50 [04:26<09:19, 18.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 19/50 (Device: CPU):\n",
      "Instruction: another standard accounmt for father\n",
      "New Input: another standard accounmt for father [Classified: Intent is 'switch_account', Category is 'account', Entities are standard (account_type), acc (account_type), father (delivery_city)]\n",
      "Generated Response: assuredly! i'm here to assist you in switching to another {{account type}} account for your father. allow me to guide you through the process step by step: 1. access your account: begin by logging into our platform using your existing credentials. this will allow you to switch seamlessly to the new account type. 2. account settings: once you're logged in, navigate to your '{{settings}}' section. from there, you should be able to find an option to manage your personal information, such as your name, email address, phone number, or any other details associated with your dad. 3. locate the {{profile}}: look for a tab or link that says \"{{account change}}\" or something similar. click on it, and you'll be directed to a page where you can update your address or contact information. 4. select {{upgrade account}} from the available account options: choose the option that best suits your needs and provide the necessary details to complete the switch. if you encounter any difficulties or have further questions, feel free to reach out to our dedicated customer support team. they will be more than happy to help you with any questions or concerns you may have. please don't hesitate to let me know if there's anything specific you need assistance with. together, we'll make this transition smooth and hassle-free for you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4061                    | 32.0798                   | 32.4859                   | CPU        | 4.5547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  40%|████      | 20/50 [04:36<07:55, 15.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 20/50 (Device: CPU):\n",
      "Instruction: i need infrmation about the cancellation of my enterprise account\n",
      "New Input: i need infrmation about the cancellation of my enterprise account [Classified: Intent is 'delete_account', Category is 'account', Entities are enterprise (account_type)]\n",
      "Generated Response: i'm here to assist you with canceling your {{account type}} account. your satisfaction is our top priority, and we want to ensure that you have all the information you need to make a successful cancellation. to get started, could you please provide me with your account details, such as your username or email address? with this information, i'll be able to guide you through the process step by step, ensuring a smooth and hassle-free cancellation experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.3917                    | 10.3552                   | 10.7469                   | CPU        | 1.2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  42%|████▏     | 21/50 [04:58<08:26, 17.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 21/50 (Device: CPU):\n",
      "Instruction: i want help ordering a few of your product\n",
      "New Input: i want help ordering a few of your product [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for your interest in ordering our products! i'm here to assist you with the ordering process. to get started, could you please provide me with more details about the specific products you would like to order? this will help me guide you through the necessary steps and ensure a seamless experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 2.0473                    | 19.2018                   | 21.2491                   | CPU        | 1.2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  44%|████▍     | 22/50 [05:37<11:13, 24.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 22/50 (Device: CPU):\n",
      "Instruction: checking status of order ord-6772250\n",
      "New Input: checking status of order ord-6772250 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-6772250 (order_number)]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to know if you have any information regarding your order with the order number {{order number}}. to provide you with accurate and up-to-date information, i kindly request you to visit our website and navigate to the \"order status\" or \"track order\" section. there, you will be able to view the current status and estimated time of arrival (eta) for your purchase. should you encounter any difficulties or have further questions, please don't hesitate to let me know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 1.7449                    | 37.6612                   | 39.4061                   | CPU        | 1.2773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  46%|████▌     | 23/50 [06:04<11:13, 24.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 23/50 (Device: CPU):\n",
      "Instruction: deleting government account\n",
      "New Input: deleting government account [Classified: Intent is 'delete_account', Category is 'account', Entities are government (account_type)]\n",
      "Generated Response: i understand that you would like to delete your {{account type}} account. i apologize for any inconvenience this may have caused you. to assist you further, could you please provide me with your account details, such as your username or email address, so that i can locate the account and guide you through the deletion process? rest assured, i'm here to support you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 1.6351                    | 25.3620                   | 26.9971                   | CPU        | 10.1523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  48%|████▊     | 24/50 [06:31<11:06, 25.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 24/50 (Device: CPU):\n",
      "Instruction: my experience with your brand was bad, make a consumer claim\n",
      "New Input: my experience with your brand was bad, make a consumer claim [Classified: Intent is 'complaint', Category is 'feedback', Entities are consumer (account_type)]\n",
      "Generated Response: i apologize for any negative experience you may have had with our brand. your feedback is important to us, and we want to ensure that your concerns are addressed appropriately. to assist you further, could you please provide me with more details about the specific issue you encountered? this will enable us to investigate the matter thoroughly and take the necessary steps to resolve it. thank you for bringing this to our attention.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 2.0526                    | 25.2037                   | 27.2564                   | CPU        | 1.2852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  50%|█████     | 25/50 [07:06<11:45, 28.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 25/50 (Device: CPU):\n",
      "Instruction: wann locate order ord-3779640\n",
      "New Input: wann locate order ord-3779640 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-3779640 (order_number)]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to locate your order with the order number {{order number}}. to assist you further, could you please provide me with some additional information? specifically, i would need to know the platform or website where you made the purchase. this will allow me to provide you with accurate and up-to-date information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 1.6724                    | 32.5128                   | 34.1852                   | CPU        | 1.2812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  52%|█████▏    | 26/50 [08:06<15:12, 38.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 26/50 (Device: CPU):\n",
      "Instruction: i have to sde in what cases can i request a refund\n",
      "New Input: i have to sde in what cases can i request a refund [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to provide you with the information you need regarding the situations in which you can request refunds. here are some common scenarios where you may be eligible for a reimbursement: 1. **product/service defect:** if the product or service you received has a defect or does not meet the description provided, you usually have grounds to request compensation. REMEMBER, your satisfaction is of utmost importance to us, and we want to ensure that you have a seamless experience with our products and services, regardless of the quality or quality of your purchase. if you encounter any difficulties or have further questions, please don't hesitate to reach out to our customer support team. they will be more than happy to assist you in resolving the issue.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 1.8603                    | 59.0411                   | 60.9014                   | CPU        | 1.2812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  54%|█████▍    | 27/50 [08:37<13:39, 35.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 27/50 (Device: CPU):\n",
      "Instruction: i'm trying to see at what time i can call customer support\n",
      "New Input: i'm trying to see at what time i can call customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are none]\n",
      "Generated Response: we're here to help! i take note that you would like to know the operating hours during which you can reach our dedicated customer assistance team. our team is available to assist you during our business hours, which can be found on our website at {{website url}}. if you have any other questions or concerns, feel free to let me know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 1.7384                    | 28.3597                   | 30.0982                   | CPU        | 11.1914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  56%|█████▌    | 28/50 [08:53<11:00, 30.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 28/50 (Device: CPU):\n",
      "Instruction: assistance sending some feedback for ur proiducts\n",
      "New Input: assistance sending some feedback for ur proiducts [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: i'll take care of it! i'm here to assist you in sending your feedback about our proids. your opinion matters to us, and we appreciate your willingness to share your thoughts. please let me know if you have any specific questions or if there's anything else i can help you with.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 1.8342                    | 15.1163                   | 16.9505                   | CPU        | 1.2773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  58%|█████▊    | 29/50 [09:03<08:22, 23.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 29/50 (Device: CPU):\n",
      "Instruction: need assistance to cancel order ord-8227420\n",
      "New Input: need assistance to cancel order ord-8227420 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8227420 (order_number)]\n",
      "Generated Response: i'm here to assist you with canceling your order with the order number {{order number}}. rest assured, i'll guide you through the process step by step to ensure a smooth cancellation experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.3884                    | 9.2368                   | 9.6252                   | CPU        | 1.2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  60%|██████    | 30/50 [09:20<07:14, 21.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 30/50 (Device: CPU):\n",
      "Instruction: need assistance cancelinv purchase ord-8042947\n",
      "New Input: need assistance cancelinv purchase ord-8042947 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8042947 (order_number)]\n",
      "Generated Response: i'm here to assist you with canceling your purchase with the order number {{order number}}. your satisfaction is our top priority, and we want to ensure a seamless cancellation process for you. here's what you need to do:\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.3943                    | 16.1670                   | 16.5613                   | CPU        | 1.2773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  62%|██████▏   | 31/50 [09:33<06:07, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 31/50 (Device: CPU):\n",
      "Instruction: i don't know how i can download my bill #12588\n",
      "New Input: i don't know how i can download my bill #12588 [Classified: Intent is 'get_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about how to download your bill with the number #{{client last name}}. let me guide you through the process step by step. first, please log in to your account on our website. once logged in, navigate to the \"billing\" or \"invoices\" section. there, you should be able to find a list of your invoices. look for a download or print option, and click on it. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4098                    | 13.3653                   | 13.7750                   | CPU        | 1.2656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  64%|██████▍   | 32/50 [09:42<04:49, 16.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 32/50 (Device: CPU):\n",
      "Instruction: i need help to see purchase ord-0521414 status\n",
      "New Input: i need help to see purchase ord-0521414 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-0521414 status (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status of your purchase with the purchase number {{order number}}. to view the current status, please visit the '{{order status}}' section on our website. if you have any other questions or need further assistance, feel free to let me know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4215                    | 8.0529                   | 8.4744                   | CPU        | 1.2695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  66%|██████▌   | 33/50 [09:51<03:57, 13.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 33/50 (Device: CPU):\n",
      "Instruction: am i entitled to a refund according to your policy?\n",
      "New Input: am i entitled to a refund according to your policy? [Classified: Intent is 'get_refund', Category is 'refund', Entities are none]\n",
      "Generated Response: i'll take care of it! i understand that you would like to inquire about the status of your refund. rest assured, i'm here to assist you every step of the way. to provide you with the most accurate information, could you please provide me with your order number or any other relevant details? this will enable me to guide you through the refund process and ensure that your concerns are addressed promptly and effectively.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4093                    | 8.6600                   | 9.0692                   | CPU        | 1.2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  68%|██████▊   | 34/50 [10:14<04:27, 16.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 34/50 (Device: CPU):\n",
      "Instruction: need to check what methods for delivery i can choose help me\n",
      "New Input: need to check what methods for delivery i can choose help me [Classified: Intent is 'delivery_options', Category is 'delivery', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in exploring the various delivery methods available to you. let's dive right in: 1. standard shipping: this method typically takes {{date range}} business days for your package to arrive at your doorstep. it's perfect for non-urgent items when you have a bit more time to spare before you receive your desired items. if you need your items sooner, you can opt for expedited shipping, which typically arrives within {{customer support hours}}. 2. in-store pickup: if time is of the essence, this option allows you to conveniently place your order online and have it delivered right to your door. don't hesitate to reach out to our customer support team, and they'll be more than happy to guide you through the process step by step. 3. overnight delivery: for those urgent needs, we offer next business day delivery, ensuring a swift and hassle-free delivery experience. whether you're in a hurry or need immediate assistance, our team is ready to provide you with the support you deserve.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4291                    | 22.7864                   | 23.2155                   | CPU        | 4.5117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  70%|███████   | 35/50 [10:25<03:43, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 35/50 (Device: CPU):\n",
      "Instruction: ya have an address to submit some feednack\n",
      "New Input: ya have an address to submit some feednack [Classified: Intent is 'review', Category is 'shipping', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in submitting your feedback about our products and services. to provide you with the most accurate information, could you please provide me with your full name, email address, and a preferred username? once i have these details, i'll guide you through the process step by step, ensuring that your input reaches the right department.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4100                    | 10.0847                   | 10.4947                   | CPU        | 10.1602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  72%|███████▏  | 36/50 [10:38<03:22, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 36/50 (Device: CPU):\n",
      "Instruction: show me in which situations  can i ask for refunds\n",
      "New Input: show me in which situations  can i ask for refunds [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'll take care of it! i understand that you would like to know the situations where you can request a refund. let me provide you with a comprehensive breakdown of the common scenarios:\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4373                    | 13.0983                   | 13.5355                   | CPU        | 1.9727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  74%|███████▍  | 37/50 [10:49<02:54, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 37/50 (Device: CPU):\n",
      "Instruction: help mke chatting with a person\n",
      "New Input: help mke chatting with a person [Classified: Intent is 'contact_human_agent', Category is 'contact', Entities are none]\n",
      "Generated Response: thank you for reaching out! i completely understand that you would like to chat with one of our human representatives. rest assured, i'm here to assist you every step of the way. to connect you with the right person, could you please provide me with some more details about the specific issue or question you have? this will help me direct you to the most suitable representative who can provide the support you need. your satisfaction is our top priority, and we're committed to resolving any concerns you may have.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4029                    | 10.4878                   | 10.8907                   | CPU        | 1.2773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  76%|███████▌  | 38/50 [10:57<02:21, 11.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 38/50 (Device: CPU):\n",
      "Instruction: there are errors trying to update my shipping address\n",
      "New Input: there are errors trying to update my shipping address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're encountering errors while updating your address. to assist you further, could you please provide more details about the specific errors you are facing? this will help me better understand the issue and provide you with the most accurate solution. thank you for reaching out to us, and i appreciate your patience as we work together to resolve this matter.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4079                    | 7.5460                   | 7.9538                   | CPU        | 2.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  78%|███████▊  | 39/50 [11:06<02:01, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 39/50 (Device: CPU):\n",
      "Instruction: i wnt help swapping several items of order ord-9073141\n",
      "New Input: i wnt help swapping several items of order ord-9073141 [Classified: Intent is 'change_order', Category is 'order', Entities are ord-9073141 (order_number)]\n",
      "Generated Response: we understand that you need assistance with swapping multiple items in order number {{order number}}. we're here to help! to proceed with the swap, could you please provide more details about the items you would like to swap? this will allow us to process your request accurately and efficiently. your satisfaction is our top priority, and we want to ensure that your order meets your expectations.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4297                    | 8.8378                   | 9.2675                   | CPU        | 8.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  80%|████████  | 40/50 [11:26<02:14, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 40/50 (Device: CPU):\n",
      "Instruction: i don't know how i can order some of your article\n",
      "New Input: i don't know how i can order some of your article [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for reaching out to us for assistance with placing an order for our articles. i understand that you may be unsure about the process, and i'm here to help guide you through it. to place your order, you can visit our website and navigate to the \"order history\" or \"my orders\" section. there you will find a comprehensive list of all the articles you wish to order. if you have any specific questions or need further assistance, please feel free to let me know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4226                    | 18.8230                   | 19.2456                   | CPU        | 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  82%|████████▏ | 41/50 [11:36<01:53, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 41/50 (Device: CPU):\n",
      "Instruction: i don't know how i could modify my address\n",
      "New Input: i don't know how i could modify my address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about how to modify your address. let me guide you through the process step by step. first, please log in to your account on our website and navigate to the \"my account\" or \"profile\" section. from there, you should be able to find an option to edit your shipping address or add a new one. if you encounter any difficulties or need further assistance, feel free to reach out to our customer support team.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4511                    | 10.0099                   | 10.4610                   | CPU        | 2.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  84%|████████▍ | 42/50 [11:46<01:35, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 42/50 (Device: CPU):\n",
      "Instruction: order ord-9186353 status\n",
      "New Input: order ord-9186353 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-9186353 status (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status of your order with the order number {{order number}}. to provide you with accurate information, could you please provide me with your account details or any other relevant information? once i have the necessary details, i will be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4401                    | 9.8402                   | 10.2803                   | CPU        | 2.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  86%|████████▌ | 43/50 [11:56<01:17, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 43/50 (Device: CPU):\n",
      "Instruction: help me making a claim against your business\n",
      "New Input: help me making a claim against your business [Classified: Intent is 'complaint', Category is 'feedback', Entities are business (account_type)]\n",
      "Generated Response: i'm sorry to hear that you're facing an issue with your claim. your satisfaction is our top priority, and we want to ensure that your concerns are addressed promptly and effectively. to assist you further, could you please provide me with more details about the nature of the claim? this will help us understand the situation better and provide you with the most appropriate guidance and support.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4381                    | 8.8131                   | 9.2512                   | CPU        | 2.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  88%|████████▊ | 44/50 [12:12<01:16, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 44/50 (Device: CPU):\n",
      "Instruction: i have got to see in which cases can i request my money back\n",
      "New Input: i have got to see in which cases can i request my money back [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the situations where you can request a refund. let me provide you with the information you need: 1. **product/service defect:** if you receive a product or service that has a defect or does not meet the description provided, you are generally eligible for a reimbursement under the terms and conditions outlined in our refund policy.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4232                    | 16.4200                   | 16.8432                   | CPU        | 1.2773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  90%|█████████ | 45/50 [12:21<00:58, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 45/50 (Device: CPU):\n",
      "Instruction: can you help me report troubles with payment?\n",
      "New Input: can you help me report troubles with payment? [Classified: Intent is 'payment_issue', Category is 'payment', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in reporting any troubles you're facing with your payment. please provide me with more details about the specific issue or error you encountered during the payment process. this will help us understand the situation better and provide you with the most appropriate solution. thank you for reaching out to us, and rest assured, we're committed to resolving this matter for you as quickly as possible.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4507                    | 8.3580                   | 8.8087                   | CPU        | 1.2656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  92%|█████████▏| 46/50 [12:32<00:45, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 46/50 (Device: CPU):\n",
      "Instruction: i try to notify of an error with registrations\n",
      "New Input: i try to notify of an error with registrations [Classified: Intent is 'registration_problems', Category is 'account', Entities are none]\n",
      "Generated Response: thank you for bringing this error to our attention. we understand that encountering errors during the registration process can be frustrating, and we apologize for any inconvenience caused. to assist you further, could you please provide more details about the specific error you are facing? this information will help us investigate and resolve the issue as quickly as possible. your feedback is highly valuable to us, as it helps us improve our services and ensure a seamless registration experience for all our users.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4126                    | 10.1026                   | 10.5152                   | CPU        | 1.2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  94%|█████████▍| 47/50 [12:44<00:34, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 47/50 (Device: CPU):\n",
      "Instruction: what do i have to do to cancel my platinum account?\n",
      "New Input: what do i have to do to cancel my platinum account? [Classified: Intent is 'delete_account', Category is 'account', Entities are platinum (account_type)]\n",
      "Generated Response: thank you for reaching out to us regarding the cancellation of your {{account type}} account. i understand that you would like assistance with canceling your account, and i'm here to guide you through the process. to proceed with cancellation, please provide me with some additional information such as the account details or any specific concerns you may have. this will help me better understand your situation and provide you with the most appropriate guidance. rest assured, we value your satisfaction and are committed to resolving this matter for you as quickly as possible.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.5005                    | 11.5246                   | 12.0251                   | CPU        | 1.2695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  96%|█████████▌| 48/50 [12:54<00:22, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 48/50 (Device: CPU):\n",
      "Instruction: how can i submit some feedback abot ur company\n",
      "New Input: how can i submit some feedback abot ur company [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: thank you for your interest in providing feedback on our company! to submit your feedback, you can visit our website and navigate to the \"contact us\" or \"feedback\" page. there you will find a form or a contact information to reach out to our dedicated customer support team. they will be more than happy to assist you with the feedback submission process. if you have any further questions or concerns, feel free to let me know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4188                    | 9.6740                   | 10.0927                   | CPU        | 1.2695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  98%|█████████▊| 49/50 [13:03<00:10, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 49/50 (Device: CPU):\n",
      "Instruction: change  order ord-8335461\n",
      "New Input: change  order ord-8335461 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-8335461 (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the current status of your order with the order number {{order number}}. to provide you with accurate information, could you please confirm if you made the purchase on our website or through another platform? once i have the necessary details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4549                    | 9.1248                   | 9.5797                   | CPU        | 1.2695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU: 100%|██████████| 50/50 [13:11<00:00, 15.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 50/50 (Device: CPU):\n",
      "Instruction: i need assistance to send some feedback for ur produucts\n",
      "New Input: i need assistance to send some feedback for ur produucts [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: we appreciate your willingness to share your feedback about our products and services. your insights are highly valuable to us as they help us understand how we can improve and deliver a better experience to our valued customers like yourself. please let me know if there's anything specific you would like to provide feedback on, and i'll be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.4192                    | 7.5008                   | 7.9199                   | CPU        | 12.1094\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK SUMMARY FOR CPU\n",
      "================================================================================\n",
      "Average Classification Time: 0.6490 seconds\n",
      "Average Generation Time: 15.1875 seconds\n",
      "Average Overall Time: 15.8365 seconds\n",
      "Average Memory Usage: 3.3430 MB\n",
      "\n",
      "================================================================================\n",
      "RUNNING BENCHMARK ON CUDA\n",
      "================================================================================\n",
      "\n",
      "Loading models on CUDA...\n",
      "Loading from: ../../Hybrid_Dense_Layer_Freeze\\label_encoders.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "All GPT-2 and DistilBERT layers remain frozen\n",
      "Attempting to load generation model from ../../text_generation_results_03-09-25/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   2%|▏         | 1/50 [00:08<07:02,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1/50 (Device: CUDA):\n",
      "Instruction: i do not know what i have to do to get my invoice #85632\n",
      "New Input: i do not know what i have to do to get my invoice #85632 [Classified: Intent is 'get_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: i'm clearly cognizant that you're unsure about the steps to take in order to retrieve your invoice with the number #{{imbursement number}}. let me guide you through the process. first, you'll need to log in to your account on our website. once logged in, navigate to the \"billing\" or \"invoices\" section, where you should be able to find a list of your invoices. look for a tab or link that says \"view invoice.\" click on that, and it will display the invoice you need. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team for assistance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 1.6400                    | 6.9829                   | 8.6229                   | CUDA        | 2775.2036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   4%|▍         | 2/50 [00:12<04:41,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 2/50 (Device: CUDA):\n",
      "Instruction: transfer to live agent\n",
      "New Input: transfer to live agent [Classified: Intent is 'switch_account', Category is 'contact', Entities are live (account_type)]\n",
      "Generated Response: thank you for expressing your interest in transferring to our {{account type}} agent. i'm here to assist you every step of the way. to initiate the transfer, please provide me with your account details such as your full name, email address, and preferred username. once i have these details, i will guide you through the process and ensure a smooth transfer. rest assured, we value your privacy and will handle your information with the utmost care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2550                    | 3.6909                   | 3.9459                   | CUDA        | 2700.2817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   6%|▌         | 3/50 [00:16<03:58,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 3/50 (Device: CUDA):\n",
      "Instruction: i need help to see how long refunds take\n",
      "New Input: i need help to see how long refunds take [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the timeline for refunds. the duration for processing refunds can vary depending on several factors, such as the payment method used and the specific policies of the vendor or service provider. to provide you with accurate information, could you please provide me with the refund reference number or any other relevant details? this will allow me to quickly look into the status of your refund and give you an estimated timeframe for its completion. thank you for your cooperation, and i appreciate your patience while i gather the necessary information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1512                    | 3.9790                   | 4.1303                   | CUDA        | 2713.7114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   8%|▊         | 4/50 [00:20<03:23,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 4/50 (Device: CUDA):\n",
      "Instruction: help to pseak to customer support\n",
      "New Input: help to pseak to customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are customer (account_type)]\n",
      "Generated Response: we're here to help! i take note that you would like to know if you can provide a specific query or if there's anything else i can assist you with. your satisfaction is our top priority, and we want to ensure that we address your concerns promptly and effectively. please feel free to share any additional details or questions you may have. together, we'll make sure you receive the support you need.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1547                    | 3.2482                   | 3.4029                   | CUDA        | 2692.8843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  10%|█         | 5/50 [00:23<03:10,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 5/50 (Device: CUDA):\n",
      "Instruction: i do not know what to do to swtch to the premium account\n",
      "New Input: i do not know what to do to swtch to the premium account [Classified: Intent is 'switch_account', Category is 'account', Entities are premium (account_type)]\n",
      "Generated Response: i appreciate your interest in switching to our {{account type}} account, and i'm here to guide you through the process. to initiate the switch, could you please provide me with your account details, such as your username or email address? once i have this information, i'll be able to assist you further and ensure a seamless transition. remember, your security and privacy are of utmost importance to us, so rest assured that your information will be handled with the highest level of care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2701                    | 3.6200                   | 3.8901                   | CUDA        | 2707.1929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  12%|█▏        | 6/50 [00:34<04:33,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 6/50 (Device: CUDA):\n",
      "Instruction: i need assistance resetting the key of my user account\n",
      "New Input: i need assistance resetting the key of my user account [Classified: Intent is 'recover_password', Category is 'account', Entities are user (account_type)]\n",
      "Generated Response: i'm here to provide you with the assistance you need to reset your key to regain access to your account. rest assured, i'll guide you through the process step by step to ensure a smooth and secure reset. to begin, please visit our platform's \"{{login page url}}\". from there, you should be able to navigate to the account settings or security settings section. look for the option that says \"reset key\" and click on it. you'll be prompted to enter the email address associated with your user profile. once you've entered your email, an email will be sent with instructions on how to recover your access key. if you encounter any difficulties or have further questions, feel free to reach out to our dedicated customer support team.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2087                    | 9.8991                   | 10.1078                   | CUDA        | 2825.7002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  14%|█▍        | 7/50 [00:37<03:53,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 7/50 (Device: CUDA):\n",
      "Instruction: i want help taking a quick look at the bills from john robertson\n",
      "New Input: i want help taking a quick look at the bills from john robertson [Classified: Intent is 'check_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to quickly view the invoices from {{salutation}} {{client last name}}. to assist you with this, could you please provide me with some additional details such as the date range or any other relevant information? this will help me locate the specific bills you are looking for more efficiently. rest assured, i am here to guide you every step of the way and ensure a seamless experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1736                    | 3.5970                   | 3.7706                   | CUDA        | 2704.0649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  16%|█▌        | 8/50 [00:42<03:33,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 8/50 (Device: CUDA):\n",
      "Instruction: i can't enter a delivery address\n",
      "New Input: i can't enter a delivery address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're having trouble entering a shipping address. let's work together to resolve this issue. to better assist you, could you please provide more details about the specific problem you are facing? are you encountering any error messages or is the system not accepting the address format? once i have a better understanding of the issue, i'll be able to provide you with the most accurate guidance to overcome this hurdle. your satisfaction is our top priority, and we're committed to resolving this matter for you as quickly as possible.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1614                    | 4.1926                   | 4.3540                   | CUDA        | 2715.4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  18%|█▊        | 9/50 [00:44<02:57,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 9/50 (Device: CUDA):\n",
      "Instruction: can you help me place an order?\n",
      "New Input: can you help me place an order? [Classified: Intent is 'delivery_options', Category is 'delivery', Entities are none]\n",
      "Generated Response: i'll take care of it! i'm here to assist you in placing your order with us. to get started, could you please provide me with the specific details of the items you would like to purchase? once i have that information, i will guide you through the ordering process step by step, ensuring a seamless experience for you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1510                    | 2.4897                   | 2.6407                   | CUDA        | 2676.4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  20%|██        | 10/50 [00:55<04:11,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 10/50 (Device: CUDA):\n",
      "Instruction: i do not know how i can set up my shipping address\n",
      "New Input: i do not know how i can set up my shipping address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm here to guide you through the process of setting up your delivery address. to get started, please follow these steps: 1. log in to your account on our website or mobile app using your username and password to access the account settings. 2. once you're logged in, navigate to the \"my account\" or \"profile\" section, where you should find an option to manage your shipping addresses. 3. click on that option, and you'll be directed to a page that provides step-by-step instructions on how to add a new address or edit your existing one. 4. fill in the required fields with the necessary details, such as your name, email address, city, state, postal code, or any other relevant information. 5. follow the on-screen prompts or instructions to complete the address modification. 6. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team, who will be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1679                    | 10.5106                   | 10.6785                   | CUDA        | 2823.6885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  22%|██▏       | 11/50 [01:09<05:39,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 11/50 (Device: CUDA):\n",
      "Instruction: i need assistance to set my new delovery address up\n",
      "New Input: i need assistance to set my new delovery address up [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in setting up your new delivery address. let's work together to make this process as smooth as possible: 1. begin by logging in to your account on our website or mobile app. you can find the \"my account\" or \"profile\" section in the settings or profile section, depending on which platform or app you're using for the address creation. rest assured, your security and privacy are of utmost importance to us, and we'll handle your information with the highest level of care and utmost care. 2. look for an option that says \"add a new address,\" \"edit shipping addresses,\" or something similar within the shipping address settings. click on that option and it will guide you through the necessary steps to update your address, including updating your existing address details. please note that it may take a few moments to complete the process, so don't hesitate to reach out if you encounter any difficulties or have any questions along the way. it's our goal to ensure a seamless and hassle-free experience for you. if there's anything else i can help you with, please feel free to let me know. your satisfaction is our top priority!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2883                    | 13.8772                   | 14.1655                   | CUDA        | 2859.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  24%|██▍       | 12/50 [01:12<04:26,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 12/50 (Device: CUDA):\n",
      "Instruction: help me to check what payment methods are available\n",
      "New Input: help me to check what payment methods are available [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the payment options available to you. to provide you with accurate information, could you please provide me with your full name, email address, and a preferred payment method? once i have these details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2833                    | 2.9103                   | 3.1936                   | CUDA        | 2681.3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  26%|██▌       | 13/50 [01:15<03:32,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 13/50 (Device: CUDA):\n",
      "Instruction: i call to leave my feedback for ur services\n",
      "New Input: i call to leave my feedback for ur services [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: we appreciate your willingness to share your feedback about our services. your insights are valuable to us as they help us understand how we can improve and deliver a better experience to our valued customers. please let me know if you have any specific questions or if there's anything else i can assist you with.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1603                    | 2.6393                   | 2.7996                   | CUDA        | 2673.7026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  28%|██▊       | 14/50 [01:20<03:12,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 14/50 (Device: CUDA):\n",
      "Instruction: open freemium account for wife\n",
      "New Input: open freemium account for wife [Classified: Intent is 'create_account', Category is 'account', Entities are ium (account_type), wife (account_type)]\n",
      "Generated Response: i appreciate your interest in opening a {{account type}} account with us! to proceed with the account creation, could you please provide me with your wife's full name, email address, and a preferred username? once i have these details, i'll guide you through the process step by step, ensuring a seamless experience for both of you. rest assured, we prioritize the security and privacy of our customers' information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2095                    | 4.1858                   | 4.3954                   | CUDA        | 2707.1929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  30%|███       | 15/50 [01:23<02:42,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 15/50 (Device: CUDA):\n",
      "Instruction: i am trying to see the status of order ord-1135283\n",
      "New Input: i am trying to see the status of order ord-1135283 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-1135283 (order_number)]\n",
      "Generated Response: thank you for reaching out! i completely understand that you would like to know if you are currently tracking your order with the order number {{order number}}. to provide you with accurate information, could you please provide me with your account details or any other relevant information? this will help me assist you more effectively.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1727                    | 2.8403                   | 3.0130                   | CUDA        | 2682.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  32%|███▏      | 16/50 [01:27<02:34,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 16/50 (Device: CUDA):\n",
      "Instruction: wanna see ur payment modalities will yahelp me\n",
      "New Input: wanna see ur payment modalities will yahelp me [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking our available payment methods. we strive to provide a seamless and convenient payment experience for our valued customers. to view our payment options, you can visit our website and navigate to the \"payments\" or \"checkout\" section. alternatively, if you have any specific questions or need further guidance, please don't hesitate to let me know. i'll be more than happy to help you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1491                    | 4.2094                   | 4.3585                   | CUDA        | 2706.2114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  34%|███▍      | 17/50 [01:31<02:24,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 17/50 (Device: CUDA):\n",
      "Instruction: what do i ned to do to change to the real estate account?\n",
      "New Input: what do i ned to do to change to the real estate account? [Classified: Intent is 'switch_account', Category is 'account', Entities are real estate (account_category)]\n",
      "Generated Response: thank you for your interest in upgrading to our {{account category}} account! i'm here to guide you through the process step by step. to get started, could you please provide me with your full name, email address, and a preferred username? once i have these details, i'll be able to assist you in making the necessary changes to your account. remember, your security and privacy are of utmost importance to us, so rest assured that your information will be handled with the utmost care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2449                    | 3.7337                   | 3.9786                   | CUDA        | 2715.7231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  36%|███▌      | 18/50 [01:35<02:17,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 18/50 (Device: CUDA):\n",
      "Instruction: notify of errorw with online registration\n",
      "New Input: notify of errorw with online registration [Classified: Intent is 'registration_problems', Category is 'account', Entities are none]\n",
      "Generated Response: we understand that encountering errors during the registration process can be frustrating, and we apologize for any inconvenience caused. to assist you further, could you please provide us with more details about the specific error you are facing? this will enable us to investigate the issue thoroughly and provide you with the most suitable solution. your feedback is crucial in helping us improve our services and ensure a seamless registration experience for all our users. thank you for bringing this to our attention.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2130                    | 3.8714                   | 4.0844                   | CUDA        | 2696.2085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  38%|███▊      | 19/50 [01:39<02:06,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 19/50 (Device: CUDA):\n",
      "Instruction: another standard accounmt for father\n",
      "New Input: another standard accounmt for father [Classified: Intent is 'switch_account', Category is 'account', Entities are standard (account_type), acc (account_type), father (delivery_city)]\n",
      "Generated Response: assuredly! i'm here to assist you in switching to another {{account type}} account for your father. to proceed, could you please provide me with your dad's full name, email address, and a preferred username? once i have these details, i'll guide you through the process step by step, ensuring a seamless experience for both of you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2399                    | 3.3121                   | 3.5520                   | CUDA        | 2704.2505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  40%|████      | 20/50 [01:42<01:58,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 20/50 (Device: CUDA):\n",
      "Instruction: i need infrmation about the cancellation of my enterprise account\n",
      "New Input: i need infrmation about the cancellation of my enterprise account [Classified: Intent is 'delete_account', Category is 'account', Entities are enterprise (account_type)]\n",
      "Generated Response: i'm here to assist you with canceling your {{account type}} account. your satisfaction is our top priority, and we want to ensure that you have all the information you need to make a successful cancellation. please provide me with your account details, such as your username or email address, so that i can guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2062                    | 3.5058                   | 3.7120                   | CUDA        | 2695.5166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  42%|████▏     | 21/50 [01:45<01:43,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 21/50 (Device: CUDA):\n",
      "Instruction: i want help ordering a few of your product\n",
      "New Input: i want help ordering a few of your product [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for your interest in ordering our products! i'm here to assist you with the ordering process. to get started, could you please provide me with more details about the specific products you would like to order? this will help me guide you through the necessary steps and ensure a seamless ordering experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2295                    | 2.3662                   | 2.5957                   | CUDA        | 2676.4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  44%|████▍     | 22/50 [01:49<01:40,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 22/50 (Device: CUDA):\n",
      "Instruction: checking status of order ord-6772250\n",
      "New Input: checking status of order ord-6772250 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-6772250 (order_number)]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to know if you have any status updates regarding your order with the order number {{order number}}. to provide you with accurate information, could you please provide me with your email address or phone number associated with this order? this will allow me to retrieve the necessary details and give you the most up-to-date status. your satisfaction is our top priority, and we're here to assist you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1679                    | 3.4837                   | 3.6516                   | CUDA        | 2708.1743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  46%|████▌     | 23/50 [01:52<01:36,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 23/50 (Device: CUDA):\n",
      "Instruction: deleting government account\n",
      "New Input: deleting government account [Classified: Intent is 'delete_account', Category is 'account', Entities are government (account_type)]\n",
      "Generated Response: i understand that you would like to delete your {{account type}} account. i apologize for any inconvenience this may have caused you. to assist you further, could you please provide me with your account details or any specific concerns you have regarding the deletion process? this will help me guide you through the necessary steps and ensure a smooth account deletion experience. your satisfaction is our top priority, and i'm here to support you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1525                    | 3.4305                   | 3.5831                   | CUDA        | 2687.2261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  48%|████▊     | 24/50 [01:55<01:27,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 24/50 (Device: CUDA):\n",
      "Instruction: my experience with your brand was bad, make a consumer claim\n",
      "New Input: my experience with your brand was bad, make a consumer claim [Classified: Intent is 'complaint', Category is 'feedback', Entities are consumer (account_type)]\n",
      "Generated Response: i apologize for any negative experience you may have had with our brand. your feedback is important to us, and we want to ensure that your concerns are addressed appropriately. could you please provide me with more details about the specific issue you encountered? this will help us understand the situation better and take the necessary steps to resolve it.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1912                    | 2.6978                   | 2.8890                   | CUDA        | 2681.5771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  50%|█████     | 25/50 [01:59<01:26,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 25/50 (Device: CUDA):\n",
      "Instruction: wann locate order ord-3779640\n",
      "New Input: wann locate order ord-3779640 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-3779640 (order_number)]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to locate your order with the order number {{order number}}. to assist you further, could you please provide me with some additional information? specifically, i would need to know the platform or website where you made the purchase. this will help me provide you with accurate and up-to-date information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1774                    | 3.5026                   | 3.6800                   | CUDA        | 2706.2114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  52%|█████▏    | 26/50 [02:09<02:13,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 26/50 (Device: CUDA):\n",
      "Instruction: i have to sde in what cases can i request a refund\n",
      "New Input: i have to sde in what cases can i request a refund [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'll take care of it! i understand that you would like to know the situations in which you can request refunds. here are some common scenarios where you may be eligible for a reimbursement: : **product/service defect:** if the product or service you received has a defect or does not meet the description provided, you typically have grounds to request an refund. however, it's important to note that it may take additional time for the defects to be reflected in your account, and this can vary depending on your vendor's policies and the specific policies of the company you are referring to. in most cases, refunds are processed within a few business days after the return or cancellation request has been approved by the appropriate department or customer support team. this timeframe allows us to gather all the necessary details and provide you with the most accurate information regarding the refund process and any associated fees or penalties associated with canceling your purchase before the agreed-upon time frame. if you have any specific concerns or questions regarding refund eligibility, please feel free to let me know. i'm here to assist you in providing the information you need.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2011                    | 10.3011                   | 10.5022                   | CUDA        | 2840.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  54%|█████▍    | 27/50 [02:13<01:55,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 27/50 (Device: CUDA):\n",
      "Instruction: i'm trying to see at what time i can call customer support\n",
      "New Input: i'm trying to see at what time i can call customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are none]\n",
      "Generated Response: thank you for reaching out! i completely understand that you would like to know the operating hours during which you can contact our customer assistance team. our dedicated team is available to assist you during our business hours. please feel free to reach out to us during these hours, and we will be more than happy to help you with any questions or concerns you may have. is there anything else i may be of assistance to you?\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1866                    | 3.4968                   | 3.6834                   | CUDA        | 2695.3994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  56%|█████▌    | 28/50 [02:17<01:46,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 28/50 (Device: CUDA):\n",
      "Instruction: assistance sending some feedback for ur proiducts\n",
      "New Input: assistance sending some feedback for ur proiducts [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: we appreciate your willingness to share your feedback about our products and services. your input is highly valuable to us as it helps us understand our customers' needs better and make improvements. to send your valuable feedback, you can visit our website and navigate to the \"contact us\" or \"feedback\" page. there you will find a form or contact information to reach out to our dedicated customer support team. they will be more than happy to assist you with any questions or concerns you may have. we look forward to hearing from you!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1691                    | 4.2080                   | 4.3771                   | CUDA        | 2717.0615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  58%|█████▊    | 29/50 [02:23<01:44,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 29/50 (Device: CUDA):\n",
      "Instruction: need assistance to cancel order ord-8227420\n",
      "New Input: need assistance to cancel order ord-8227420 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8227420 (order_number)]\n",
      "Generated Response: i'm here to assist you with canceling your order with the order number {{order number}}. rest assured, i'll guide you through the process step by step to ensure a smooth cancellation experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1527                    | 5.1424                   | 5.2951                   | CUDA        | 2742.7227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  60%|██████    | 30/50 [02:26<01:30,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 30/50 (Device: CUDA):\n",
      "Instruction: need assistance cancelinv purchase ord-8042947\n",
      "New Input: need assistance cancelinv purchase ord-8042947 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8042947 (order_number)]\n",
      "Generated Response: i'm here to assist you with canceling your purchase with the order number {{order number}}. your satisfaction is our top priority, and we want to ensure that you have a seamless experience with us. to proceed, could you please provide me with some additional details about the specific purchase you would like to cancel? this will help me guide you through the cancellation process accurately and efficiently.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1993                    | 3.2211                   | 3.4203                   | CUDA        | 2695.8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  62%|██████▏   | 31/50 [02:31<01:30,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 31/50 (Device: CUDA):\n",
      "Instruction: i don't know how i can download my bill #12588\n",
      "New Input: i don't know how i can download my bill #12588 [Classified: Intent is 'get_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about how to download your bill with the number #{{client last name}}. let me guide you through the process step by step. first, please log in to your account on our website. once logged in, navigate to the \"billing\" or \"invoices\" section. there, you should be able to find a list of your invoices. look for the specific invoice number or any other relevant information. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1665                    | 5.2073                   | 5.3738                   | CUDA        | 2742.6533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  64%|██████▍   | 32/50 [02:35<01:17,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 32/50 (Device: CUDA):\n",
      "Instruction: i need help to see purchase ord-0521414 status\n",
      "New Input: i need help to see purchase ord-0521414 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-0521414 status (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status of your purchase with the purchase number {{order number}}. to provide you with accurate information, could you please provide me with your account details or any other relevant information? once i have the necessary details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2022                    | 2.9753                   | 3.1775                   | CUDA        | 2689.1787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  66%|██████▌   | 33/50 [02:38<01:08,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 33/50 (Device: CUDA):\n",
      "Instruction: am i entitled to a refund according to your policy?\n",
      "New Input: am i entitled to a refund according to your policy? [Classified: Intent is 'get_refund', Category is 'refund', Entities are none]\n",
      "Generated Response: assuredly! i understand that you would like to know the status of your refund. your satisfaction is our top priority, and i'm here to assist you every step of the way. to provide you with accurate information, could you please provide me with your order number or any other relevant details? once i have these details, i'll be able to guide you through the refund process and ensure a smooth resolution.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1965                    | 3.3087                   | 3.5052                   | CUDA        | 2689.1787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  68%|██████▊   | 34/50 [02:46<01:24,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 34/50 (Device: CUDA):\n",
      "Instruction: need to check what methods for delivery i can choose help me\n",
      "New Input: need to check what methods for delivery i can choose help me [Classified: Intent is 'delivery_options', Category is 'delivery', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in exploring the various delivery methods available to you. let's dive into the world of delivery options tailored to your needs: standard shipping: this is our most popular option, with an estimated delivery time of {{date range}} business days. it's perfect for non-urgent items when you have a bit more time to spare. if you need your items sooner, you can opt for expedited shipping, which typically arrives within {{customer support hours}}. this option is suitable for urgent needs, and it ensures that your package arrives safely and in a timely manner. however, please note that it may take additional time for the delivery to arrive. to provide you with accurate information, could you please provide me with the {{tracking number}} or {{order number}}, and i'll be more than happy to guide you through the process?\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1890                    | 8.0562                   | 8.2452                   | CUDA        | 2795.7163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  70%|███████   | 35/50 [02:50<01:13,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 35/50 (Device: CUDA):\n",
      "Instruction: ya have an address to submit some feednack\n",
      "New Input: ya have an address to submit some feednack [Classified: Intent is 'review', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'll take care of it! i'm here to assist you in submitting your feedback about our products and services. to get started, could you please provide me with your full name, email address, and a preferred username? once i have these details, i will guide you through the process step by step, ensuring that your input reaches the right department. rest assured, your privacy and security are of utmost importance to us, so your information will be handled with the utmost care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2100                    | 3.7245                   | 3.9345                   | CUDA        | 2698.1523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  72%|███████▏  | 36/50 [02:55<01:07,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 36/50 (Device: CUDA):\n",
      "Instruction: show me in which situations  can i ask for refunds\n",
      "New Input: show me in which situations  can i ask for refunds [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the situations where you can request a refund. let me provide you with the information you need: 1. **product/service defect:** if you receive a product or service that has a defect or does not meet the description provided, you are typically eligible for a reimbursement.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1582                    | 4.5503                   | 4.7085                   | CUDA        | 2717.4219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  74%|███████▍  | 37/50 [02:58<00:57,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 37/50 (Device: CUDA):\n",
      "Instruction: help mke chatting with a person\n",
      "New Input: help mke chatting with a person [Classified: Intent is 'contact_human_agent', Category is 'contact', Entities are none]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to chat with an individual. our team of dedicated human representatives is here to assist you. to connect you with the right person, could you please provide some more details about the nature of your inquiry or the specific topic you need assistance with? this will help us ensure that you receive the most accurate and personalized support. we appreciate your patience and look forward to assisting you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.3225                    | 3.1154                   | 3.4379                   | CUDA        | 2688.1865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  76%|███████▌  | 38/50 [03:01<00:45,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 38/50 (Device: CUDA):\n",
      "Instruction: there are errors trying to update my shipping address\n",
      "New Input: there are errors trying to update my shipping address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're encountering errors while updating your delivery address. let's work together to resolve this issue. could you please provide more details about the specific errors you are facing? this will help us better understand the problem and provide you with the most accurate solution.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1804                    | 2.0956                   | 2.2760                   | CUDA        | 2672.8838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  78%|███████▊  | 39/50 [03:04<00:41,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 39/50 (Device: CUDA):\n",
      "Instruction: i wnt help swapping several items of order ord-9073141\n",
      "New Input: i wnt help swapping several items of order ord-9073141 [Classified: Intent is 'change_order', Category is 'order', Entities are ord-9073141 (order_number)]\n",
      "Generated Response: we understand that you need assistance with swapping multiple items in order number {{order number}}. we're here to help! to proceed with the swap, could you please provide more details about the items you would like to swap? this will allow us to process your request accurately and efficiently. your satisfaction is our top priority, and we want to ensure that your order meets your expectations.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2081                    | 3.4567                   | 3.6648                   | CUDA        | 2699.3428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  80%|████████  | 40/50 [03:08<00:38,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 40/50 (Device: CUDA):\n",
      "Instruction: i don't know how i can order some of your article\n",
      "New Input: i don't know how i can order some of your article [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for expressing your interest in ordering our articles! i'm here to guide you through the process and ensure a seamless experience. to place an order, you can visit our website and navigate to the \"order history\" or \"my orders\" section. there you will find detailed information about the specific articles you wish to order. if you have any questions or need further assistance, feel free to reach out to our customer support team. they will be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2588                    | 3.7874                   | 4.0462                   | CUDA        | 2701.3755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  82%|████████▏ | 41/50 [03:12<00:35,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 41/50 (Device: CUDA):\n",
      "Instruction: i don't know how i could modify my address\n",
      "New Input: i don't know how i could modify my address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about how to modify your address. let me guide you through the process step by step. first, please log in to your account on our website and navigate to the \"my account\" or \"profile\" section. from there, you should be able to find an option to edit your shipping address and save the changes. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team for assistance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1956                    | 3.8718                   | 4.0674                   | CUDA        | 2700.3228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  84%|████████▍ | 42/50 [03:16<00:31,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 42/50 (Device: CUDA):\n",
      "Instruction: order ord-9186353 status\n",
      "New Input: order ord-9186353 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-9186353 status (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status of your order with the order number {{order number}}. to provide you with accurate information, could you please confirm if you made the purchase on our website or through a third-party platform? once i have the necessary details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2037                    | 3.8307                   | 4.0343                   | CUDA        | 2705.2949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  86%|████████▌ | 43/50 [03:20<00:25,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 43/50 (Device: CUDA):\n",
      "Instruction: help me making a claim against your business\n",
      "New Input: help me making a claim against your business [Classified: Intent is 'complaint', Category is 'feedback', Entities are business (account_type)]\n",
      "Generated Response: i'm sorry to hear that you're facing an issue with your claim. your satisfaction is our top priority, and we want to ensure that your concerns are addressed promptly and effectively. to assist you further, could you please provide me with more details about the nature of the claim? this will help us understand the situation better and provide you with the most appropriate guidance and support.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1443                    | 2.9457                   | 3.0900                   | CUDA        | 2684.9580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  88%|████████▊ | 44/50 [03:27<00:28,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 44/50 (Device: CUDA):\n",
      "Instruction: i have got to see in which cases can i request my money back\n",
      "New Input: i have got to see in which cases can i request my money back [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm happy to help! i understand that you would like to have a clear understanding of the cases where you can request a refund. let me provide you with the information you need:\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1716                    | 7.1353                   | 7.3069                   | CUDA        | 2783.2549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  90%|█████████ | 45/50 [03:30<00:22,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 45/50 (Device: CUDA):\n",
      "Instruction: can you help me report troubles with payment?\n",
      "New Input: can you help me report troubles with payment? [Classified: Intent is 'payment_issue', Category is 'payment', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in reporting any troubles you're facing with your payment. please provide me with more details about the specific issue you encountered, such as error messages, payment methods used, or any other relevant information. this will help us understand the situation better and provide you with the most appropriate solution. thank you for reaching out to us, and rest assured, we'll work together to resolve this matter.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2142                    | 3.3634                   | 3.5776                   | CUDA        | 2686.7803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  92%|█████████▏| 46/50 [03:34<00:16,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 46/50 (Device: CUDA):\n",
      "Instruction: i try to notify of an error with registrations\n",
      "New Input: i try to notify of an error with registrations [Classified: Intent is 'registration_problems', Category is 'account', Entities are none]\n",
      "Generated Response: thank you for bringing this error to our attention. we understand that encountering errors during the registration process can be frustrating, and we apologize for any inconvenience caused. to assist you further, could you please provide more details about the specific error you are facing? this information will enable us to investigate and resolve the issue promptly. your feedback is highly valuable to us as it helps us improve our services and ensure a seamless registration experience for all our users.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1451                    | 3.0766                   | 3.2217                   | CUDA        | 2696.2539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  94%|█████████▍| 47/50 [03:40<00:14,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 47/50 (Device: CUDA):\n",
      "Instruction: what do i have to do to cancel my platinum account?\n",
      "New Input: what do i have to do to cancel my platinum account? [Classified: Intent is 'delete_account', Category is 'account', Entities are platinum (account_type)]\n",
      "Generated Response: thank you for reaching out to us regarding the cancellation of your {{account type}} account. we understand that circumstances may change, and we're here to assist you every step of the way. to proceed with canceling your account, you can follow these simple steps: 1. log in to our {{online company portal info}} using your credentials. 2. navigate to the account settings or profile section. 3. look for the option to manage or cancel your subscription. 4. follow the prompts or instructions provided to confirm your cancellation. if you encounter any difficulties or have further questions, please don't hesitate to let us know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2065                    | 6.0762                   | 6.2828                   | CUDA        | 2760.9062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  96%|█████████▌| 48/50 [03:43<00:08,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 48/50 (Device: CUDA):\n",
      "Instruction: how can i submit some feedback abot ur company\n",
      "New Input: how can i submit some feedback abot ur company [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: we appreciate your willingness to provide feedback on our company. to submit your feedback, you can visit our website and navigate to the \"contact us\" or \"feedback\" page. there you will find a form or contact information to reach out to our dedicated customer support team. they will be more than happy to assist you in submitting your valuable feedback. please let me know if there's anything else i can help you with!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.2687                    | 2.9864                   | 3.2551                   | CUDA        | 2686.8457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  98%|█████████▊| 49/50 [03:46<00:03,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 49/50 (Device: CUDA):\n",
      "Instruction: change  order ord-8335461\n",
      "New Input: change  order ord-8335461 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-8335461 (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in updating your order with the number {{order number}}. to provide you with accurate information, could you please provide me with your email address or any other relevant details related to the order? this will help me locate the specific order and guide you through the necessary steps to update it.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1628                    | 2.2792                   | 2.4419                   | CUDA        | 2680.3574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA: 100%|██████████| 50/50 [03:48<00:00,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 50/50 (Device: CUDA):\n",
      "Instruction: i need assistance to send some feedback for ur produucts\n",
      "New Input: i need assistance to send some feedback for ur produucts [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: we appreciate your willingness to share your feedback about our products and services. your input is highly valuable to us as it helps us understand how we can improve and deliver a better experience to our valued customers. please let me know if you have any specific questions or if there's anything else i can assist you with.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid_Dense_Layer  | 0.1507                    | 2.3409                   | 2.4916                   | CUDA        | 2676.2612\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK SUMMARY FOR CUDA\n",
      "================================================================================\n",
      "Average Classification Time: 0.2257 seconds\n",
      "Average Generation Time: 4.3466 seconds\n",
      "Average Overall Time: 4.5723 seconds\n",
      "Average Memory Usage: 2715.2363 MB\n",
      "CPU results saved to performance_results_cpu.json\n",
      "GPU results saved to performance_results_gpu.json\n",
      "Performance log saved to performance_log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance comparison plots saved to performance_comparison.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2Model, DistilBertModel, GPT2TokenizerFast, DistilBertTokenizerFast,GPT2Tokenizer,GPT2LMHeadModel\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import gc  # Add this import at the top\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2Model, DistilBertModel, GPT2TokenizerFast, DistilBertTokenizerFast\n",
    "from typing import Optional, Dict  # Add this import for type hints\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Optional\n",
    "from transformers import GPT2TokenizerFast, DistilBertTokenizerFast, GPT2Config, GPT2Model, DistilBertModel\n",
    "\n",
    "def log_to_file(message):\n",
    "    \"\"\"Helper function for logging\"\"\"\n",
    "    print(message)\n",
    "\n",
    "class DenseFusionLayer(nn.Module):\n",
    "    def __init__(self, gpt2_dim: int, bert_dim: int, output_dim: int, dropout_rate: float):\n",
    "        super().__init__()\n",
    "        self.gpt2_proj = nn.Linear(gpt2_dim, output_dim)\n",
    "        self.bert_proj = nn.Linear(bert_dim, output_dim)\n",
    "        self.dense = nn.Linear(output_dim, output_dim)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, gpt2_features: torch.Tensor, bert_features: torch.Tensor,\n",
    "                attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        gpt2_proj = self.gpt2_proj(gpt2_features)\n",
    "        bert_proj = self.bert_proj(bert_features)\n",
    "        combined_features = gpt2_proj + bert_proj\n",
    "        fused_features = self.dense(combined_features)\n",
    "        fused_features = self.activation(fused_features)\n",
    "        fused_features = self.dropout(fused_features)\n",
    "        return self.layer_norm(fused_features)\n",
    "\n",
    "class HybridGPT2DistilBERTMultiTask(nn.Module):\n",
    "    def __init__(self, num_intents: int, num_categories: int, num_ner_labels: int,\n",
    "                 dropout_rate: float, loss_weights: Optional[Dict[str, float]] = None,\n",
    "                 ner_class_weights: Optional[torch.Tensor] = None,\n",
    "                 category_class_weights: Optional[torch.Tensor] = None,\n",
    "                 intent_class_weights: Optional[torch.Tensor] = None):\n",
    "        super().__init__()\n",
    "        log_to_file(\"Initializing model...\")\n",
    "        self.gpt2_config = GPT2Config.from_pretrained('gpt2')\n",
    "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        # Freeze all layers\n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.distilbert.parameters():\n",
    "            param.requires_grad = False\n",
    "        log_to_file(\"All GPT-2 and DistilBERT layers remain frozen\")\n",
    "\n",
    "        gpt2_dim = self.gpt2_config.n_embd\n",
    "        bert_dim = self.distilbert.config.hidden_size\n",
    "        hidden_size = gpt2_dim\n",
    "\n",
    "        self.fusion_layer = DenseFusionLayer(gpt2_dim, bert_dim, hidden_size, dropout_rate)\n",
    "\n",
    "        self.intent_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_intents)\n",
    "        )\n",
    "        self.category_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_categories)\n",
    "        )\n",
    "        self.ner_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_ner_labels)\n",
    "        )\n",
    "\n",
    "        # Set default loss weights if not provided\n",
    "        self.loss_weights = loss_weights or {\n",
    "            'intent': 0.3,\n",
    "            'category': 0.3,\n",
    "            'ner': 0.4\n",
    "        }\n",
    "\n",
    "        # Loss functions with class weights\n",
    "        self.intent_loss_fn = nn.CrossEntropyLoss(weight=intent_class_weights) if intent_class_weights is not None else nn.CrossEntropyLoss()\n",
    "        self.category_loss_fn = nn.CrossEntropyLoss(weight=category_class_weights) if category_class_weights is not None else nn.CrossEntropyLoss()\n",
    "        self.ner_loss_fn = nn.CrossEntropyLoss(weight=ner_class_weights) if ner_class_weights is not None else nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, gpt2_input_ids: torch.Tensor, gpt2_attention_mask: torch.Tensor,\n",
    "                distilbert_input_ids: torch.Tensor, distilbert_attention_mask: torch.Tensor,\n",
    "                intent_labels: Optional[torch.Tensor] = None,\n",
    "                category_labels: Optional[torch.Tensor] = None,\n",
    "                ner_labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        gpt2_outputs = self.gpt2(input_ids=gpt2_input_ids, attention_mask=gpt2_attention_mask)\n",
    "        distilbert_outputs = self.distilbert(input_ids=distilbert_input_ids, attention_mask=distilbert_attention_mask)\n",
    "\n",
    "        gpt2_features = gpt2_outputs.last_hidden_state\n",
    "        bert_features = distilbert_outputs.last_hidden_state\n",
    "\n",
    "        fused_features = self.fusion_layer(gpt2_features, bert_features, gpt2_attention_mask)\n",
    "\n",
    "        masked_features = fused_features * gpt2_attention_mask.unsqueeze(-1)\n",
    "        sequence_repr = masked_features.sum(dim=1) / gpt2_attention_mask.sum(dim=1, keepdim=True)\n",
    "\n",
    "        intent_logits = self.intent_head(sequence_repr)\n",
    "        category_logits = self.category_head(sequence_repr)\n",
    "        ner_logits = self.ner_head(fused_features)\n",
    "\n",
    "        output_dict = {\n",
    "            'intent_logits': intent_logits,\n",
    "            'category_logits': category_logits,\n",
    "            'ner_logits': ner_logits\n",
    "        }\n",
    "\n",
    "        if all(label is not None for label in [intent_labels, category_labels, ner_labels]):\n",
    "            intent_loss = self.intent_loss_fn(intent_logits, intent_labels)\n",
    "            category_loss = self.category_loss_fn(category_logits, category_labels)\n",
    "            combined_mask = (gpt2_attention_mask * distilbert_attention_mask)\n",
    "            active_loss = combined_mask.view(-1) == 1\n",
    "            active_logits = ner_logits.view(-1, ner_logits.size(-1))[active_loss]\n",
    "            active_labels = ner_labels.view(-1)[active_loss]\n",
    "            ner_loss = self.ner_loss_fn(active_logits, active_labels)\n",
    "\n",
    "            total_loss = (self.loss_weights['intent'] * intent_loss +\n",
    "                          self.loss_weights['category'] * category_loss +\n",
    "                          self.loss_weights['ner'] * ner_loss)\n",
    "\n",
    "            output_dict.update({\n",
    "                'loss': total_loss,\n",
    "                'intent_loss': intent_loss,\n",
    "                'category_loss': category_loss,\n",
    "                'ner_loss': ner_loss\n",
    "            })\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "def inference_hybrid(model, text, gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length, device):\n",
    "    \"\"\"Run inference with the HybridGPT2DistilBERTMultiTask model with confidence scores as decimals\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    gpt2_encoding = gpt2_tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    distilbert_encoding = distilbert_tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    inputs = {\n",
    "        \"gpt2_input_ids\": gpt2_encoding[\"input_ids\"].to(device),\n",
    "        \"gpt2_attention_mask\": gpt2_encoding[\"attention_mask\"].to(device),\n",
    "        \"distilbert_input_ids\": distilbert_encoding[\"input_ids\"].to(device),\n",
    "        \"distilbert_attention_mask\": distilbert_encoding[\"attention_mask\"].to(device)\n",
    "    }\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get softmax probabilities for confidence scores\n",
    "    intent_logits = outputs[\"intent_logits\"]\n",
    "    category_logits = outputs[\"category_logits\"]\n",
    "    ner_logits = outputs[\"ner_logits\"]\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    intent_probs = torch.nn.functional.softmax(intent_logits, dim=-1)[0]\n",
    "    category_probs = torch.nn.functional.softmax(category_logits, dim=-1)[0]\n",
    "    ner_probs = torch.nn.functional.softmax(ner_logits, dim=-1)\n",
    "\n",
    "    # Get top predictions and their confidences\n",
    "    intent_pred = torch.argmax(intent_probs).cpu().item()\n",
    "    intent_confidence = intent_probs[intent_pred].cpu().item()  # As decimal (0.0-1.0)\n",
    "\n",
    "    category_pred = torch.argmax(category_probs).cpu().item()\n",
    "    category_confidence = category_probs[category_pred].cpu().item()  # As decimal (0.0-1.0)\n",
    "\n",
    "    ner_preds = torch.argmax(ner_probs, dim=-1).cpu().numpy()[0]\n",
    "    ner_confidences = torch.max(ner_probs, dim=-1)[0][0].cpu().numpy()  # Get max probability for each token\n",
    "\n",
    "    # Map to labels\n",
    "    intent_decoder = {v: k for k, v in label_encoders[\"intent_encoder\"].items()}\n",
    "    category_decoder = {v: k for k, v in label_encoders[\"category_encoder\"].items()}\n",
    "    ner_decoder = {v: k for k, v in label_encoders[\"ner_label_encoder\"].items()}\n",
    "\n",
    "    intent_label = intent_decoder[intent_pred]\n",
    "    category_label = category_decoder[category_pred]\n",
    "\n",
    "    tokens = gpt2_tokenizer.convert_ids_to_tokens(inputs[\"gpt2_input_ids\"][0].tolist())\n",
    "    seq_len = int(inputs[\"gpt2_attention_mask\"][0].sum().item())\n",
    "    ner_labels = [ner_decoder[pred] for pred in ner_preds[:seq_len]]\n",
    "\n",
    "    # Extract entities from NER labels with confidence\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    entity_tokens = []\n",
    "    entity_confidences = []\n",
    "    entity_type = None\n",
    "\n",
    "    for i, (token, label, confidence) in enumerate(zip(tokens[:seq_len], ner_labels, ner_confidences[:seq_len])):\n",
    "        if label.startswith(\"B-\"):\n",
    "            # If we were tracking an entity, save it before starting a new one\n",
    "            if current_entity is not None:\n",
    "                entity_text = gpt2_tokenizer.convert_tokens_to_string(entity_tokens).strip()\n",
    "                if entity_text:\n",
    "                    # Calculate average confidence for the entity (as decimal)\n",
    "                    avg_confidence = sum(entity_confidences) / len(entity_confidences)\n",
    "                    entities.append({\n",
    "                        \"entity\": entity_text,\n",
    "                        \"label\": entity_type,\n",
    "                        \"confidence\": avg_confidence\n",
    "                    })\n",
    "\n",
    "            # Start a new entity\n",
    "            current_entity = label[2:]\n",
    "            entity_type = label[2:]\n",
    "            entity_tokens = [token]\n",
    "            entity_confidences = [confidence]\n",
    "\n",
    "        elif label.startswith(\"I-\") and current_entity == label[2:]:\n",
    "            # Continue current entity\n",
    "            entity_tokens.append(token)\n",
    "            entity_confidences.append(confidence)\n",
    "\n",
    "        elif current_entity is not None:\n",
    "            # End of an entity\n",
    "            entity_text = gpt2_tokenizer.convert_tokens_to_string(entity_tokens).strip()\n",
    "            if entity_text:\n",
    "                # Calculate average confidence for the entity (as decimal)\n",
    "                avg_confidence = sum(entity_confidences) / len(entity_confidences)\n",
    "                entities.append({\n",
    "                    \"entity\": entity_text,\n",
    "                    \"label\": entity_type,\n",
    "                    \"confidence\": avg_confidence\n",
    "                })\n",
    "            current_entity = None\n",
    "            entity_tokens = []\n",
    "            entity_confidences = []\n",
    "            entity_type = None\n",
    "\n",
    "    # Check for unfinished entity\n",
    "    if current_entity is not None:\n",
    "        entity_text = gpt2_tokenizer.convert_tokens_to_string(entity_tokens).strip()\n",
    "        if entity_text:\n",
    "            # Calculate average confidence for the entity (as decimal)\n",
    "            avg_confidence = sum(entity_confidences) / len(entity_confidences)\n",
    "            entities.append({\n",
    "                \"entity\": entity_text,\n",
    "                \"label\": entity_type,\n",
    "                \"confidence\": avg_confidence\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"intent\": {\"label\": intent_label, \"confidence\": intent_confidence},\n",
    "        \"category\": {\"label\": category_label, \"confidence\": category_confidence},\n",
    "        \"ner\": entities\n",
    "    }\n",
    "\n",
    "def generate_response(model, tokenizer, instruction, classification, max_length=512, device=\"cuda\"):\n",
    "    \"\"\"Generate a response using GPT-2 model\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Extract classification details\n",
    "    intent = classification[\"intent\"][\"label\"] if isinstance(classification[\"intent\"], dict) else classification[\"intent\"]\n",
    "    category = classification[\"category\"][\"label\"] if isinstance(classification[\"category\"], dict) else classification[\"category\"]\n",
    "\n",
    "    # Clean up intent and category strings\n",
    "    if isinstance(intent, str) and \"[\" in intent:\n",
    "        intent = intent.strip(\"[]'\")\n",
    "    if isinstance(category, str) and \"[\" in category:\n",
    "        category = category.strip(\"[]'\")\n",
    "\n",
    "    # Format entity information\n",
    "    entities_text = \"\"\n",
    "    if \"ner\" in classification and classification[\"ner\"]:\n",
    "        entities = []\n",
    "        for entity in classification[\"ner\"]:\n",
    "            if isinstance(entity, dict) and \"entity\" in entity and \"label\" in entity:\n",
    "                entities.append(f\"{entity['entity']} ({entity['label']})\")\n",
    "        entities_text = \", \".join(entities)\n",
    "    else:\n",
    "        entities_text = \"none\"\n",
    "\n",
    "    # Create a prompt for generating response\n",
    "    input_text = f\"[INST] User query: {instruction}\\n\\n\" \\\n",
    "                 f\"Based on the following classification:\\n\" \\\n",
    "                 f\"- Intent: {intent}\\n\" \\\n",
    "                 f\"- Category: {category}\\n\" \\\n",
    "                 f\"- Entities: {entities_text}\\n\\n\" \\\n",
    "                 f\"Provide a helpful customer service response: [RESP]\"\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "    try:\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                num_beams=5,\n",
    "                no_repeat_ngram_size=2,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "\n",
    "        # Extract response part\n",
    "        if \"[RESP]\" in generated_text:\n",
    "            response = generated_text.split(\"[RESP]\")[1].strip()\n",
    "            # Clean up any trailing tokens\n",
    "            if \"[EOS]\" in response:\n",
    "                response = response.split(\"[EOS]\")[0].strip()\n",
    "        else:\n",
    "            # Fallback extraction\n",
    "            response = generated_text[len(input_text):].strip()\n",
    "        # Format steps if present\n",
    "        steps_pattern = re.search(r'(\\d+)\\.\\s+([A-Z])', response)\n",
    "        if steps_pattern or \"step\" in response.lower() or \"follow\" in response.lower():\n",
    "            # Format steps to be on separate lines\n",
    "            for i in range(1, 10):\n",
    "                step_marker = f\"{i}. \"\n",
    "                if step_marker in response and f\"\\n{i}. \" not in response:\n",
    "                    response = response.replace(step_marker, f\"\\n{i}. \")\n",
    "\n",
    "            # Clean up any excess newlines\n",
    "            response = re.sub(r'\\n\\s*\\n', '\\n\\n', response)\n",
    "            response = response.lstrip('\\n')\n",
    "\n",
    "        # Clean any technical artifacts\n",
    "        response = re.sub(r'https?://\\S+', '', response)  # Remove URLs\n",
    "        response = re.sub(r'<[^>]*>', '', response)  # Remove HTML tags\n",
    "        response = re.sub(r'\\{\\s*\"[^\"]*\":', '', response)  # Remove JSON-like content\n",
    "        response = re.sub(r'\\s+', ' ', response).strip()  # Clean up whitespace\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_response: {e}\")\n",
    "        return f\"I apologize, but I couldn't generate a response. Error: {str(e)}\"\n",
    "\n",
    "# Memory measurement functions\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / 1024 / 1024  # Convert bytes to MB\n",
    "\n",
    "# Fix the get_peak_memory_usage function\n",
    "def get_peak_memory_usage(func, *args, **kwargs):\n",
    "    device_param = kwargs.pop('device', None)  # Remove device parameter before calling func\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    start_mem = get_memory_usage()\n",
    "    result = func(*args, **kwargs)  # Call without device parameter\n",
    "    end_mem = get_memory_usage()\n",
    "    \n",
    "    peak_gpu_mem = 0\n",
    "    if torch.cuda.is_available() and device_param == \"cuda\":\n",
    "        peak_gpu_mem = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB\n",
    "        return result, peak_gpu_mem\n",
    "    else:\n",
    "        return result, max(0, end_mem - start_mem)\n",
    "\n",
    "class PerformanceTest:\n",
    "    def __init__(self, model_paths, test_data_path, num_samples=50):\n",
    "        self.output_dir = model_paths[\"hybrid_model_dir\"]\n",
    "        self.generation_model_path = model_paths[\"generation_model_path\"]\n",
    "        self.generation_tokenizer_path = model_paths[\"generation_tokenizer_path\"]\n",
    "        self.test_data_path = test_data_path\n",
    "        self.num_samples = num_samples\n",
    "        self.devices = [\"cpu\", \"cuda\"] if torch.cuda.is_available() else [\"cpu\"]\n",
    "        self.results = {device: [] for device in self.devices}\n",
    "        self.summary = {device: {\"classification_time\": [], \"generation_time\": [], \"overall_time\": [], \"memory_usage\": []} for device in self.devices}\n",
    "    \n",
    "    def load_models(self, device):\n",
    "        print(f\"\\nLoading models on {device.upper()}...\")\n",
    "        \n",
    "        # Load hybrid model for classification\n",
    "        encoders_path = os.path.join(self.output_dir, \"label_encoders.json\")\n",
    "        hyperparams_path = os.path.join(self.output_dir, \"hyperparameters.json\")\n",
    "        model_path = os.path.join(self.output_dir, \"hybrid_model.pth\")\n",
    "        \n",
    "        print(f\"Loading from: {encoders_path}\")\n",
    "        with open(encoders_path, 'r', encoding='utf-8') as f:\n",
    "            self.label_encoders = json.load(f)\n",
    "            \n",
    "        with open(hyperparams_path, 'r', encoding='utf-8') as f:\n",
    "            self.hyperparameters = json.load(f)\n",
    "                \n",
    "        self.gpt2_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        self.distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        if self.gpt2_tokenizer.pad_token is None:\n",
    "            self.gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        if self.distilbert_tokenizer.pad_token is None:\n",
    "            self.distilbert_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                \n",
    "        self.classification_model = HybridGPT2DistilBERTMultiTask(\n",
    "            num_intents=len(self.label_encoders[\"intent_encoder\"]),\n",
    "            num_categories=len(self.label_encoders[\"category_encoder\"]),\n",
    "            num_ner_labels=len(self.label_encoders[\"ner_label_encoder\"]),\n",
    "            dropout_rate=self.hyperparameters[\"dropout_rate\"]\n",
    "        )\n",
    "        \n",
    "        if self.gpt2_tokenizer.pad_token_id is not None:\n",
    "            self.classification_model.gpt2.resize_token_embeddings(len(self.gpt2_tokenizer))\n",
    "        \n",
    "        self.classification_model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "        self.classification_model.to(device)\n",
    "        self.classification_model.eval()\n",
    "        \n",
    "        # Load generation model using from_pretrained instead of torch.load\n",
    "        try:\n",
    "            print(f\"Attempting to load generation model from {self.generation_model_path}\")\n",
    "            self.generation_model = GPT2LMHeadModel.from_pretrained(self.generation_model_path).to(device)\n",
    "            self.generation_tokenizer = GPT2Tokenizer.from_pretrained(self.generation_tokenizer_path)\n",
    "            self.generation_tokenizer.pad_token = self.generation_tokenizer.eos_token\n",
    "            self.generation_tokenizer.add_special_tokens({'additional_special_tokens': ['[INST]', '[RESP]', '[EOS]']})\n",
    "            self.generation_model.resize_token_embeddings(len(self.generation_tokenizer))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading generation model: {e}\")\n",
    "            print(\"Falling back to default GPT2...\")\n",
    "            self.generation_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "            self.generation_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            self.generation_tokenizer.pad_token = self.generation_tokenizer.eos_token\n",
    "            self.generation_tokenizer.add_special_tokens({'additional_special_tokens': ['[INST]', '[RESP]', '[EOS]']})\n",
    "            self.generation_model.resize_token_embeddings(len(self.generation_tokenizer))\n",
    "        \n",
    "        self.generation_model.eval()\n",
    "        \n",
    "        return self.classification_model, self.generation_model, self.gpt2_tokenizer, self.distilbert_tokenizer, self.generation_tokenizer\n",
    "    \n",
    "    def run_benchmark(self):\n",
    "        # Load test data\n",
    "        with open(self.test_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "            \n",
    "        # Select samples for testing\n",
    "        if len(test_data) > self.num_samples:\n",
    "            test_samples = test_data[:self.num_samples]\n",
    "        else:\n",
    "            test_samples = test_data\n",
    "            \n",
    "        print(f\"Running performance test on {len(test_samples)} samples\")\n",
    "        \n",
    "        # Run tests on each device\n",
    "        for device in self.devices:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"RUNNING BENCHMARK ON {device.upper()}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            classification_model, generation_model, gpt2_tokenizer, distilbert_tokenizer, generation_tokenizer = self.load_models(device)\n",
    "            \n",
    "            for idx, sample in enumerate(tqdm(test_samples, desc=f\"Testing on {device.upper()}\")):\n",
    "                instruction = sample[\"instruction\"]\n",
    "                \n",
    "                # Measure classification time and memory\n",
    "                def run_classification():\n",
    "                    return inference_hybrid(\n",
    "                        classification_model, \n",
    "                        instruction, \n",
    "                        gpt2_tokenizer, \n",
    "                        distilbert_tokenizer, \n",
    "                        self.label_encoders, \n",
    "                        self.hyperparameters[\"max_length\"],\n",
    "                        device\n",
    "                    )\n",
    "                \n",
    "                classification_start = time.time()\n",
    "                classification_result, classification_memory = get_peak_memory_usage(\n",
    "                    run_classification, device=device\n",
    "                )\n",
    "                classification_end = time.time()\n",
    "                classification_time = classification_end - classification_start\n",
    "                \n",
    "                # Create new input with classification results\n",
    "                intent = classification_result[\"intent\"][\"label\"]\n",
    "                category = classification_result[\"category\"][\"label\"]\n",
    "                entities_text = \", \".join([f\"{entity['entity']} ({entity['label']})\" for entity in classification_result[\"ner\"]]) if classification_result[\"ner\"] else \"none\"\n",
    "                new_input = f\"{instruction} [Classified: Intent is '{intent}', Category is '{category}', Entities are {entities_text}]\"\n",
    "                \n",
    "                # Measure generation time and memory\n",
    "                def run_generation():\n",
    "                    return generate_response(\n",
    "                        generation_model,\n",
    "                        generation_tokenizer,\n",
    "                        instruction,\n",
    "                        classification_result,\n",
    "                        device=device\n",
    "                    )\n",
    "                \n",
    "                generation_start = time.time()\n",
    "                generated_response, generation_memory = get_peak_memory_usage(\n",
    "                    run_generation, device=device\n",
    "                )\n",
    "                generation_end = time.time()\n",
    "                generation_time = generation_end - generation_start\n",
    "                \n",
    "                # Calculate overall time and memory\n",
    "                overall_time = classification_time + generation_time\n",
    "                overall_memory = classification_memory + generation_memory\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    \"sample_id\": idx + 1,\n",
    "                    \"instruction\": instruction,\n",
    "                    \"new_input\": new_input,\n",
    "                    \"generated_response\": generated_response,\n",
    "                    \"classification_time\": classification_time,\n",
    "                    \"generation_time\": generation_time,\n",
    "                    \"overall_time\": overall_time,\n",
    "                    \"classification_memory\": classification_memory,\n",
    "                    \"generation_memory\": generation_memory,\n",
    "                    \"overall_memory\": overall_memory\n",
    "                }\n",
    "                self.results[device].append(result)\n",
    "                \n",
    "                # Update summary statistics\n",
    "                self.summary[device][\"classification_time\"].append(classification_time)\n",
    "                self.summary[device][\"generation_time\"].append(generation_time)\n",
    "                self.summary[device][\"overall_time\"].append(overall_time)\n",
    "                self.summary[device][\"memory_usage\"].append(overall_memory)\n",
    "                \n",
    "                # Print sample result\n",
    "                print(f\"\\nSample {idx+1}/{len(test_samples)} (Device: {device.upper()}):\")\n",
    "                print(f\"Instruction: {instruction}\")\n",
    "                print(f\"New Input: {new_input}\")\n",
    "                print(f\"Generated Response: {generated_response}\")\n",
    "                print(f\"Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\")\n",
    "                print(f\"Hybrid_Dense_Layer  | {classification_time:.4f}                    | {generation_time:.4f}                   | {overall_time:.4f}                   | {device.upper()}        | {overall_memory:.4f}\")\n",
    "            \n",
    "            # Calculate average times\n",
    "            self.summary[device][\"avg_classification_time\"] = sum(self.summary[device][\"classification_time\"]) / len(test_samples)\n",
    "            self.summary[device][\"avg_generation_time\"] = sum(self.summary[device][\"generation_time\"]) / len(test_samples)\n",
    "            self.summary[device][\"avg_overall_time\"] = sum(self.summary[device][\"overall_time\"]) / len(test_samples)\n",
    "            self.summary[device][\"avg_memory_usage\"] = sum(self.summary[device][\"memory_usage\"]) / len(test_samples)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"BENCHMARK SUMMARY FOR {device.upper()}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Average Classification Time: {self.summary[device]['avg_classification_time']:.4f} seconds\")\n",
    "            print(f\"Average Generation Time: {self.summary[device]['avg_generation_time']:.4f} seconds\")\n",
    "            print(f\"Average Overall Time: {self.summary[device]['avg_overall_time']:.4f} seconds\")\n",
    "            print(f\"Average Memory Usage: {self.summary[device]['avg_memory_usage']:.4f} MB\")\n",
    "\n",
    "        # Save results to files and create plots\n",
    "        self.save_results()\n",
    "\n",
    "    # Add to your PerformanceTest class\n",
    "    def save_results(self):\n",
    "        \"\"\"Save benchmark results to JSON files in the specified format\"\"\"\n",
    "        \n",
    "        # Save CPU results\n",
    "        if \"cpu\" in self.devices and \"avg_classification_time\" in self.summary[\"cpu\"]:\n",
    "            cpu_results = []\n",
    "            \n",
    "            for result in self.results[\"cpu\"]:\n",
    "                formatted_result = {\n",
    "                    \"Model\": \"Hybrid Dense Layer\",\n",
    "                    \"Classification Time (s)\": result[\"classification_time\"],\n",
    "                    \"Text Generation Time (s)\": result[\"generation_time\"],\n",
    "                    \"Overall Inference Time (s)\": result[\"overall_time\"],\n",
    "                    \"Device\": \"CPU\",\n",
    "                    \"Memory Usage (MB)\": result[\"overall_memory\"],\n",
    "                    \"Instruction\": result[\"instruction\"],\n",
    "                    \"New Input\": result[\"new_input\"],\n",
    "                    \"Generated Response\": result[\"generated_response\"]\n",
    "                }\n",
    "                cpu_results.append(formatted_result)\n",
    "            \n",
    "            with open(\"performance_results_cpu.json\", \"w\") as f:\n",
    "                json.dump(cpu_results, f, indent=4)\n",
    "            print(\"CPU results saved to performance_results_cpu.json\")\n",
    "        \n",
    "        # Save GPU results\n",
    "        if \"cuda\" in self.devices and \"avg_classification_time\" in self.summary[\"cuda\"]:\n",
    "            gpu_results = []\n",
    "            \n",
    "            for result in self.results[\"cuda\"]:\n",
    "                formatted_result = {\n",
    "                    \"Model\": \"GPT2-Based Pipeline\",\n",
    "                    \"Classification Time (s)\": result[\"classification_time\"],\n",
    "                    \"Text Generation Time (s)\": result[\"generation_time\"],\n",
    "                    \"Overall Inference Time (s)\": result[\"overall_time\"],\n",
    "                    \"Device\": \"GPU\",\n",
    "                    \"Memory Usage (MB)\": result[\"overall_memory\"],\n",
    "                    \"Instruction\": result[\"instruction\"],\n",
    "                    \"New Input\": result[\"new_input\"],\n",
    "                    \"Generated Response\": result[\"generated_response\"]\n",
    "                }\n",
    "                gpu_results.append(formatted_result)\n",
    "            \n",
    "            with open(\"performance_results_gpu.json\", \"w\") as f:\n",
    "                json.dump(gpu_results, f, indent=4)\n",
    "            print(\"GPU results saved to performance_results_gpu.json\")\n",
    "        \n",
    "        # Create performance log and comparison plots\n",
    "        self.create_performance_log()\n",
    "        self.create_comparison_plots()\n",
    "\n",
    "    def create_performance_log(self):\n",
    "        \"\"\"Create a text file with performance comparisons\"\"\"\n",
    "        with open(\"performance_log.txt\", \"w\") as f:\n",
    "            f.write(\"=== PERFORMANCE BENCHMARK RESULTS ===\\n\\n\")\n",
    "            \n",
    "            # Write CPU results\n",
    "            if \"cpu\" in self.devices:\n",
    "                f.write(\"CPU PERFORMANCE:\\n\")\n",
    "                f.write(f\"Average Classification Time: {self.summary['cpu']['avg_classification_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Generation Time: {self.summary['cpu']['avg_generation_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Overall Time: {self.summary['cpu']['avg_overall_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Memory Usage: {self.summary['cpu']['avg_memory_usage']:.4f} MB\\n\\n\")\n",
    "            \n",
    "            # Write GPU results\n",
    "            if \"cuda\" in self.devices and \"cuda\" in self.results:\n",
    "                f.write(\"GPU PERFORMANCE:\\n\")\n",
    "                f.write(f\"Average Classification Time: {self.summary['cuda']['avg_classification_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Generation Time: {self.summary['cuda']['avg_generation_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Overall Time: {self.summary['cuda']['avg_overall_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Memory Usage: {self.summary['cuda']['avg_memory_usage']:.4f} MB\\n\\n\")\n",
    "            \n",
    "            # Write comparison\n",
    "            if \"cpu\" in self.devices and \"cuda\" in self.devices and \"cuda\" in self.results:\n",
    "                f.write(\"CPU vs GPU COMPARISON:\\n\")\n",
    "                class_speedup = self.summary['cpu']['avg_classification_time'] / self.summary['cuda']['avg_classification_time']\n",
    "                gen_speedup = self.summary['cpu']['avg_generation_time'] / self.summary['cuda']['avg_generation_time']\n",
    "                overall_speedup = self.summary['cpu']['avg_overall_time'] / self.summary['cuda']['avg_overall_time']\n",
    "                \n",
    "                f.write(f\"Classification Speed Improvement: {class_speedup:.2f}x faster on GPU\\n\")\n",
    "                f.write(f\"Generation Speed Improvement: {gen_speedup:.2f}x faster on GPU\\n\")\n",
    "                f.write(f\"Overall Speed Improvement: {overall_speedup:.2f}x faster on GPU\\n\\n\")\n",
    "                \n",
    "                f.write(\"Notes:\\n\")\n",
    "                f.write(\"- GPU memory usage is typically higher but processing is faster\\n\")\n",
    "                f.write(\"- The generation task shows the largest speed improvement on GPU\\n\")\n",
    "            \n",
    "            f.write(\"\\n=== END OF REPORT ===\\n\")\n",
    "        \n",
    "        print(\"Performance log saved to performance_log.txt\")\n",
    "\n",
    "    def create_comparison_plots(self):\n",
    "        \"\"\"Create matplotlib comparison plots\"\"\"\n",
    "        if \"cpu\" not in self.devices or \"cuda\" not in self.devices or \"cuda\" not in self.results:\n",
    "            print(\"Both CPU and GPU results are needed for comparison plots\")\n",
    "            return\n",
    "        \n",
    "        # Set up the plots\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        \n",
    "        # 1. Processing Time Comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        \n",
    "        labels = ['Classification', 'Generation', 'Overall']\n",
    "        cpu_times = [self.summary['cpu']['avg_classification_time'], \n",
    "                    self.summary['cpu']['avg_generation_time'],\n",
    "                    self.summary['cpu']['avg_overall_time']]\n",
    "        \n",
    "        gpu_times = [self.summary['cuda']['avg_classification_time'], \n",
    "                    self.summary['cuda']['avg_generation_time'],\n",
    "                    self.summary['cuda']['avg_overall_time']]\n",
    "        \n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, cpu_times, width, label='CPU')\n",
    "        plt.bar(x + width/2, gpu_times, width, label='GPU')\n",
    "        \n",
    "        plt.xlabel('Task')\n",
    "        plt.ylabel('Time (seconds)')\n",
    "        plt.title('Average Processing Time Comparison')\n",
    "        plt.xticks(x, labels)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on the bars\n",
    "        for i, v in enumerate(cpu_times):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.2f}s\", ha='center')\n",
    "        \n",
    "        for i, v in enumerate(gpu_times):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.2f}s\", ha='center')\n",
    "        \n",
    "        # 2. Speed Improvement\n",
    "        plt.subplot(2, 2, 2)\n",
    "        \n",
    "        speedups = [\n",
    "            cpu_times[0] / gpu_times[0] if gpu_times[0] > 0 else 0,\n",
    "            cpu_times[1] / gpu_times[1] if gpu_times[1] > 0 else 0,\n",
    "            cpu_times[2] / gpu_times[2] if gpu_times[2] > 0 else 0\n",
    "        ]\n",
    "        \n",
    "        plt.bar(x, speedups, width=0.5)\n",
    "        plt.axhline(y=1.0, color='r', linestyle='-', alpha=0.3)\n",
    "        plt.xlabel('Task')\n",
    "        plt.ylabel('Times Faster on GPU')\n",
    "        plt.title('CPU vs GPU Speed Improvement')\n",
    "        plt.xticks(x, labels)\n",
    "        \n",
    "        # Add speed improvement labels\n",
    "        for i, v in enumerate(speedups):\n",
    "            plt.text(i, v + 0.1, f\"{v:.1f}x\", ha='center')\n",
    "        \n",
    "        # 3. Memory Usage\n",
    "        plt.subplot(2, 2, 3)\n",
    "        \n",
    "        memory_labels = ['CPU', 'GPU']\n",
    "        memory_usage = [self.summary['cpu']['avg_memory_usage'],\n",
    "                    self.summary['cuda']['avg_memory_usage']]\n",
    "        \n",
    "        plt.bar(memory_labels, memory_usage)\n",
    "        plt.xlabel('Device')\n",
    "        plt.ylabel('Memory Usage (MB)')\n",
    "        plt.title('Average Memory Usage')\n",
    "        \n",
    "        # Add memory usage labels\n",
    "        for i, v in enumerate(memory_usage):\n",
    "            plt.text(i, v + 0.1, f\"{v:.1f} MB\", ha='center')\n",
    "        \n",
    "        # 4. Time breakdown for both devices\n",
    "        plt.subplot(2, 2, 4)\n",
    "        \n",
    "        # CPU breakdown\n",
    "        cpu_breakdown = [\n",
    "            self.summary['cpu']['avg_classification_time'] / self.summary['cpu']['avg_overall_time'] * 100,\n",
    "            self.summary['cpu']['avg_generation_time'] / self.summary['cpu']['avg_overall_time'] * 100\n",
    "        ]\n",
    "        \n",
    "        # GPU breakdown  \n",
    "        gpu_breakdown = [\n",
    "            self.summary['cuda']['avg_classification_time'] / self.summary['cuda']['avg_overall_time'] * 100,\n",
    "            self.summary['cuda']['avg_generation_time'] / self.summary['cuda']['avg_overall_time'] * 100\n",
    "        ]\n",
    "        \n",
    "        task_labels = ['Classification', 'Generation']\n",
    "        \n",
    "        x = np.arange(len(memory_labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, [cpu_breakdown[0], gpu_breakdown[0]], width, label='Classification')\n",
    "        plt.bar(x - width/2, [cpu_breakdown[1], gpu_breakdown[1]], width, bottom=[cpu_breakdown[0], gpu_breakdown[0]], \n",
    "                label='Generation')\n",
    "        \n",
    "        plt.xlabel('Device')\n",
    "        plt.ylabel('Percentage of Overall Time (%)')\n",
    "        plt.title('Task Time Distribution')\n",
    "        plt.xticks(x - width/2, memory_labels)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for i, (c, g) in enumerate(zip(cpu_breakdown, gpu_breakdown)):\n",
    "            plt.text(0 - width/2, c/2 if i == 0 else cpu_breakdown[0] + c/2, \n",
    "                    f\"{c:.1f}%\", ha='center')\n",
    "            plt.text(1 - width/2, g/2 if i == 0 else gpu_breakdown[0] + g/2,\n",
    "                    f\"{g:.1f}%\", ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('performance_comparison.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Performance comparison plots saved to performance_comparison.png\")\n",
    "def main():\n",
    "    # Define paths\n",
    "    model_paths = {\n",
    "        \"hybrid_model_dir\": \"../../Hybrid_Dense_Layer_Freeze\",\n",
    "        \"generation_model_path\": \"../../text_generation_results_03-09-25/model\",\n",
    "        \"generation_tokenizer_path\": \"../../text_generation_results_03-09-25/tokenizer\"\n",
    "    }\n",
    "    \n",
    "    test_data_path = \"../../test.json\"\n",
    "    \n",
    "    # Verify and print paths\n",
    "    print(f\"Hybrid model directory: {model_paths['hybrid_model_dir']}\")\n",
    "    print(f\"Test data path: {test_data_path}\")\n",
    "    \n",
    "    # Number of samples to test\n",
    "    num_samples = 50  # You can adjust this number\n",
    "    \n",
    "    # Run the benchmark\n",
    "    benchmark = PerformanceTest(model_paths, test_data_path, num_samples)\n",
    "    benchmark.run_benchmark()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check available devices\n",
    "    device_info = \"CUDA\" if torch.cuda.is_available() else \"CPU\"\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"Running on: {device_info}\")\n",
    "    \n",
    "    # Run the benchmark\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
