{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0+cu117)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (6.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement rogue-score (from versions: none)\n",
      "ERROR: No matching distribution found for rogue-score\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch psutil tqdm numpy rogue-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Running on: CUDA\n",
      "Hybrid model directory: ../../Hybrid_Cross_Attention_Freeze\n",
      "Test data path: ../../test.json\n",
      "Running performance test on 50 samples\n",
      "\n",
      "================================================================================\n",
      "RUNNING BENCHMARK ON CPU\n",
      "================================================================================\n",
      "\n",
      "Loading models on CPU...\n",
      "Loading from: ../../Hybrid_Cross_Attention_Freeze\\label_encoders.json\n",
      "Initializing model...\n",
      "All GPT-2 and DistilBERT layers remain frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load generation model from ../../text_generation_results_03-09-25/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   2%|▏         | 1/50 [00:17<14:04, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1/50 (Device: CPU):\n",
      "Instruction: i do not know what i have to do to get my invoice #85632\n",
      "New Input: i do not know what i have to do to get my invoice #85632 [Classified: Intent is 'check_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about the steps to take in order to retrieve your invoice with the number #{{client last name}}. let me guide you through the process. first, please log in to your account on our website. once logged in, navigate to the \"billing\" or \"invoices\" section. there, you should be able to find a list of your invoices. look for a tab or link that says \"view invoice.\" click on that tab, and you'll be directed to a page where you can view the invoice details. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team. they will be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.7095                    | 16.5298                   | 17.2392                   | CPU        | 29.2344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   4%|▍         | 2/50 [00:25<09:43, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 2/50 (Device: CPU):\n",
      "Instruction: transfer to live agent\n",
      "New Input: transfer to live agent [Classified: Intent is 'contact_human_agent', Category is 'contact', Entities are live (delivery_city), agent (account_type)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in transferring to one of our live agents. your satisfaction is our top priority, and we want to ensure that you receive the support you need. could you please provide me with more details about the specific issue or question you have? this will help me direct you to the most suitable agent who can address your needs effectively.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.4511                    | 8.1363                   | 8.5875                   | CPU        | 0.0469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   6%|▌         | 3/50 [00:37<09:25, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 3/50 (Device: CPU):\n",
      "Instruction: i need help to see how long refunds take\n",
      "New Input: i need help to see how long refunds take [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the timeframe for refunds. the time it takes for a refund to be processed can vary depending on several factors, such as the payment method used and the specific policies of the vendor or service provider. to provide you with accurate information, could you please provide me with your order number or any other relevant details? this will help me locate the refund request and give you the most up-to-date timeline. thank you for your cooperation, and i appreciate your patience while i gather the necessary information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.3921                    | 11.4996                   | 11.8916                   | CPU        | 9.7148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   8%|▊         | 4/50 [00:52<10:07, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 4/50 (Device: CPU):\n",
      "Instruction: help to pseak to customer support\n",
      "New Input: help to pseak to customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are customer (account_type)]\n",
      "Generated Response: thank you for reaching out! i'm here to provide you with the assistance you need to reach our dedicated support team. to get started, could you please provide me with more details about the specific issue or question you have? this will help me guide you through the process and ensure that your query is directed to the right department.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.4130                    | 14.5885                   | 15.0015                   | CPU        | 1.9844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  10%|█         | 5/50 [01:25<15:16, 20.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 5/50 (Device: CPU):\n",
      "Instruction: i do not know what to do to swtch to the premium account\n",
      "New Input: i do not know what to do to swtch to the premium account [Classified: Intent is 'switch_account', Category is 'account', Entities are premium (account_type)]\n",
      "Generated Response: i appreciate your interest in switching to our {{account type}} account, and i'm here to guide you through the process step by step. to get started, could you please provide me with your account details, such as your username or email address? once i have this information, i'll be able to assist you further and ensure a smooth transition. rest assured, your security and privacy are of utmost importance to us, so your information will be handled with the utmost care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.8451                    | 31.2312                   | 33.0762                   | CPU        | 1.5312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  12%|█▏        | 6/50 [02:32<26:28, 36.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 6/50 (Device: CPU):\n",
      "Instruction: i need assistance resetting the key of my user account\n",
      "New Input: i need assistance resetting the key of my user account [Classified: Intent is 'recover_password', Category is 'account', Entities are user (account_type)]\n",
      "Generated Response: assuredly! i'm here to provide you with the assistance you need to reset your key to regain access to your account. let's work together to resolve this issue and ensure a smooth and secure reset process. here are the steps you can follow: 1. begin by visiting our platform's \"{{login page url}}\" to initiate the account recovery journey. you'll be prompted to enter the email address associated with your user profile. remember, your security and privacy are of utmost importance to us, so please refrain from sharing any personal identifiers here. 2. once you've entered your email, keep an eye on your inbox for any messages or notifications from our customer support team. they will be able to guide you through the necessary steps and address any questions or concerns you may have along the way. if you encounter any difficulties or have further questions during the process, please don't hesitate to let me know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.9548                    | 64.6777                   | 66.6325                   | CPU        | 0.6797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  14%|█▍        | 7/50 [03:08<25:55, 36.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 7/50 (Device: CPU):\n",
      "Instruction: i want help taking a quick look at the bills from john robertson\n",
      "New Input: i want help taking a quick look at the bills from john robertson [Classified: Intent is 'check_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in quickly reviewing the invoices from {{salutation}} {{client last name}}. to provide you with the most accurate and up-to-date information, could you please provide me with some additional details such as the date range or any specific invoice numbers? this will help me narrow down the search and find the information you need. rest assured, i'll do my best to guide you through the process and ensure you have a seamless experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7366                    | 34.6054                   | 36.3420                   | CPU        | 0.8945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  16%|█▌        | 8/50 [03:45<25:31, 36.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 8/50 (Device: CPU):\n",
      "Instruction: i can't enter a delivery address\n",
      "New Input: i can't enter a delivery address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are delivery (account_category)]\n",
      "Generated Response: i'm sorry to hear that you're having trouble entering a shipping address. let's work together to resolve this issue. to better assist you, could you please provide more details about the specific problem you are facing? are you encountering any error messages or is the system not accepting the address format? once i have a better understanding of the issue, i'll be able to provide you with the most accurate guidance and support. your satisfaction is our top priority, and we're committed to resolving this matter for you as quickly as possible.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.6904                    | 35.3520                   | 37.0424                   | CPU        | 1.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  18%|█▊        | 9/50 [04:12<22:46, 33.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 9/50 (Device: CPU):\n",
      "Instruction: can you help me place an order?\n",
      "New Input: can you help me place an order? [Classified: Intent is 'delivery_options', Category is 'contact', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in placing your order. your satisfaction is our top priority, and we want to ensure that you have a seamless experience with us. could you please provide me with more details about the specific products or services you are interested in? this will allow me to guide you through the ordering process accurately and efficiently.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7032                    | 24.7756                   | 26.4788                   | CPU        | 1.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  20%|██        | 10/50 [05:22<29:53, 44.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 10/50 (Device: CPU):\n",
      "Instruction: i do not know how i can set up my shipping address\n",
      "New Input: i do not know how i can set up my shipping address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about the process of setting up your delivery address. allow me to guide you through it step by step: 1. log in to your account on our website or app by navigating to the \"my account\" or \"profile\" section, where you'll find a dedicated section to manage your shipping addresses. you can usually find this option by clicking on your profile picture or selecting the option to \"add a new address\" from the dropdown menu. make sure to double-check all the required fields, such as street address, city, state, and zip code, to ensure accurate and up-to-date information for each address you want to add. if you encounter any difficulties or have further questions, please don't hesitate to reach out to our customer support team. they are available 24/7 at {{customer support phone number}} or through our live chat on {{website url}}. we're here to assist you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.8406                    | 68.7205                   | 70.5611                   | CPU        | 0.7188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  22%|██▏       | 11/50 [06:57<39:06, 60.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 11/50 (Device: CPU):\n",
      "Instruction: i need assistance to set my new delovery address up\n",
      "New Input: i need assistance to set my new delovery address up [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are new (account_type)]\n",
      "Generated Response: i'll take care of it! i'm here to assist you in setting up your new delivery address. to get started, please follow these steps: 1. log in to your account on our website or app by navigating to the \"my account\" or \"profile\" section, where you should find an option to manage your shipping addresses. look for the option that says \"add new address\" and click on it. you will then be prompted to enter the required details, such as your street name, city, state, and zip code. make sure to double-check all the fields before saving the necessary information for each address you want to add.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7717                    | 93.1312                   | 94.9029                   | CPU        | 1.0664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  24%|██▍       | 12/50 [07:23<31:24, 49.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 12/50 (Device: CPU):\n",
      "Instruction: help me to check what payment methods are available\n",
      "New Input: help me to check what payment methods are available [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the available payment options. to provide you with accurate information, could you please provide me with your full name, email address, and a preferred payment method? once i have these details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7563                    | 23.6898                   | 25.4462                   | CPU        | 10.1758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  26%|██▌       | 13/50 [07:45<25:26, 41.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 13/50 (Device: CPU):\n",
      "Instruction: i call to leave my feedback for ur services\n",
      "New Input: i call to leave my feedback for ur services [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: we appreciate your willingness to share your feedback about our services. your insights are valuable to us as they help us understand how we can improve and provide a better experience for our customers. please let me know if you have any specific questions or if there's anything else i can assist you with.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.6823                    | 20.3641                   | 22.0463                   | CPU        | 0.6406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  28%|██▊       | 14/50 [08:19<23:32, 39.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 14/50 (Device: CPU):\n",
      "Instruction: open freemium account for wife\n",
      "New Input: open freemium account for wife [Classified: Intent is 'create_account', Category is 'account', Entities are open fre (account_type), em (account_type), ium (account_type), account (account_type), wife (account_type)]\n",
      "Generated Response: thank you for your interest in opening a {{account type}} account with us! i'm here to assist you every step of the way. to get started, could you please provide me with your wife's full name, email address, and a preferred username? once i have these details, i'll guide you through the account creation process and ensure a smooth experience for both of you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.8357                    | 32.6992                   | 34.5349                   | CPU        | 1.0547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  30%|███       | 15/50 [08:52<21:41, 37.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 15/50 (Device: CPU):\n",
      "Instruction: i am trying to see the status of order ord-1135283\n",
      "New Input: i am trying to see the status of order ord-1135283 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-1135283 (order_number)]\n",
      "Generated Response: i'm sorry to hear that you're experiencing difficulties with canceling your order with the order number {{order number}}. your satisfaction is our top priority, and we're here to assist you every step of the way. to provide you with accurate information, could you please confirm if you made the purchase on our website or through another platform? once we have the necessary details, we'll be able to guide you through the cancellation process and ensure a smooth experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.8811                    | 30.5609                   | 32.4421                   | CPU        | 0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  32%|███▏      | 16/50 [09:34<21:52, 38.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 16/50 (Device: CPU):\n",
      "Instruction: wanna see ur payment modalities will yahelp me\n",
      "New Input: wanna see ur payment modalities will yahelp me [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in viewing our available payment methods. we strive to provide a seamless and convenient payment experience for our valued customers. to view our payment options, you can visit our website and navigate to the \"payments\" or \"checkout\" section. there you will find a comprehensive list of our accepted methods, including credit/debit cards, paypal, bank transfers, apple pay, and google wallet. if you have any further questions or need additional guidance, please don't hesitate to let me know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7234                    | 40.2152                   | 41.9386                   | CPU        | 1.0664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  34%|███▍      | 17/50 [10:15<21:39, 39.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 17/50 (Device: CPU):\n",
      "Instruction: what do i ned to do to change to the real estate account?\n",
      "New Input: what do i ned to do to change to the real estate account? [Classified: Intent is 'switch_account', Category is 'account', Entities are real (account_category), estate (account_category)]\n",
      "Generated Response: i appreciate your interest in upgrading to our {{account category}} account. let me guide you through the process step by step. first, log in to your account on our website or mobile app. once you're logged in, locate the account settings or profile section. look for an option that says \"upgrade\" or \"switch account type\". click on that option, and you'll be directed to a page where you can make the necessary changes. if you encounter any difficulties or have further questions, please don't hesitate to reach out. i'm here to assist you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.6842                    | 39.4642                   | 41.1485                   | CPU        | 1.3398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  36%|███▌      | 18/50 [10:42<19:03, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 18/50 (Device: CPU):\n",
      "Instruction: notify of errorw with online registration\n",
      "New Input: notify of errorw with online registration [Classified: Intent is 'registration_problems', Category is 'account', Entities are none]\n",
      "Generated Response: we understand that encountering errors during the registration process can be frustrating. rest assured, we are here to assist you in resolving this issue. could you please provide more details about the specific error you are facing? this will help us identify the root cause and provide you with the most suitable solution. thank you for bringing this to our attention, and we appreciate your patience as we work together to resolve this matter.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.8127                    | 25.3840                   | 27.1967                   | CPU        | 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  38%|███▊      | 19/50 [11:07<16:46, 32.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 19/50 (Device: CPU):\n",
      "Instruction: another standard accounmt for father\n",
      "New Input: another standard accounmt for father [Classified: Intent is 'contact_human_agent', Category is 'account', Entities are standard (account_type), acc (delivery_country), father (delivery_city)]\n",
      "Generated Response: i'm here to assist you in connecting with a human agent who can provide the support you need. please bear with me for a moment while i transfer you to one of our dedicated representatives who will be able to address your concerns. your satisfaction is our top priority, and we appreciate your patience as we work together to resolve this matter.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.8925                    | 22.9476                   | 24.8401                   | CPU        | 1.0586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  40%|████      | 20/50 [11:37<15:48, 31.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 20/50 (Device: CPU):\n",
      "Instruction: i need infrmation about the cancellation of my enterprise account\n",
      "New Input: i need infrmation about the cancellation of my enterprise account [Classified: Intent is 'delete_account', Category is 'account', Entities are enterprise (account_type)]\n",
      "Generated Response: i understand that you need assistance with canceling your {{account type}} account. i'm here to help you through the process. could you please provide me with your account details, such as your username or email address, so that i can assist you further? your privacy and security are of utmost importance to us, and we will handle your information with the utmost care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 2.0455                    | 27.5950                   | 29.6406                   | CPU        | 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  42%|████▏     | 21/50 [12:03<14:30, 30.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 21/50 (Device: CPU):\n",
      "Instruction: i want help ordering a few of your product\n",
      "New Input: i want help ordering a few of your product [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in placing your order for our products. your satisfaction is our top priority, and we want to ensure that you have a seamless experience with us. could you please provide me with more details about the specific products you would like to order? this will help me guide you through the ordering process more effectively.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7253                    | 24.5292                   | 26.2544                   | CPU        | 1.0156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  44%|████▍     | 22/50 [12:42<15:20, 32.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 22/50 (Device: CPU):\n",
      "Instruction: checking status of order ord-6772250\n",
      "New Input: checking status of order ord-6772250 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-6772250 (order_number)]\n",
      "Generated Response: i'm sorry to hear that you're having trouble with canceling your order with the order number {{order number}}. your satisfaction is our top priority, and we're here to assist you every step of the way. to resolve this issue, could you please provide me with some additional information? specifically, i would need to know the platform or website where you made the purchase and any relevant order details. with this information, we can quickly look into the status and provide you with an accurate update. thank you for your patience and cooperation.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 2.0340                    | 37.4814                   | 39.5154                   | CPU        | 0.6406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  46%|████▌     | 23/50 [13:12<14:20, 31.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 23/50 (Device: CPU):\n",
      "Instruction: deleting government account\n",
      "New Input: deleting government account [Classified: Intent is 'delete_account', Category is 'account', Entities are government (account_type), account (account_type)]\n",
      "Generated Response: i understand that you would like to delete your {{account type}} account. i apologize for any inconvenience this may have caused. to assist you further, could you please provide me with your account details, such as your username or email address, so that i can locate the account and guide you through the deletion process? rest assured, i'm here to support you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.9832                    | 27.5367                   | 29.5199                   | CPU        | 0.6289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  48%|████▊     | 24/50 [13:40<13:17, 30.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 24/50 (Device: CPU):\n",
      "Instruction: my experience with your brand was bad, make a consumer claim\n",
      "New Input: my experience with your brand was bad, make a consumer claim [Classified: Intent is 'complaint', Category is 'feedback', Entities are consumer (account_type)]\n",
      "Generated Response: i apologize for any negative experience you may have had with our brand. your feedback is important to us, and we want to ensure that your concerns are addressed appropriately. to assist you further, could you please provide me with more details about the specific issue you encountered? this will help us understand the situation better and take the necessary steps to resolve it.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.8208                    | 26.0860                   | 27.9068                   | CPU        | 0.6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  50%|█████     | 25/50 [14:09<12:32, 30.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 25/50 (Device: CPU):\n",
      "Instruction: wann locate order ord-3779640\n",
      "New Input: wann locate order ord-3779640 [Classified: Intent is 'change_order', Category is 'order', Entities are ord-3779640 (order_number)]\n",
      "Generated Response: we understand that you would like to locate your order with the order number {{order number}}. we apologize for any inconvenience caused. to assist you further, could you please provide more details about the specific issue you are facing? this will help us better understand the situation and provide you with an appropriate solution. your satisfaction is our top priority, and we are here to support you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7432                    | 27.0633                   | 28.8064                   | CPU        | 0.9766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  52%|█████▏    | 26/50 [15:05<15:12, 38.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 26/50 (Device: CPU):\n",
      "Instruction: i have to sde in what cases can i request a refund\n",
      "New Input: i have to sde in what cases can i request a refund [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm happy to help! i understand that you need to know the situations in which you can request refunds. here are some common scenarios where you may be eligible for a reimbursement: 1. **product/service defect:** if the product or service you received has a defect or doesn't meet the description provided, you usually have grounds to request compensation. REMEMBER, our goal is to ensure your satisfaction, and we want to make sure you have a seamless experience with our products and services. if you encounter any issues or have further questions, please don't hesitate to reach out to our customer support team. they are available {{customer support hours}} and can provide you with the necessary guidance and support to resolve the issue you're facing. we're here to assist you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.9057                    | 54.5383                   | 56.4440                   | CPU        | 0.6328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  54%|█████▍    | 27/50 [15:27<12:45, 33.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 27/50 (Device: CPU):\n",
      "Instruction: i'm trying to see at what time i can call customer support\n",
      "New Input: i'm trying to see at what time i can call customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are none]\n",
      "Generated Response: we're here to help! i take note that you would like to know the operating hours during which you can reach our dedicated customer assistance team. our team is available to assist you during our business hours, which can be found on our website at {{website url}}. if you have any other questions or concerns, please feel free to let me know. your satisfaction is our top priority, and we are committed to providing you with the support you need.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.5431                    | 21.7617                   | 22.3049                   | CPU        | 10.1797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  56%|█████▌    | 28/50 [15:53<11:25, 31.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 28/50 (Device: CPU):\n",
      "Instruction: assistance sending some feedback for ur proiducts\n",
      "New Input: assistance sending some feedback for ur proiducts [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in sending your feedback about our products and services. your opinion matters to us, and we appreciate your willingness to share your thoughts. could you please provide me with more details about the specific product or service you would like to provide feedback on? this will help me guide you through the process more effectively.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.8136                    | 24.3059                   | 26.1195                   | CPU        | 0.6289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  58%|█████▊    | 29/50 [16:29<11:22, 32.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 29/50 (Device: CPU):\n",
      "Instruction: need assistance to cancel order ord-8227420\n",
      "New Input: need assistance to cancel order ord-8227420 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8227420 (order_number)]\n",
      "Generated Response: thank you for reaching out to us for assistance with canceling your order with the order number {{order number}}. we understand that circumstances may change, and we're here to help you through the cancellation process. to proceed with your request, please provide us with some additional information, such as your account details or any specific concerns you may have. this will allow us to assist you more effectively and ensure a smooth cancellation experience. your satisfaction is our top priority, so please don't hesitate to reach out if you have any further questions or concerns.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7002                    | 34.0049                   | 35.7051                   | CPU        | 0.5273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  60%|██████    | 30/50 [17:17<12:22, 37.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 30/50 (Device: CPU):\n",
      "Instruction: need assistance cancelinv purchase ord-8042947\n",
      "New Input: need assistance cancelinv purchase ord-8042947 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8042947 (order_number)]\n",
      "Generated Response: i'm here to assist you with canceling your purchase with the order number {{order number}}. rest assured, i'll guide you through the process step by step to ensure a smooth cancellation experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.9013                    | 45.9391                   | 47.8403                   | CPU        | 1.0664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  62%|██████▏   | 31/50 [18:06<12:51, 40.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 31/50 (Device: CPU):\n",
      "Instruction: i don't know how i can download my bill #12588\n",
      "New Input: i don't know how i can download my bill #12588 [Classified: Intent is 'get_invoice', Category is 'invoice', Entities are #12588 (person_name)]\n",
      "Generated Response: i'm clearly cognizant your confusion about how to download your bill with the number {{person name}}. let me guide you through the process step by step. first, please log in to your account on our website. once you're logged in, navigate to the \"billing\" or \"invoices\" section. there, you should be able to find a list of your invoices. look for the specific invoice number, date range, or any other relevant information. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team. they will be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.8139                    | 46.8872                   | 48.7011                   | CPU        | 0.5234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  64%|██████▍   | 32/50 [18:33<10:58, 36.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 32/50 (Device: CPU):\n",
      "Instruction: i need help to see purchase ord-0521414 status\n",
      "New Input: i need help to see purchase ord-0521414 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-0521414 status (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status of your purchase with the purchase number {{order number}}. to provide you with accurate information, could you please provide me with your full name, email address, and a preferred username? once i have these details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.9615                    | 25.3462                   | 27.3077                   | CPU        | 0.9102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  66%|██████▌   | 33/50 [19:02<09:42, 34.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 33/50 (Device: CPU):\n",
      "Instruction: am i entitled to a refund according to your policy?\n",
      "New Input: am i entitled to a refund according to your policy? [Classified: Intent is 'get_refund', Category is 'refund', Entities are refund (currency_symbol)]\n",
      "Generated Response: assuredly! i understand that you would like to know the status of your refund. your satisfaction is our top priority, and i'm here to assist you every step of the way. to provide you with accurate information, could you please provide me with your order number or any other relevant details? once i have these details, i'll be able to guide you through the refund process and ensure a smooth resolution.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7055                    | 27.1243                   | 28.8298                   | CPU        | 0.5234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  68%|██████▊   | 34/50 [22:00<20:39, 77.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 34/50 (Device: CPU):\n",
      "Instruction: need to check what methods for delivery i can choose help me\n",
      "New Input: need to check what methods for delivery i can choose help me [Classified: Intent is 'delivery_options', Category is 'delivery', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in exploring the various delivery methods available to you. let's dive into the possibilities together: 1. standard shipping: this option typically takes {{date range}} business days for your package to arrive. it's perfect for non-urgent items when you have a bit more time to spare during the checkout process. you can also choose expedited shipping if you need your items a little faster, and overnight shipping for a quicker delivery if time is of the essence. this is an excellent choice for those who are not in a rush to receive their items, as it typically arrives within {{customer support hours}} to provide you with the best possible service.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.8439                    | 176.4208                   | 178.2647                   | CPU        | 11.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  70%|███████   | 35/50 [22:24<15:20, 61.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 35/50 (Device: CPU):\n",
      "Instruction: ya have an address to submit some feednack\n",
      "New Input: ya have an address to submit some feednack [Classified: Intent is 'review', Category is 'subscription', Entities are none]\n",
      "Generated Response: thank you for expressing your interest in submitting your feedback about our services! we greatly value your input as it helps us understand your needs better and continuously improve our offerings. if you have any specific questions or if there's anything else i can assist you with, please feel free to let me know. i'm here to help!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7090                    | 22.1445                   | 23.8535                   | CPU        | 0.5156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  72%|███████▏  | 36/50 [23:26<14:22, 61.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 36/50 (Device: CPU):\n",
      "Instruction: show me in which situations  can i ask for refunds\n",
      "New Input: show me in which situations  can i ask for refunds [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the situations where you can request a refund. let me provide you with the information you need: 1. **product/service defect:** if you receive a product or service that has a defect or does not meet the description provided, you are generally eligible for a reimbursement. however, it's important to note that it may take additional time for the product to be returned to you, and this can vary depending on several factors, such as the quality of the item, the vendor's policies, or the specific circumstances surrounding the defect.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.8285                    | 60.2519                   | 62.0804                   | CPU        | 4.5781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  74%|███████▍  | 37/50 [23:57<11:22, 52.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 37/50 (Device: CPU):\n",
      "Instruction: help mke chatting with a person\n",
      "New Input: help mke chatting with a person [Classified: Intent is 'contact_human_agent', Category is 'contact', Entities are none]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to chat with one of our representatives. our dedicated team is available during {{customer support hours}} to assist you with any questions or concerns you may have. please feel free to reach out to us during these hours, and we'll be more than happy to help you. is there anything specific you need assistance with?\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.9234                    | 29.3344                   | 31.2578                   | CPU        | 11.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  76%|███████▌  | 38/50 [24:22<08:48, 44.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 38/50 (Device: CPU):\n",
      "Instruction: there are errors trying to update my shipping address\n",
      "New Input: there are errors trying to update my shipping address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're encountering errors while updating your delivery address. let's work together to resolve this issue. could you please provide more details about the specific errors you are facing? this will help us better understand the problem and provide you with the most appropriate solution. your feedback is valuable to us, and we appreciate your patience and cooperation in resolving this matter.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7548                    | 22.5664                   | 24.3211                   | CPU        | 0.5195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  78%|███████▊  | 39/50 [24:48<07:07, 38.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 39/50 (Device: CPU):\n",
      "Instruction: i wnt help swapping several items of order ord-9073141\n",
      "New Input: i wnt help swapping several items of order ord-9073141 [Classified: Intent is 'change_order', Category is 'order', Entities are ord-9073141 (order_number)]\n",
      "Generated Response: we understand that you need assistance with swapping multiple items in order number {{order number}}. we're here to help! to proceed with the swap, could you please provide more details about the items you would like to swap? this will allow us to process your request accurately and efficiently. your satisfaction is our top priority, and we want to ensure that your order meets your expectations.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7055                    | 25.1435                   | 26.8490                   | CPU        | 5.4492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  80%|████████  | 40/50 [25:19<06:04, 36.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 40/50 (Device: CPU):\n",
      "Instruction: i don't know how i can order some of your article\n",
      "New Input: i don't know how i can order some of your article [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to guide you through the process of ordering our articles. to place an order, you can visit our website and navigate to the \"order history\" or \"my orders\" section. there you will find a comprehensive list of all the articles you wish to order. if you have any specific questions or need further assistance, feel free to let me know. i'll be more than happy to help you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.6830                    | 28.9198                   | 30.6028                   | CPU        | 0.5195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  82%|████████▏ | 41/50 [25:56<05:27, 36.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 41/50 (Device: CPU):\n",
      "Instruction: i don't know how i could modify my address\n",
      "New Input: i don't know how i could modify my address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about how to modify your address. let me guide you through the process step by step. first, please log in to your account on our website and navigate to the \"my account\" or \"profile\" section. from there, you should be able to find an option to edit or update your shipping information. if you encounter any difficulties or need further assistance, feel free to reach out to our customer support team.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7433                    | 34.7553                   | 36.4986                   | CPU        | 0.5156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  84%|████████▍ | 42/50 [26:21<04:24, 33.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 42/50 (Device: CPU):\n",
      "Instruction: order ord-9186353 status\n",
      "New Input: order ord-9186353 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-9186353 status (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status of your order with the order number {{order number}}. to provide you with accurate information, could you please provide me with some additional details such as your name or any other relevant information? this will help me locate the specific order and give you the most up-to-date status.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7104                    | 23.6273                   | 25.3377                   | CPU        | 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  86%|████████▌ | 43/50 [26:47<03:36, 30.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 43/50 (Device: CPU):\n",
      "Instruction: help me making a claim against your business\n",
      "New Input: help me making a claim against your business [Classified: Intent is 'complaint', Category is 'feedback', Entities are business (account_type)]\n",
      "Generated Response: i'm sorry to hear that you're facing an issue with our business. your feedback is important to us, and i want to ensure that we address your concerns appropriately. to assist you further, could you please provide me with more details about the nature of your claim? this will help us understand the situation better and take the necessary steps to resolve it.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7695                    | 23.8924                   | 25.6620                   | CPU        | 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  88%|████████▊ | 44/50 [27:24<03:16, 32.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 44/50 (Device: CPU):\n",
      "Instruction: i have got to see in which cases can i request my money back\n",
      "New Input: i have got to see in which cases can i request my money back [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the cases where you can request a refund. let me provide you with the information you need: 1. **product/service defect:** if you receive a product or service that has a defect or does not meet the description provided, you may be eligible for a reimbursement.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 2.0325                    | 35.1572                   | 37.1898                   | CPU        | 0.5234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  90%|█████████ | 45/50 [27:38<02:16, 27.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 45/50 (Device: CPU):\n",
      "Instruction: can you help me report troubles with payment?\n",
      "New Input: can you help me report troubles with payment? [Classified: Intent is 'payment_issue', Category is 'payment', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in reporting any troubles you're facing with your payment. your feedback is highly valuable to us, and we want to ensure that we address your concerns promptly and effectively. to report any payment issues, i recommend reaching out to our dedicated customer support team. they have the expertise to guide you through the process and address any concerns you may have. you can contact them through our website's live chat feature at {{website url}} or by calling {{customer support phone number}}. rest assured, we are committed to resolving this matter and ensuring a seamless payment experience for you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.5001                    | 14.0411                   | 14.5412                   | CPU        | 0.5156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  92%|█████████▏| 46/50 [27:48<01:28, 22.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 46/50 (Device: CPU):\n",
      "Instruction: i try to notify of an error with registrations\n",
      "New Input: i try to notify of an error with registrations [Classified: Intent is 'registration_problems', Category is 'account', Entities are none]\n",
      "Generated Response: thank you for bringing this error to our attention. we understand that encountering errors during the registration process can be frustrating, and we apologize for any inconvenience caused. to assist you further, could you please provide more details about the specific error you are facing? this will enable us to investigate the issue thoroughly and provide you with the most appropriate solution. your feedback is crucial in helping us improve our services and ensure a seamless registration experience for all our users.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.4635                    | 9.3181                   | 9.7816                   | CPU        | 10.1719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  94%|█████████▍| 47/50 [28:00<00:57, 19.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 47/50 (Device: CPU):\n",
      "Instruction: what do i have to do to cancel my platinum account?\n",
      "New Input: what do i have to do to cancel my platinum account? [Classified: Intent is 'delete_account', Category is 'account', Entities are platinum (account_type)]\n",
      "Generated Response: thank you for reaching out to us regarding the cancellation of your {{account type}} account. i understand that you would like to terminate your account, and i'm here to guide you through the process. to proceed with cancellation, i kindly request you to provide me with some additional information. could you please let me know which platform or service you are referring to? this will help me provide you with the most accurate and tailored assistance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.4792                    | 11.5155                   | 11.9947                   | CPU        | 0.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  96%|█████████▌| 48/50 [28:10<00:32, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 48/50 (Device: CPU):\n",
      "Instruction: how can i submit some feedback abot ur company\n",
      "New Input: how can i submit some feedback abot ur company [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: thank you for your interest in providing feedback on our company! to submit your feedback, you can visit our website and navigate to the \"contact us\" or \"feedback\" section. there, there you will find a form or a contact information to reach out to our dedicated customer support team. they will be more than happy to assist you with the feedback submission process. we value your input and look forward to hearing from you!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.4082                    | 9.2469                   | 9.6550                   | CPU        | 4.6992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  98%|█████████▊| 49/50 [28:19<00:14, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 49/50 (Device: CPU):\n",
      "Instruction: change  order ord-8335461\n",
      "New Input: change  order ord-8335461 [Classified: Intent is 'cancel_order', Category is 'order', Entities are order (order_number), ord-8335461 (order_number)]\n",
      "Generated Response: we understand that you would like to make changes to your order with the number {{order number}}. we apologize for any inconvenience caused. to assist you further, could you please provide more details about the specific changes you are interested in? this will help us ensure that we address your request accurately and efficiently. your satisfaction is our top priority, and we are here to support you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.4303                    | 9.0360                   | 9.4663                   | CPU        | 11.2227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU: 100%|██████████| 50/50 [28:27<00:00, 34.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 50/50 (Device: CPU):\n",
      "Instruction: i need assistance to send some feedback for ur produucts\n",
      "New Input: i need assistance to send some feedback for ur produucts [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: we appreciate your willingness to share your feedback about our products and services. your insights are valuable to us as they help us understand how we can better serve our customers. please let me know if there's anything specific you'd like to provide feedback on, and i'll be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.4313                    | 7.1601                   | 7.5914                   | CPU        | 0.6289\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK SUMMARY FOR CPU\n",
      "================================================================================\n",
      "Average Classification Time: 1.5178 seconds\n",
      "Average Generation Time: 32.6261 seconds\n",
      "Average Overall Time: 34.1439 seconds\n",
      "Average Memory Usage: 2.9399 MB\n",
      "\n",
      "================================================================================\n",
      "RUNNING BENCHMARK ON CUDA\n",
      "================================================================================\n",
      "\n",
      "Loading models on CUDA...\n",
      "Loading from: ../../Hybrid_Cross_Attention_Freeze\\label_encoders.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "All GPT-2 and DistilBERT layers remain frozen\n",
      "Attempting to load generation model from ../../text_generation_results_03-09-25/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   2%|▏         | 1/50 [00:06<05:00,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1/50 (Device: CUDA):\n",
      "Instruction: i do not know what i have to do to get my invoice #85632\n",
      "New Input: i do not know what i have to do to get my invoice #85632 [Classified: Intent is 'check_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about the steps to take in order to retrieve your invoice with the number #{{client last name}}. let me guide you through the process. first, please log in to your account on our website. once logged in, navigate to the \"billing\" or \"invoices\" section. there, you should be able to find a list of your invoices. look for a tab or option that says \"view invoice.\" click on that option, and you'll be directed to a page where you can view and download the invoice. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team. we're here to assist you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 1.7810                    | 4.3496                   | 6.1306                   | CUDA        | 2777.5889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   4%|▍         | 2/50 [00:08<03:07,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 2/50 (Device: CUDA):\n",
      "Instruction: transfer to live agent\n",
      "New Input: transfer to live agent [Classified: Intent is 'contact_human_agent', Category is 'contact', Entities are live (delivery_city), agent (account_type)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in transferring to a live representative. your satisfaction is our top priority, and we want to ensure that you receive the support you need. could you please provide me with more details about the specific issue or question you have? this will enable me to guide you through the transfer process and provide you with the most accurate and personalized assistance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1857                    | 2.1603                   | 2.3460                   | CUDA        | 2700.4609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   6%|▌         | 3/50 [00:10<02:32,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 3/50 (Device: CUDA):\n",
      "Instruction: i need help to see how long refunds take\n",
      "New Input: i need help to see how long refunds take [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the timeframe for refunds. the duration for processing refunds can vary depending on several factors, such as the payment method used and the specific policies of the vendor or service provider. to provide you with accurate information, could you please provide me with your order number or any other relevant details? this will enable me to check the status of your refund and give you an estimated timeframe. thank you for your cooperation, and i appreciate your patience while i gather the necessary information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1693                    | 2.2689                   | 2.4383                   | CUDA        | 2720.0435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   8%|▊         | 4/50 [00:13<02:11,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 4/50 (Device: CUDA):\n",
      "Instruction: help to pseak to customer support\n",
      "New Input: help to pseak to customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are customer (account_type)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you with your query. to ensure a seamless experience, could you please provide me with more details about the specific issue or question you have? this will help me direct you to the most appropriate department or channel to address your concerns. your satisfaction is our top priority, and we're committed to resolving any concerns you may have.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.2893                    | 2.0101                   | 2.2994                   | CUDA        | 2697.0469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  10%|█         | 5/50 [00:15<02:02,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 5/50 (Device: CUDA):\n",
      "Instruction: i do not know what to do to swtch to the premium account\n",
      "New Input: i do not know what to do to swtch to the premium account [Classified: Intent is 'switch_account', Category is 'account', Entities are premium (account_type)]\n",
      "Generated Response: i appreciate your interest in switching to our {{account type}} account, and i'm here to guide you through the process. to initiate the switch, could you please provide me with your account username or any other relevant information? once i have this information, i'll be able to assist you further and ensure a smooth transition. rest assured, your security and privacy are of utmost importance to us, so your information will be handled with the utmost care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1830                    | 2.3011                   | 2.4840                   | CUDA        | 2716.8662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  12%|█▏        | 6/50 [00:18<01:56,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 6/50 (Device: CUDA):\n",
      "Instruction: i need assistance resetting the key of my user account\n",
      "New Input: i need assistance resetting the key of my user account [Classified: Intent is 'recover_password', Category is 'account', Entities are user (account_type)]\n",
      "Generated Response: assuredly! i'm here to provide you with the assistance you need to reset your key to regain access to your account. let's work together to resolve this issue and ensure a smooth and secure reset process. could you please provide me with more details about the specific issue you're facing? this will allow me to guide you through the necessary steps and provide the most accurate guidance. your satisfaction is our top priority, and we're committed to assisting you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1406                    | 2.3563                   | 2.4969                   | CUDA        | 2719.0640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  14%|█▍        | 7/50 [00:20<01:49,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 7/50 (Device: CUDA):\n",
      "Instruction: i want help taking a quick look at the bills from john robertson\n",
      "New Input: i want help taking a quick look at the bills from john robertson [Classified: Intent is 'check_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in quickly accessing and reviewing the invoices from {{salutation}} {{client last name}}. to provide you with the most accurate and up-to-date information, could you please provide me with some additional details such as the {{date range}} or any other relevant information? this will help me locate the specific bills you are looking for and guide you through the process of retrieving them.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1917                    | 2.1728                   | 2.3645                   | CUDA        | 2712.8232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  16%|█▌        | 8/50 [00:22<01:45,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 8/50 (Device: CUDA):\n",
      "Instruction: i can't enter a delivery address\n",
      "New Input: i can't enter a delivery address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are delivery (account_category)]\n",
      "Generated Response: i'm sorry to hear that you're having trouble entering your address. let's work together to resolve this issue. to better assist you, could you please provide more details about the specific problem you are facing? are you encountering any error messages or facing any other challenges? by understanding the nature of the issue, i'll be able to provide you with the most accurate guidance and support. your satisfaction is our top priority, and we're committed to resolving this matter for you as quickly as possible.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1325                    | 2.2783                   | 2.4109                   | CUDA        | 2725.0210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  18%|█▊        | 9/50 [00:24<01:33,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 9/50 (Device: CUDA):\n",
      "Instruction: can you help me place an order?\n",
      "New Input: can you help me place an order? [Classified: Intent is 'delivery_options', Category is 'contact', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in placing your order and ensuring a seamless experience. to get started, could you please provide me with more details about the specific products or services you are interested in? this will allow me to provide you with accurate information and guide you through the ordering process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1781                    | 1.5630                   | 1.7411                   | CUDA        | 2686.9067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  20%|██        | 10/50 [00:28<01:49,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 10/50 (Device: CUDA):\n",
      "Instruction: i do not know how i can set up my shipping address\n",
      "New Input: i do not know how i can set up my shipping address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about the process of setting up your delivery address. allow me to guide you through it step by step: 1. log in to your account on our website or app by navigating to the \"my account\" or \"profile\" section, where you should find an option to manage your shipping addresses. you can usually find this option by clicking on your profile icon or a similar option in the settings or profile section of your web browser. if you encounter any difficulties or have further questions, please don't hesitate to reach out to our customer support team. they will be more than happy to assist you and provide you with the necessary instructions to set the address up successfully. remember, your satisfaction is our top priority, and we're here to help you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1540                    | 3.6015                   | 3.7555                   | CUDA        | 2792.4067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  22%|██▏       | 11/50 [00:32<01:58,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 11/50 (Device: CUDA):\n",
      "Instruction: i need assistance to set my new delovery address up\n",
      "New Input: i need assistance to set my new delovery address up [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are new (account_type)]\n",
      "Generated Response: i'm here to assist you in setting up your new delivery address. to ensure a smooth process, please follow these steps: 1. log in to your account on our website or app using your username and password. 2. navigate to the \"my account\" or \"profile\" section. 3. look for the option to manage your shipping addresses. 4. click on that option and select the desired address format. 5. fill in the required details such as street name, city, state, and zip code. 6. follow the on-screen prompts or instructions to complete the setup process. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.2412                    | 3.5344                   | 3.7756                   | CUDA        | 2790.4419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  24%|██▍       | 12/50 [00:34<01:44,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 12/50 (Device: CUDA):\n",
      "Instruction: help me to check what payment methods are available\n",
      "New Input: help me to check what payment methods are available [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the payment options available to you. to provide you with accurate information, could you please provide me with your full name, email address, and a preferred payment method? once i have these details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1699                    | 1.8636                   | 2.0335                   | CUDA        | 2696.1968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  26%|██▌       | 13/50 [00:35<01:29,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 13/50 (Device: CUDA):\n",
      "Instruction: i call to leave my feedback for ur services\n",
      "New Input: i call to leave my feedback for ur services [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: thank you for expressing your interest in providing feedback on our services! your feedback is highly valuable to us as it helps us understand our customers' experiences and make necessary improvements. please let me know if you have any specific questions or if there's anything else i can assist you with.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1911                    | 1.4817                   | 1.6728                   | CUDA        | 2685.1992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  28%|██▊       | 14/50 [00:38<01:30,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 14/50 (Device: CUDA):\n",
      "Instruction: open freemium account for wife\n",
      "New Input: open freemium account for wife [Classified: Intent is 'create_account', Category is 'account', Entities are open fre (account_type), em (account_type), ium (account_type), account (account_type), wife (account_type)]\n",
      "Generated Response: thank you for your interest in opening a {{account type}} account with us! i'm here to assist you every step of the way. to get started, could you please provide me with your wife's full name, email address, and a preferred username? once i have these details, i'll guide you through the account creation process, ensuring a seamless experience for both of you. rest assured, we prioritize the security and privacy of our customers' information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.2271                    | 2.4884                   | 2.7156                   | CUDA        | 2755.3589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  30%|███       | 15/50 [00:41<01:29,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 15/50 (Device: CUDA):\n",
      "Instruction: i am trying to see the status of order ord-1135283\n",
      "New Input: i am trying to see the status of order ord-1135283 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-1135283 (order_number)]\n",
      "Generated Response: i'm sorry to hear that you're having trouble viewing the progress of your order with the order number {{order number}}. i understand how important it is for you to have visibility on this matter. to assist you further, could you please provide me with some additional information? specifically, i would like to know the platform or website where you made the purchase. this will allow me to provide you with accurate and up-to-date information. your satisfaction is our top priority, and we're here to support you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1384                    | 2.4976                   | 2.6360                   | CUDA        | 2737.7881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  32%|███▏      | 16/50 [00:43<01:20,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 16/50 (Device: CUDA):\n",
      "Instruction: wanna see ur payment modalities will yahelp me\n",
      "New Input: wanna see ur payment modalities will yahelp me [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in viewing our available payment methods. to provide you with accurate information, could you please provide me with your full name, email address, and a preferred payment method? once i have these details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1850                    | 1.7506                   | 1.9355                   | CUDA        | 2694.5947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  34%|███▍      | 17/50 [00:46<01:23,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 17/50 (Device: CUDA):\n",
      "Instruction: what do i ned to do to change to the real estate account?\n",
      "New Input: what do i ned to do to change to the real estate account? [Classified: Intent is 'switch_account', Category is 'account', Entities are real (account_category), estate (account_category)]\n",
      "Generated Response: i appreciate your interest in upgrading to our {{account category}} account. let me guide you through the process step by step. first, log in to your account on our website or mobile app. once you're logged in, locate the account settings or profile section. look for an option that says \"upgrade\" or \"switch account type.\" click on that option, and it will direct you to a page where you can make the necessary changes. if you encounter any difficulties or have further questions, please don't hesitate to reach out. i'm here to assist you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1451                    | 2.7471                   | 2.8922                   | CUDA        | 2755.6509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  36%|███▌      | 18/50 [00:48<01:18,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 18/50 (Device: CUDA):\n",
      "Instruction: notify of errorw with online registration\n",
      "New Input: notify of errorw with online registration [Classified: Intent is 'registration_problems', Category is 'account', Entities are none]\n",
      "Generated Response: we understand that encountering errors during the registration process can be frustrating, and we apologize for any inconvenience caused. your feedback is crucial in helping us improve our services and ensure a seamless registration experience for all our users. to assist you further, could you please provide more details about the specific error you are facing? this will enable us to investigate the issue thoroughly and provide you with the most suitable solution. thank you for bringing this to our attention.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1616                    | 2.1642                   | 2.3258                   | CUDA        | 2712.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  38%|███▊      | 19/50 [00:50<01:10,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 19/50 (Device: CUDA):\n",
      "Instruction: another standard accounmt for father\n",
      "New Input: another standard accounmt for father [Classified: Intent is 'contact_human_agent', Category is 'account', Entities are standard (account_type), acc (delivery_country), father (delivery_city)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in connecting with a human agent who can provide the support you need. please bear with me for a moment while i transfer you to one of our dedicated representatives who will be able to address your concerns. your satisfaction is our top priority, and we appreciate your patience as we work together to resolve this matter.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1481                    | 1.6554                   | 1.8035                   | CUDA        | 2708.5518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  40%|████      | 20/50 [00:51<01:03,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 20/50 (Device: CUDA):\n",
      "Instruction: i need infrmation about the cancellation of my enterprise account\n",
      "New Input: i need infrmation about the cancellation of my enterprise account [Classified: Intent is 'delete_account', Category is 'account', Entities are enterprise (account_type)]\n",
      "Generated Response: i'm here to assist you with canceling your {{account type}} account. your satisfaction is our top priority, and we want to ensure that you have a seamless experience with us. could you please provide me with more details about your specific query or concern? this will help us better understand your needs and provide the most accurate guidance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1898                    | 1.5315                   | 1.7213                   | CUDA        | 2696.9038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  42%|████▏     | 21/50 [00:53<00:55,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 21/50 (Device: CUDA):\n",
      "Instruction: i want help ordering a few of your product\n",
      "New Input: i want help ordering a few of your product [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for your interest in ordering our products! i'm here to assist you with the ordering process. could you please provide me with more details about the specific products you would like to order? this will help me guide you through the necessary steps and ensure a seamless experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1420                    | 1.3501                   | 1.4922                   | CUDA        | 2686.2852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  44%|████▍     | 22/50 [00:56<01:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 22/50 (Device: CUDA):\n",
      "Instruction: checking status of order ord-6772250\n",
      "New Input: checking status of order ord-6772250 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-6772250 (order_number)]\n",
      "Generated Response: i'm sorry to hear that you're experiencing difficulties while trying to check the status for your order with the order number {{order number}}. i understand how important it is for you to have a seamless experience with us. to assist you further, could you please provide me with some additional information? specifically, i would like to know the platform or website where you made the purchase. this will allow me to provide you with accurate and up-to-date information. your satisfaction is our top priority, and we're here to support you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1652                    | 2.6128                   | 2.7780                   | CUDA        | 2736.7461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  46%|████▌     | 23/50 [00:58<00:59,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 23/50 (Device: CUDA):\n",
      "Instruction: deleting government account\n",
      "New Input: deleting government account [Classified: Intent is 'delete_account', Category is 'account', Entities are government (account_type), account (account_type)]\n",
      "Generated Response: i understand that you would like to delete your {{account type}} account, and i'm here to assist you with that. to proceed with the deletion, please provide me with your account details, such as your username or email address, so that i can locate the account and guide you through the necessary steps.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1710                    | 2.0403                   | 2.2113                   | CUDA        | 2711.4741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  48%|████▊     | 24/50 [01:00<00:53,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 24/50 (Device: CUDA):\n",
      "Instruction: my experience with your brand was bad, make a consumer claim\n",
      "New Input: my experience with your brand was bad, make a consumer claim [Classified: Intent is 'complaint', Category is 'feedback', Entities are consumer (account_type)]\n",
      "Generated Response: i'm sorry to hear that your experience was unsatisfactory. your feedback is important to us, and i want to ensure that we address your concerns effectively. could you please provide me with more details about the specific issue you encountered? this will help us understand the situation better and take the necessary steps to resolve it.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1491                    | 1.5660                   | 1.7151                   | CUDA        | 2696.9043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  50%|█████     | 25/50 [01:02<00:51,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 25/50 (Device: CUDA):\n",
      "Instruction: wann locate order ord-3779640\n",
      "New Input: wann locate order ord-3779640 [Classified: Intent is 'change_order', Category is 'order', Entities are ord-3779640 (order_number)]\n",
      "Generated Response: we understand that you would like to locate your order with the order number {{order number}}. we apologize for any inconvenience caused. to assist you further, could you please provide more details about the specific issue you are facing? this will help us better understand the situation and provide you with an appropriate solution. your satisfaction is our top priority, and we are here to support you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.2133                    | 1.8540                   | 2.0673                   | CUDA        | 2703.4932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  52%|█████▏    | 26/50 [01:06<01:02,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 26/50 (Device: CUDA):\n",
      "Instruction: i have to sde in what cases can i request a refund\n",
      "New Input: i have to sde in what cases can i request a refund [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the situations in which you may be eligible to request refunds. let me provide you with the information you need to understand the different scenarios: 1. **product/service defect:** if you receive a product or service that has a defect or does not meet the description provided, you are typically entitled to a reimbursement. however, it's important to note that the exact nature of the defect may depend on several factors, such as the manufacturing process, the vendor's refund policy, and the specific circumstances surrounding the product's delivery, so it may vary slightly from the company's official statement or the customer support provided by the manufacturer.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1575                    | 3.7483                   | 3.9058                   | CUDA        | 2802.6133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  54%|█████▍    | 27/50 [01:08<00:57,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 27/50 (Device: CUDA):\n",
      "Instruction: i'm trying to see at what time i can call customer support\n",
      "New Input: i'm trying to see at what time i can call customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are none]\n",
      "Generated Response: we're here to help! i take note that you would like to know the operating hours during which you can reach our dedicated customer assistance team. our team is available to assist you during our business hours, which can be found on our website at {{website url}}. if you have any other questions or concerns, feel free to let me know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1657                    | 2.0670                   | 2.2327                   | CUDA        | 2715.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  56%|█████▌    | 28/50 [01:11<00:57,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 28/50 (Device: CUDA):\n",
      "Instruction: assistance sending some feedback for ur proiducts\n",
      "New Input: assistance sending some feedback for ur proiducts [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for expressing your interest in providing feedback on our {{order number}}. we greatly value your input as it helps us improve our products and services. to send your feedback, you can visit our website and navigate to the \"contact us\" or \"feedback\" section. there, simply fill out the form with your contact information, such as your name, email address, and preferred method of communication. our dedicated team will be more than happy to assist you with the feedback submission process.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.2289                    | 2.6320                   | 2.8609                   | CUDA        | 2734.6660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  58%|█████▊    | 29/50 [01:13<00:53,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 29/50 (Device: CUDA):\n",
      "Instruction: need assistance to cancel order ord-8227420\n",
      "New Input: need assistance to cancel order ord-8227420 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8227420 (order_number)]\n",
      "Generated Response: i understand your need for assistance in canceling your order with the order number {{order number}}. rest assured, i'm here to guide you through the process and ensure a smooth cancellation experience. to proceed, could you please provide me with some additional details such as your account information or any specific concerns you have regarding the cancellation? this will allow me to provide you with accurate and tailored assistance. your satisfaction is our top priority, and we're committed to resolving this matter for you as quickly as possible.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.2091                    | 2.2289                   | 2.4380                   | CUDA        | 2730.4033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  60%|██████    | 30/50 [01:15<00:47,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 30/50 (Device: CUDA):\n",
      "Instruction: need assistance cancelinv purchase ord-8042947\n",
      "New Input: need assistance cancelinv purchase ord-8042947 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8042947 (order_number)]\n",
      "Generated Response: i'm sorry to hear that you're having trouble canceling your purchase with the order number {{order number}}. i understand how important it is for you to have control over the cancellation process. to assist you further, could you please provide me with more details about the specific issue you are facing? this will help me guide you through the necessary steps and ensure a smooth cancellation experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1656                    | 1.7251                   | 1.8907                   | CUDA        | 2701.6475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  62%|██████▏   | 31/50 [01:18<00:50,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 31/50 (Device: CUDA):\n",
      "Instruction: i don't know how i can download my bill #12588\n",
      "New Input: i don't know how i can download my bill #12588 [Classified: Intent is 'get_invoice', Category is 'invoice', Entities are #12588 (person_name)]\n",
      "Generated Response: i'm clearly cognizant your confusion about how to download your bill with the number #{{person name}}. let me guide you through the process step by step. first, please log in to your account on our website. once you're logged in, navigate to the \"billing\" or \"invoices\" section. from there, you should be able to find a list of your invoices. look for a download or print button next to each invoice, and click on it. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.2735                    | 3.0356                   | 3.3091                   | CUDA        | 2760.7104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  64%|██████▍   | 32/50 [01:20<00:43,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 32/50 (Device: CUDA):\n",
      "Instruction: i need help to see purchase ord-0521414 status\n",
      "New Input: i need help to see purchase ord-0521414 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-0521414 status (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status of your purchase with the purchase number {{order number}}. to view the current status, please visit the '{{order status}}' section on our website. if you have any other questions or need further assistance, feel free to let me know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1616                    | 1.7729                   | 1.9345                   | CUDA        | 2702.4824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  66%|██████▌   | 33/50 [01:23<00:41,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 33/50 (Device: CUDA):\n",
      "Instruction: am i entitled to a refund according to your policy?\n",
      "New Input: am i entitled to a refund according to your policy? [Classified: Intent is 'get_refund', Category is 'refund', Entities are refund (currency_symbol)]\n",
      "Generated Response: i'm happy to help! i understand that you would like to know the status of your refund. your satisfaction is our top priority, and we want to ensure that your concerns are addressed promptly and effectively. to provide you with accurate information, could you please provide me with your order number or any other relevant details? this will enable me to assist you further in obtaining the refund you deserve.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.2487                    | 2.1951                   | 2.4438                   | CUDA        | 2718.4995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  68%|██████▊   | 34/50 [01:33<01:16,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 34/50 (Device: CUDA):\n",
      "Instruction: need to check what methods for delivery i can choose help me\n",
      "New Input: need to check what methods for delivery i can choose help me [Classified: Intent is 'delivery_options', Category is 'delivery', Entities are none]\n",
      "Generated Response: i'll take care of it! i'm here to assist you in exploring the various delivery methods we offer. let's dive into the possibilities together. here are some of the delivery options we have available: 1. standard shipping: this option typically takes {{date range}} business days for your package to arrive. it's perfect for non-urgent items when you don't have an urgent need. if you need your items a bit faster, you can also opt for expedited shipping, which typically arrives within {{customer support hours}}. you'll be more than happy to receive your order as soon as possible, ensuring a hassle-free delivery experience for you and your loved ones in {{delivery city}}, {{shipping city}}.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1913                    | 10.0731                   | 10.2644                   | CUDA        | 3089.8149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  70%|███████   | 35/50 [01:35<01:00,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 35/50 (Device: CUDA):\n",
      "Instruction: ya have an address to submit some feednack\n",
      "New Input: ya have an address to submit some feednack [Classified: Intent is 'review', Category is 'subscription', Entities are none]\n",
      "Generated Response: thank you for your interest in submitting feedback about our products and services! we appreciate your willingness to share your thoughts and experiences. to provide feedback, you can visit our website and navigate to the \"contact us\" or \"feedback\" section. there, there you will find a form or contact information to reach out to our dedicated customer support team who will be more than happy to assist you with submitting your feedback. we value your input and look forward to hearing from you!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1692                    | 2.1669                   | 2.3361                   | CUDA        | 2711.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  72%|███████▏  | 36/50 [01:39<00:56,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 36/50 (Device: CUDA):\n",
      "Instruction: show me in which situations  can i ask for refunds\n",
      "New Input: show me in which situations  can i ask for refunds [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the situations where you can request a refund. let me provide you with the information you need: 1. **product/service defect:** if you receive a product or service that has a defect or does not meet the description provided, you are generally eligible for a reimbursement. however, it's important to note that it may take additional time for the product to reflect in your hands and be returned to you as soon as possible after the purchase has been made. this can vary depending on various factors, such as the type of product, the vendor's refund policy, and the specific circumstances surrounding the defect. if your satisfaction is of the utmost importance, i recommend reaching out to our customer support team. they will be more than happy to guide you through the refund process and ensure that your concerns are addressed promptly and effectively.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1734                    | 3.8760                   | 4.0495                   | CUDA        | 2799.4932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  74%|███████▍  | 37/50 [01:41<00:44,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 37/50 (Device: CUDA):\n",
      "Instruction: help mke chatting with a person\n",
      "New Input: help mke chatting with a person [Classified: Intent is 'contact_human_agent', Category is 'contact', Entities are none]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to chat with someone. our team of dedicated human representatives is here to assist you. please allow me a moment to connect you with one of our representatives who will be more than happy to help you further. your satisfaction is our top priority, and we appreciate your patience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.2015                    | 1.7147                   | 1.9162                   | CUDA        | 2694.5938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  76%|███████▌  | 38/50 [01:43<00:36,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 38/50 (Device: CUDA):\n",
      "Instruction: there are errors trying to update my shipping address\n",
      "New Input: there are errors trying to update my shipping address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're encountering errors while updating your delivery address. our team is here to assist you in resolving this issue. could you please provide more details about the specific errors you are facing? this will help us better understand the problem and provide you with the most accurate solution. thank you for bringing this to our attention, and we appreciate your patience as we work together to resolve this matter.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1634                    | 1.8851                   | 2.0485                   | CUDA        | 2698.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  78%|███████▊  | 39/50 [01:45<00:29,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 39/50 (Device: CUDA):\n",
      "Instruction: i wnt help swapping several items of order ord-9073141\n",
      "New Input: i wnt help swapping several items of order ord-9073141 [Classified: Intent is 'change_order', Category is 'order', Entities are ord-9073141 (order_number)]\n",
      "Generated Response: we understand that you need assistance with swapping multiple items in order number {{order number}}. we're here to help! to proceed with the swap, could you please provide more details about the items you would like to exchange? this will allow us to process your request accurately and efficiently. your satisfaction is our top priority, and we want to ensure that your order meets your expectations.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1377                    | 1.7840                   | 1.9218                   | CUDA        | 2710.9150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  80%|████████  | 40/50 [01:48<00:26,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 40/50 (Device: CUDA):\n",
      "Instruction: i don't know how i can order some of your article\n",
      "New Input: i don't know how i can order some of your article [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for your interest in ordering our articles! i'm here to guide you through the process. to place an order, you can visit our website and browse through our wide range of articles. once you find the articles you wish to purchase, simply add them to your cart. if you have any specific questions or need further assistance, feel free to reach out to our customer support team. they will be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1563                    | 2.4566                   | 2.6129                   | CUDA        | 2711.4893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  82%|████████▏ | 41/50 [01:50<00:23,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 41/50 (Device: CUDA):\n",
      "Instruction: i don't know how i could modify my address\n",
      "New Input: i don't know how i could modify my address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about how to modify your address. let me guide you through the process step by step. first, please log in to your account on our website and navigate to the \"my account\" or \"profile\" section. from there, you should be able to find an option to edit your shipping address or add a new one. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team for assistance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1523                    | 2.2772                   | 2.4295                   | CUDA        | 2714.9038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  84%|████████▍ | 42/50 [01:53<00:19,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 42/50 (Device: CUDA):\n",
      "Instruction: order ord-9186353 status\n",
      "New Input: order ord-9186353 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-9186353 status (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status of your order with the order number {{order number}}. to provide you with accurate information, could you please confirm if you made the purchase on our website or through another platform? once i have the necessary details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1489                    | 2.0251                   | 2.1740                   | CUDA        | 2706.2710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  86%|████████▌ | 43/50 [01:55<00:16,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 43/50 (Device: CUDA):\n",
      "Instruction: help me making a claim against your business\n",
      "New Input: help me making a claim against your business [Classified: Intent is 'complaint', Category is 'feedback', Entities are business (account_type)]\n",
      "Generated Response: i'm here to provide you with the assistance you need to make your claim. your satisfaction is our top priority, and we want to ensure that your concerns are addressed promptly and effectively. to begin the process, could you please provide me with more details about the issue you're facing? this will help us understand the situation better and work towards a resolution that meets your needs. thank you for reaching out and giving us the opportunity to assist you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1452                    | 2.0640                   | 2.2092                   | CUDA        | 2708.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  88%|████████▊ | 44/50 [01:59<00:17,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 44/50 (Device: CUDA):\n",
      "Instruction: i have got to see in which cases can i request my money back\n",
      "New Input: i have got to see in which cases can i request my money back [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm happy to help! i understand that you would like to have a clear understanding of the cases where you can request a refund. let me provide you with the information you need: firstly, it's important to note that the exact nature of your refund request may vary depending on several factors, such as the type of product or service, the payment method used, and the specific circumstances surrounding your request. to provide accurate information, i recommend reaching out to our customer support team directly. they will be able to assist you further and guide you through the refund process. please provide them with all the relevant details regarding your specific situation, including any relevant order or transaction numbers or any other relevant information. our dedicated team is available {{customer support hours}} to address any questions or concerns you may have along the way. rest assured, we are here to support you throughout this process and ensure your satisfaction.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1890                    | 4.1199                   | 4.3089                   | CUDA        | 2814.0552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  90%|█████████ | 45/50 [02:02<00:15,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 45/50 (Device: CUDA):\n",
      "Instruction: can you help me report troubles with payment?\n",
      "New Input: can you help me report troubles with payment? [Classified: Intent is 'payment_issue', Category is 'payment', Entities are none]\n",
      "Generated Response: unquestionably! i'm here to assist you in reporting any troubles you're facing with your payment. your feedback is crucial to us, and we want to ensure that your concerns are addressed promptly and effectively. to report any payment issues, i recommend reaching out to our dedicated customer support team. they have the expertise to guide you through the process and address any concerns you may have. you can contact them through our helpline at {{customer support phone number}} or by visiting our website and initiating a live chat session with one of our representatives. rest assured, we are committed to resolving this matter and ensuring a seamless payment experience for you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1640                    | 2.9461                   | 3.1100                   | CUDA        | 2756.2690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  92%|█████████▏| 46/50 [02:04<00:11,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 46/50 (Device: CUDA):\n",
      "Instruction: i try to notify of an error with registrations\n",
      "New Input: i try to notify of an error with registrations [Classified: Intent is 'registration_problems', Category is 'account', Entities are none]\n",
      "Generated Response: thank you for bringing this error to our attention. we understand that encountering errors during the registration process can be frustrating, and we apologize for any inconvenience caused. to assist you further, could you please provide more details about the specific error you are facing? this will enable us to investigate the issue thoroughly and provide you with the appropriate solution. your feedback is highly valuable to us as it helps us improve our services and ensure a seamless registration experience for all our users.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.2188                    | 2.0479                   | 2.2667                   | CUDA        | 2710.9912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  94%|█████████▍| 47/50 [02:08<00:08,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 47/50 (Device: CUDA):\n",
      "Instruction: what do i have to do to cancel my platinum account?\n",
      "New Input: what do i have to do to cancel my platinum account? [Classified: Intent is 'delete_account', Category is 'account', Entities are platinum (account_type)]\n",
      "Generated Response: thank you for reaching out to us regarding the cancellation of your {{account type}} account. we understand that circumstances may change, and we're here to assist you every step of the way. to proceed with cancellation, please follow these steps: 1. log in to your account on our {{online company portal info}} using your username and password. 2. navigate to the account settings or profile section. 3. look for the option to manage or cancel your subscription. 4. follow the prompts or instructions provided to confirm your cancellation. if you encounter any difficulties or have further questions, feel free to let us know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1648                    | 3.2925                   | 3.4572                   | CUDA        | 2771.2598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  96%|█████████▌| 48/50 [02:10<00:05,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 48/50 (Device: CUDA):\n",
      "Instruction: how can i submit some feedback abot ur company\n",
      "New Input: how can i submit some feedback abot ur company [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: thank you for your interest in providing feedback on our company! to submit your feedback, you can visit our website and navigate to the \"contact us\" or \"feedback\" page. there you will find a form or a contact information to reach out to our dedicated customer support team. they will be more than happy to assist you in submitting your valuable feedback. we appreciate your time and look forward to hearing from you!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1760                    | 2.3215                   | 2.4975                   | CUDA        | 2711.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  98%|█████████▊| 49/50 [02:12<00:02,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 49/50 (Device: CUDA):\n",
      "Instruction: change  order ord-8335461\n",
      "New Input: change  order ord-8335461 [Classified: Intent is 'cancel_order', Category is 'order', Entities are order (order_number), ord-8335461 (order_number)]\n",
      "Generated Response: i understand that you need assistance with changing order number {{order number}}. i'm here to help you with that. could you please provide me with the specific details of the changes you would like to make to your order? this will allow me to guide you through the process and ensure a seamless experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1580                    | 1.6634                   | 1.8215                   | CUDA        | 2700.4033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA: 100%|██████████| 50/50 [02:14<00:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 50/50 (Device: CUDA):\n",
      "Instruction: i need assistance to send some feedback for ur produucts\n",
      "New Input: i need assistance to send some feedback for ur produucts [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: we appreciate your willingness to share your feedback about our products and services. your insights play a crucial role in helping us improve and deliver a better experience to our valued customers. please let me know if you have any specific questions or if there's anything else i can assist you with.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "Hybrid Cross Attention | 0.1593                    | 1.8184                   | 1.9777                   | CUDA        | 2696.9136\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK SUMMARY FOR CUDA\n",
      "================================================================================\n",
      "Average Classification Time: 0.2105 seconds\n",
      "Average Generation Time: 2.4827 seconds\n",
      "Average Overall Time: 2.6932 seconds\n",
      "Average Memory Usage: 2731.7747 MB\n",
      "CPU results saved to performance_results_cpu.json\n",
      "GPU results saved to performance_results_gpu.json\n",
      "Performance log saved to performance_log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance comparison plots saved to performance_comparison.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2Model, DistilBertModel, GPT2TokenizerFast, DistilBertTokenizerFast,GPT2Tokenizer,GPT2LMHeadModel\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import gc  # Add this import at the top\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2Model, DistilBertModel, GPT2TokenizerFast, DistilBertTokenizerFast\n",
    "from typing import Optional, Dict  # Add this import for type hints\n",
    "\n",
    "def log_to_file(message):\n",
    "    \"\"\"Helper function for logging\"\"\"\n",
    "    print(message)\n",
    "\n",
    "class CrossAttentionFusionLayer(nn.Module):\n",
    "    def __init__(self, gpt2_dim: int, bert_dim: int, output_dim: int, dropout_rate: float, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.gpt2_proj = nn.Linear(gpt2_dim, output_dim)\n",
    "        self.bert_proj = nn.Linear(bert_dim, output_dim)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=output_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, gpt2_features: torch.Tensor, bert_features: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        gpt2_proj = self.gpt2_proj(gpt2_features)  # [batch_size, seq_len, output_dim]\n",
    "        bert_proj = self.bert_proj(bert_features)  # [batch_size, seq_len, output_dim]\n",
    "        attn_mask = attention_mask.float().masked_fill(attention_mask == 0, float('-inf')).masked_fill(attention_mask == 1, 0)\n",
    "        fused_features, _ = self.cross_attention(\n",
    "            query=gpt2_proj,\n",
    "            key=bert_proj,\n",
    "            value=bert_proj,\n",
    "            key_padding_mask=attention_mask == 0\n",
    "        )\n",
    "        fused_features = self.dropout(fused_features) + gpt2_proj  # Residual connection\n",
    "        return self.layer_norm(fused_features)\n",
    "\n",
    "class HybridGPT2DistilBERTMultiTask(nn.Module):\n",
    "    def __init__(self, num_intents: int, num_categories: int, num_ner_labels: int,\n",
    "                 dropout_rate: float, loss_weights: Optional[Dict[str, float]] = None,\n",
    "                 ner_class_weights: Optional[torch.Tensor] = None,\n",
    "                 category_class_weights: Optional[torch.Tensor] = None,\n",
    "                 intent_class_weights: Optional[torch.Tensor] = None):\n",
    "        super().__init__()\n",
    "        log_to_file(\"Initializing model...\")\n",
    "        self.gpt2_config = GPT2Config.from_pretrained('gpt2')\n",
    "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        # Freeze all layers\n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.distilbert.parameters():\n",
    "            param.requires_grad = False\n",
    "        log_to_file(\"All GPT-2 and DistilBERT layers remain frozen\")\n",
    "\n",
    "        gpt2_dim = self.gpt2_config.n_embd\n",
    "        bert_dim = self.distilbert.config.hidden_size\n",
    "        hidden_size = gpt2_dim  # Keeping output dim same as GPT-2 for consistency\n",
    "\n",
    "        self.fusion_layer = CrossAttentionFusionLayer(gpt2_dim, bert_dim, hidden_size, dropout_rate)\n",
    "\n",
    "        self.intent_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_intents)\n",
    "        )\n",
    "        self.category_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_categories)\n",
    "        )\n",
    "        self.ner_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_ner_labels)\n",
    "        )\n",
    "\n",
    "        # Set default loss weights if not provided\n",
    "        self.loss_weights = loss_weights or {\n",
    "            'intent': 0.3,\n",
    "            'category': 0.3,\n",
    "            'ner': 0.4\n",
    "        }\n",
    "\n",
    "        # Loss functions with class weights\n",
    "        self.intent_loss_fn = nn.CrossEntropyLoss(weight=intent_class_weights) if intent_class_weights is not None else nn.CrossEntropyLoss()\n",
    "        self.category_loss_fn = nn.CrossEntropyLoss(weight=category_class_weights) if category_class_weights is not None else nn.CrossEntropyLoss()\n",
    "        self.ner_loss_fn = nn.CrossEntropyLoss(weight=ner_class_weights) if ner_class_weights is not None else nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, gpt2_input_ids: torch.Tensor, gpt2_attention_mask: torch.Tensor,\n",
    "                distilbert_input_ids: torch.Tensor, distilbert_attention_mask: torch.Tensor,\n",
    "                intent_labels: Optional[torch.Tensor] = None,\n",
    "                category_labels: Optional[torch.Tensor] = None,\n",
    "                ner_labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        gpt2_outputs = self.gpt2(input_ids=gpt2_input_ids, attention_mask=gpt2_attention_mask)\n",
    "        distilbert_outputs = self.distilbert(input_ids=distilbert_input_ids, attention_mask=distilbert_attention_mask)\n",
    "\n",
    "        gpt2_features = gpt2_outputs.last_hidden_state\n",
    "        bert_features = distilbert_outputs.last_hidden_state\n",
    "\n",
    "        fused_features = self.fusion_layer(gpt2_features, bert_features, gpt2_attention_mask)\n",
    "\n",
    "        batch_size = fused_features.shape[0]\n",
    "        sequence_lengths = gpt2_attention_mask.sum(dim=1) - 1\n",
    "        last_token_indexes = sequence_lengths.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, fused_features.shape[-1])\n",
    "        sequence_repr = torch.gather(fused_features, 1, last_token_indexes).squeeze(1)\n",
    "\n",
    "        intent_logits = self.intent_head(sequence_repr)\n",
    "        category_logits = self.category_head(sequence_repr)\n",
    "        ner_logits = self.ner_head(fused_features)\n",
    "\n",
    "        output_dict = {\n",
    "            'intent_logits': intent_logits,\n",
    "            'category_logits': category_logits,\n",
    "            'ner_logits': ner_logits\n",
    "        }\n",
    "\n",
    "        if all(label is not None for label in [intent_labels, category_labels, ner_labels]):\n",
    "            intent_loss = self.intent_loss_fn(intent_logits, intent_labels)\n",
    "            category_loss = self.category_loss_fn(category_logits, category_labels)\n",
    "            active_loss = gpt2_attention_mask.view(-1) == 1\n",
    "            active_logits = ner_logits.view(-1, ner_logits.size(-1))[active_loss]\n",
    "            active_labels = ner_labels.view(-1)[active_loss]\n",
    "            ner_loss = self.ner_loss_fn(active_logits, active_labels)\n",
    "\n",
    "            total_loss = (self.loss_weights['intent'] * intent_loss +\n",
    "                          self.loss_weights['category'] * category_loss +\n",
    "                          self.loss_weights['ner'] * ner_loss)\n",
    "\n",
    "            output_dict.update({\n",
    "                'loss': total_loss,\n",
    "                'intent_loss': intent_loss,\n",
    "                'category_loss': category_loss,\n",
    "                'ner_loss': ner_loss\n",
    "            })\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "def inference_hybrid(model, text, gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length, device):\n",
    "    \"\"\"Run inference with the HybridGPT2DistilBERTMultiTask model with confidence scores as decimals\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    gpt2_encoding = gpt2_tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    distilbert_encoding = distilbert_tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    inputs = {\n",
    "        \"gpt2_input_ids\": gpt2_encoding[\"input_ids\"].to(device),\n",
    "        \"gpt2_attention_mask\": gpt2_encoding[\"attention_mask\"].to(device),\n",
    "        \"distilbert_input_ids\": distilbert_encoding[\"input_ids\"].to(device),\n",
    "        \"distilbert_attention_mask\": distilbert_encoding[\"attention_mask\"].to(device)\n",
    "    }\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get softmax probabilities for confidence scores\n",
    "    intent_logits = outputs[\"intent_logits\"]\n",
    "    category_logits = outputs[\"category_logits\"]\n",
    "    ner_logits = outputs[\"ner_logits\"]\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    intent_probs = torch.nn.functional.softmax(intent_logits, dim=-1)[0]\n",
    "    category_probs = torch.nn.functional.softmax(category_logits, dim=-1)[0]\n",
    "    ner_probs = torch.nn.functional.softmax(ner_logits, dim=-1)\n",
    "\n",
    "    # Get top predictions and their confidences\n",
    "    intent_pred = torch.argmax(intent_probs).cpu().item()\n",
    "    intent_confidence = intent_probs[intent_pred].cpu().item()  # As decimal (0.0-1.0)\n",
    "\n",
    "    category_pred = torch.argmax(category_probs).cpu().item()\n",
    "    category_confidence = category_probs[category_pred].cpu().item()  # As decimal (0.0-1.0)\n",
    "\n",
    "    ner_preds = torch.argmax(ner_probs, dim=-1).cpu().numpy()[0]\n",
    "    ner_confidences = torch.max(ner_probs, dim=-1)[0][0].cpu().numpy()  # Get max probability for each token\n",
    "\n",
    "    # Map to labels\n",
    "    intent_decoder = {v: k for k, v in label_encoders[\"intent_encoder\"].items()}\n",
    "    category_decoder = {v: k for k, v in label_encoders[\"category_encoder\"].items()}\n",
    "    ner_decoder = {v: k for k, v in label_encoders[\"ner_label_encoder\"].items()}\n",
    "\n",
    "    intent_label = intent_decoder[intent_pred]\n",
    "    category_label = category_decoder[category_pred]\n",
    "\n",
    "    tokens = gpt2_tokenizer.convert_ids_to_tokens(inputs[\"gpt2_input_ids\"][0].tolist())\n",
    "    seq_len = int(inputs[\"gpt2_attention_mask\"][0].sum().item())\n",
    "    ner_labels = [ner_decoder[pred] for pred in ner_preds[:seq_len]]\n",
    "\n",
    "    # Extract entities from NER labels with confidence\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    entity_tokens = []\n",
    "    entity_confidences = []\n",
    "    entity_type = None\n",
    "\n",
    "    for i, (token, label, confidence) in enumerate(zip(tokens[:seq_len], ner_labels, ner_confidences[:seq_len])):\n",
    "        if label.startswith(\"B-\"):\n",
    "            # If we were tracking an entity, save it before starting a new one\n",
    "            if current_entity is not None:\n",
    "                entity_text = gpt2_tokenizer.convert_tokens_to_string(entity_tokens).strip()\n",
    "                if entity_text:\n",
    "                    # Calculate average confidence for the entity (as decimal)\n",
    "                    avg_confidence = sum(entity_confidences) / len(entity_confidences)\n",
    "                    entities.append({\n",
    "                        \"entity\": entity_text,\n",
    "                        \"label\": entity_type,\n",
    "                        \"confidence\": avg_confidence\n",
    "                    })\n",
    "\n",
    "            # Start a new entity\n",
    "            current_entity = label[2:]\n",
    "            entity_type = label[2:]\n",
    "            entity_tokens = [token]\n",
    "            entity_confidences = [confidence]\n",
    "\n",
    "        elif label.startswith(\"I-\") and current_entity == label[2:]:\n",
    "            # Continue current entity\n",
    "            entity_tokens.append(token)\n",
    "            entity_confidences.append(confidence)\n",
    "\n",
    "        elif current_entity is not None:\n",
    "            # End of an entity\n",
    "            entity_text = gpt2_tokenizer.convert_tokens_to_string(entity_tokens).strip()\n",
    "            if entity_text:\n",
    "                # Calculate average confidence for the entity (as decimal)\n",
    "                avg_confidence = sum(entity_confidences) / len(entity_confidences)\n",
    "                entities.append({\n",
    "                    \"entity\": entity_text,\n",
    "                    \"label\": entity_type,\n",
    "                    \"confidence\": avg_confidence\n",
    "                })\n",
    "            current_entity = None\n",
    "            entity_tokens = []\n",
    "            entity_confidences = []\n",
    "            entity_type = None\n",
    "\n",
    "    # Check for unfinished entity\n",
    "    if current_entity is not None:\n",
    "        entity_text = gpt2_tokenizer.convert_tokens_to_string(entity_tokens).strip()\n",
    "        if entity_text:\n",
    "            # Calculate average confidence for the entity (as decimal)\n",
    "            avg_confidence = sum(entity_confidences) / len(entity_confidences)\n",
    "            entities.append({\n",
    "                \"entity\": entity_text,\n",
    "                \"label\": entity_type,\n",
    "                \"confidence\": avg_confidence\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"intent\": {\"label\": intent_label, \"confidence\": intent_confidence},\n",
    "        \"category\": {\"label\": category_label, \"confidence\": category_confidence},\n",
    "        \"ner\": entities\n",
    "    }\n",
    "\n",
    "def generate_response(model, tokenizer, instruction, classification, max_length=512, device=\"cuda\"):\n",
    "    \"\"\"Generate a response using GPT-2 model\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Extract classification details\n",
    "    intent = classification[\"intent\"][\"label\"] if isinstance(classification[\"intent\"], dict) else classification[\"intent\"]\n",
    "    category = classification[\"category\"][\"label\"] if isinstance(classification[\"category\"], dict) else classification[\"category\"]\n",
    "\n",
    "    # Clean up intent and category strings\n",
    "    if isinstance(intent, str) and \"[\" in intent:\n",
    "        intent = intent.strip(\"[]'\")\n",
    "    if isinstance(category, str) and \"[\" in category:\n",
    "        category = category.strip(\"[]'\")\n",
    "\n",
    "    # Format entity information\n",
    "    entities_text = \"\"\n",
    "    if \"ner\" in classification and classification[\"ner\"]:\n",
    "        entities = []\n",
    "        for entity in classification[\"ner\"]:\n",
    "            if isinstance(entity, dict) and \"entity\" in entity and \"label\" in entity:\n",
    "                entities.append(f\"{entity['entity']} ({entity['label']})\")\n",
    "        entities_text = \", \".join(entities)\n",
    "    else:\n",
    "        entities_text = \"none\"\n",
    "\n",
    "    # Create a prompt for generating response\n",
    "    input_text = f\"[INST] User query: {instruction}\\n\\n\" \\\n",
    "                 f\"Based on the following classification:\\n\" \\\n",
    "                 f\"- Intent: {intent}\\n\" \\\n",
    "                 f\"- Category: {category}\\n\" \\\n",
    "                 f\"- Entities: {entities_text}\\n\\n\" \\\n",
    "                 f\"Provide a helpful customer service response: [RESP]\"\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "    try:\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                num_beams=5,\n",
    "                no_repeat_ngram_size=2,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "\n",
    "        # Extract response part\n",
    "        if \"[RESP]\" in generated_text:\n",
    "            response = generated_text.split(\"[RESP]\")[1].strip()\n",
    "            # Clean up any trailing tokens\n",
    "            if \"[EOS]\" in response:\n",
    "                response = response.split(\"[EOS]\")[0].strip()\n",
    "        else:\n",
    "            # Fallback extraction\n",
    "            response = generated_text[len(input_text):].strip()\n",
    "        # Format steps if present\n",
    "        steps_pattern = re.search(r'(\\d+)\\.\\s+([A-Z])', response)\n",
    "        if steps_pattern or \"step\" in response.lower() or \"follow\" in response.lower():\n",
    "            # Format steps to be on separate lines\n",
    "            for i in range(1, 10):\n",
    "                step_marker = f\"{i}. \"\n",
    "                if step_marker in response and f\"\\n{i}. \" not in response:\n",
    "                    response = response.replace(step_marker, f\"\\n{i}. \")\n",
    "\n",
    "            # Clean up any excess newlines\n",
    "            response = re.sub(r'\\n\\s*\\n', '\\n\\n', response)\n",
    "            response = response.lstrip('\\n')\n",
    "\n",
    "        # Clean any technical artifacts\n",
    "        response = re.sub(r'https?://\\S+', '', response)  # Remove URLs\n",
    "        response = re.sub(r'<[^>]*>', '', response)  # Remove HTML tags\n",
    "        response = re.sub(r'\\{\\s*\"[^\"]*\":', '', response)  # Remove JSON-like content\n",
    "        response = re.sub(r'\\s+', ' ', response).strip()  # Clean up whitespace\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_response: {e}\")\n",
    "        return f\"I apologize, but I couldn't generate a response. Error: {str(e)}\"\n",
    "\n",
    "# Memory measurement functions\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / 1024 / 1024  # Convert bytes to MB\n",
    "\n",
    "# Fix the get_peak_memory_usage function\n",
    "def get_peak_memory_usage(func, *args, **kwargs):\n",
    "    device_param = kwargs.pop('device', None)  # Remove device parameter before calling func\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    start_mem = get_memory_usage()\n",
    "    result = func(*args, **kwargs)  # Call without device parameter\n",
    "    end_mem = get_memory_usage()\n",
    "    \n",
    "    peak_gpu_mem = 0\n",
    "    if torch.cuda.is_available() and device_param == \"cuda\":\n",
    "        peak_gpu_mem = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB\n",
    "        return result, peak_gpu_mem\n",
    "    else:\n",
    "        return result, max(0, end_mem - start_mem)\n",
    "\n",
    "class PerformanceTest:\n",
    "    def __init__(self, model_paths, test_data_path, num_samples=50):\n",
    "        self.output_dir = model_paths[\"hybrid_model_dir\"]\n",
    "        self.generation_model_path = model_paths[\"generation_model_path\"]\n",
    "        self.generation_tokenizer_path = model_paths[\"generation_tokenizer_path\"]\n",
    "        self.test_data_path = test_data_path\n",
    "        self.num_samples = num_samples\n",
    "        self.devices = [\"cpu\", \"cuda\"] if torch.cuda.is_available() else [\"cpu\"]\n",
    "        self.results = {device: [] for device in self.devices}\n",
    "        self.summary = {device: {\"classification_time\": [], \"generation_time\": [], \"overall_time\": [], \"memory_usage\": []} for device in self.devices}\n",
    "    \n",
    "    def load_models(self, device):\n",
    "        print(f\"\\nLoading models on {device.upper()}...\")\n",
    "        \n",
    "        # Load hybrid model for classification\n",
    "        encoders_path = os.path.join(self.output_dir, \"label_encoders.json\")\n",
    "        hyperparams_path = os.path.join(self.output_dir, \"hyperparameters.json\")\n",
    "        model_path = os.path.join(self.output_dir, \"hybrid_model.pth\")\n",
    "        \n",
    "        print(f\"Loading from: {encoders_path}\")\n",
    "        with open(encoders_path, 'r', encoding='utf-8') as f:\n",
    "            self.label_encoders = json.load(f)\n",
    "            \n",
    "        with open(hyperparams_path, 'r', encoding='utf-8') as f:\n",
    "            self.hyperparameters = json.load(f)\n",
    "                \n",
    "        self.gpt2_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        self.distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        if self.gpt2_tokenizer.pad_token is None:\n",
    "            self.gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        if self.distilbert_tokenizer.pad_token is None:\n",
    "            self.distilbert_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                \n",
    "        self.classification_model = HybridGPT2DistilBERTMultiTask(\n",
    "            num_intents=len(self.label_encoders[\"intent_encoder\"]),\n",
    "            num_categories=len(self.label_encoders[\"category_encoder\"]),\n",
    "            num_ner_labels=len(self.label_encoders[\"ner_label_encoder\"]),\n",
    "            dropout_rate=self.hyperparameters[\"dropout_rate\"]\n",
    "        )\n",
    "        \n",
    "        if self.gpt2_tokenizer.pad_token_id is not None:\n",
    "            self.classification_model.gpt2.resize_token_embeddings(len(self.gpt2_tokenizer))\n",
    "        \n",
    "        self.classification_model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "        self.classification_model.to(device)\n",
    "        self.classification_model.eval()\n",
    "        \n",
    "        # Load generation model using from_pretrained instead of torch.load\n",
    "        try:\n",
    "            print(f\"Attempting to load generation model from {self.generation_model_path}\")\n",
    "            self.generation_model = GPT2LMHeadModel.from_pretrained(self.generation_model_path).to(device)\n",
    "            self.generation_tokenizer = GPT2Tokenizer.from_pretrained(self.generation_tokenizer_path)\n",
    "            self.generation_tokenizer.pad_token = self.generation_tokenizer.eos_token\n",
    "            self.generation_tokenizer.add_special_tokens({'additional_special_tokens': ['[INST]', '[RESP]', '[EOS]']})\n",
    "            self.generation_model.resize_token_embeddings(len(self.generation_tokenizer))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading generation model: {e}\")\n",
    "            print(\"Falling back to default GPT2...\")\n",
    "            self.generation_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "            self.generation_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            self.generation_tokenizer.pad_token = self.generation_tokenizer.eos_token\n",
    "            self.generation_tokenizer.add_special_tokens({'additional_special_tokens': ['[INST]', '[RESP]', '[EOS]']})\n",
    "            self.generation_model.resize_token_embeddings(len(self.generation_tokenizer))\n",
    "        \n",
    "        self.generation_model.eval()\n",
    "        \n",
    "        return self.classification_model, self.generation_model, self.gpt2_tokenizer, self.distilbert_tokenizer, self.generation_tokenizer\n",
    "    \n",
    "    def run_benchmark(self):\n",
    "        # Load test data\n",
    "        with open(self.test_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "            \n",
    "        # Select samples for testing\n",
    "        if len(test_data) > self.num_samples:\n",
    "            test_samples = test_data[:self.num_samples]\n",
    "        else:\n",
    "            test_samples = test_data\n",
    "            \n",
    "        print(f\"Running performance test on {len(test_samples)} samples\")\n",
    "        \n",
    "        # Run tests on each device\n",
    "        for device in self.devices:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"RUNNING BENCHMARK ON {device.upper()}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            classification_model, generation_model, gpt2_tokenizer, distilbert_tokenizer, generation_tokenizer = self.load_models(device)\n",
    "            \n",
    "            for idx, sample in enumerate(tqdm(test_samples, desc=f\"Testing on {device.upper()}\")):\n",
    "                instruction = sample[\"instruction\"]\n",
    "                \n",
    "                # Measure classification time and memory\n",
    "                def run_classification():\n",
    "                    return inference_hybrid(\n",
    "                        classification_model, \n",
    "                        instruction, \n",
    "                        gpt2_tokenizer, \n",
    "                        distilbert_tokenizer, \n",
    "                        self.label_encoders, \n",
    "                        self.hyperparameters[\"max_length\"],\n",
    "                        device\n",
    "                    )\n",
    "                \n",
    "                classification_start = time.time()\n",
    "                classification_result, classification_memory = get_peak_memory_usage(\n",
    "                    run_classification, device=device\n",
    "                )\n",
    "                classification_end = time.time()\n",
    "                classification_time = classification_end - classification_start\n",
    "                \n",
    "                # Create new input with classification results\n",
    "                intent = classification_result[\"intent\"][\"label\"]\n",
    "                category = classification_result[\"category\"][\"label\"]\n",
    "                entities_text = \", \".join([f\"{entity['entity']} ({entity['label']})\" for entity in classification_result[\"ner\"]]) if classification_result[\"ner\"] else \"none\"\n",
    "                new_input = f\"{instruction} [Classified: Intent is '{intent}', Category is '{category}', Entities are {entities_text}]\"\n",
    "                \n",
    "                # Measure generation time and memory\n",
    "                def run_generation():\n",
    "                    return generate_response(\n",
    "                        generation_model,\n",
    "                        generation_tokenizer,\n",
    "                        instruction,\n",
    "                        classification_result,\n",
    "                        device=device\n",
    "                    )\n",
    "                \n",
    "                generation_start = time.time()\n",
    "                generated_response, generation_memory = get_peak_memory_usage(\n",
    "                    run_generation, device=device\n",
    "                )\n",
    "                generation_end = time.time()\n",
    "                generation_time = generation_end - generation_start\n",
    "                \n",
    "                # Calculate overall time and memory\n",
    "                overall_time = classification_time + generation_time\n",
    "                overall_memory = classification_memory + generation_memory\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    \"sample_id\": idx + 1,\n",
    "                    \"instruction\": instruction,\n",
    "                    \"new_input\": new_input,\n",
    "                    \"generated_response\": generated_response,\n",
    "                    \"classification_time\": classification_time,\n",
    "                    \"generation_time\": generation_time,\n",
    "                    \"overall_time\": overall_time,\n",
    "                    \"classification_memory\": classification_memory,\n",
    "                    \"generation_memory\": generation_memory,\n",
    "                    \"overall_memory\": overall_memory\n",
    "                }\n",
    "                self.results[device].append(result)\n",
    "                \n",
    "                # Update summary statistics\n",
    "                self.summary[device][\"classification_time\"].append(classification_time)\n",
    "                self.summary[device][\"generation_time\"].append(generation_time)\n",
    "                self.summary[device][\"overall_time\"].append(overall_time)\n",
    "                self.summary[device][\"memory_usage\"].append(overall_memory)\n",
    "                \n",
    "                # Print sample result\n",
    "                print(f\"\\nSample {idx+1}/{len(test_samples)} (Device: {device.upper()}):\")\n",
    "                print(f\"Instruction: {instruction}\")\n",
    "                print(f\"New Input: {new_input}\")\n",
    "                print(f\"Generated Response: {generated_response}\")\n",
    "                print(f\"Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\")\n",
    "                print(f\"Hybrid Cross Attention | {classification_time:.4f}                    | {generation_time:.4f}                   | {overall_time:.4f}                   | {device.upper()}        | {overall_memory:.4f}\")\n",
    "            \n",
    "            # Calculate average times\n",
    "            self.summary[device][\"avg_classification_time\"] = sum(self.summary[device][\"classification_time\"]) / len(test_samples)\n",
    "            self.summary[device][\"avg_generation_time\"] = sum(self.summary[device][\"generation_time\"]) / len(test_samples)\n",
    "            self.summary[device][\"avg_overall_time\"] = sum(self.summary[device][\"overall_time\"]) / len(test_samples)\n",
    "            self.summary[device][\"avg_memory_usage\"] = sum(self.summary[device][\"memory_usage\"]) / len(test_samples)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"BENCHMARK SUMMARY FOR {device.upper()}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Average Classification Time: {self.summary[device]['avg_classification_time']:.4f} seconds\")\n",
    "            print(f\"Average Generation Time: {self.summary[device]['avg_generation_time']:.4f} seconds\")\n",
    "            print(f\"Average Overall Time: {self.summary[device]['avg_overall_time']:.4f} seconds\")\n",
    "            print(f\"Average Memory Usage: {self.summary[device]['avg_memory_usage']:.4f} MB\")\n",
    "\n",
    "        # Save results to files and create plots\n",
    "        self.save_results()\n",
    "\n",
    "    # Add to your PerformanceTest class\n",
    "    def save_results(self):\n",
    "        \"\"\"Save benchmark results to JSON files in the specified format\"\"\"\n",
    "        \n",
    "        # Save CPU results\n",
    "        if \"cpu\" in self.devices and \"avg_classification_time\" in self.summary[\"cpu\"]:\n",
    "            cpu_results = []\n",
    "            \n",
    "            for result in self.results[\"cpu\"]:\n",
    "                formatted_result = {\n",
    "                    \"Model\": \"Hybrid Cross attention\",\n",
    "                    \"Classification Time (s)\": result[\"classification_time\"],\n",
    "                    \"Text Generation Time (s)\": result[\"generation_time\"],\n",
    "                    \"Overall Inference Time (s)\": result[\"overall_time\"],\n",
    "                    \"Device\": \"CPU\",\n",
    "                    \"Memory Usage (MB)\": result[\"overall_memory\"],\n",
    "                    \"Instruction\": result[\"instruction\"],\n",
    "                    \"New Input\": result[\"new_input\"],\n",
    "                    \"Generated Response\": result[\"generated_response\"]\n",
    "                }\n",
    "                cpu_results.append(formatted_result)\n",
    "            \n",
    "            with open(\"performance_results_cpu.json\", \"w\") as f:\n",
    "                json.dump(cpu_results, f, indent=4)\n",
    "            print(\"CPU results saved to performance_results_cpu.json\")\n",
    "        \n",
    "        # Save GPU results\n",
    "        if \"cuda\" in self.devices and \"avg_classification_time\" in self.summary[\"cuda\"]:\n",
    "            gpu_results = []\n",
    "            \n",
    "            for result in self.results[\"cuda\"]:\n",
    "                formatted_result = {\n",
    "                    \"Model\": \"Hybrid Cross Attention\",\n",
    "                    \"Classification Time (s)\": result[\"classification_time\"],\n",
    "                    \"Text Generation Time (s)\": result[\"generation_time\"],\n",
    "                    \"Overall Inference Time (s)\": result[\"overall_time\"],\n",
    "                    \"Device\": \"GPU\",\n",
    "                    \"Memory Usage (MB)\": result[\"overall_memory\"],\n",
    "                    \"Instruction\": result[\"instruction\"],\n",
    "                    \"New Input\": result[\"new_input\"],\n",
    "                    \"Generated Response\": result[\"generated_response\"]\n",
    "                }\n",
    "                gpu_results.append(formatted_result)\n",
    "            \n",
    "            with open(\"performance_results_gpu.json\", \"w\") as f:\n",
    "                json.dump(gpu_results, f, indent=4)\n",
    "            print(\"GPU results saved to performance_results_gpu.json\")\n",
    "        \n",
    "        # Create performance log and comparison plots\n",
    "        self.create_performance_log()\n",
    "        self.create_comparison_plots()\n",
    "\n",
    "    def create_performance_log(self):\n",
    "        \"\"\"Create a text file with performance comparisons\"\"\"\n",
    "        with open(\"performance_log.txt\", \"w\") as f:\n",
    "            f.write(\"=== PERFORMANCE BENCHMARK RESULTS ===\\n\\n\")\n",
    "            \n",
    "            # Write CPU results\n",
    "            if \"cpu\" in self.devices:\n",
    "                f.write(\"CPU PERFORMANCE:\\n\")\n",
    "                f.write(f\"Average Classification Time: {self.summary['cpu']['avg_classification_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Generation Time: {self.summary['cpu']['avg_generation_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Overall Time: {self.summary['cpu']['avg_overall_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Memory Usage: {self.summary['cpu']['avg_memory_usage']:.4f} MB\\n\\n\")\n",
    "            \n",
    "            # Write GPU results\n",
    "            if \"cuda\" in self.devices and \"cuda\" in self.results:\n",
    "                f.write(\"GPU PERFORMANCE:\\n\")\n",
    "                f.write(f\"Average Classification Time: {self.summary['cuda']['avg_classification_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Generation Time: {self.summary['cuda']['avg_generation_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Overall Time: {self.summary['cuda']['avg_overall_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Memory Usage: {self.summary['cuda']['avg_memory_usage']:.4f} MB\\n\\n\")\n",
    "            \n",
    "            # Write comparison\n",
    "            if \"cpu\" in self.devices and \"cuda\" in self.devices and \"cuda\" in self.results:\n",
    "                f.write(\"CPU vs GPU COMPARISON:\\n\")\n",
    "                class_speedup = self.summary['cpu']['avg_classification_time'] / self.summary['cuda']['avg_classification_time']\n",
    "                gen_speedup = self.summary['cpu']['avg_generation_time'] / self.summary['cuda']['avg_generation_time']\n",
    "                overall_speedup = self.summary['cpu']['avg_overall_time'] / self.summary['cuda']['avg_overall_time']\n",
    "                \n",
    "                f.write(f\"Classification Speed Improvement: {class_speedup:.2f}x faster on GPU\\n\")\n",
    "                f.write(f\"Generation Speed Improvement: {gen_speedup:.2f}x faster on GPU\\n\")\n",
    "                f.write(f\"Overall Speed Improvement: {overall_speedup:.2f}x faster on GPU\\n\\n\")\n",
    "                \n",
    "                f.write(\"Notes:\\n\")\n",
    "                f.write(\"- GPU memory usage is typically higher but processing is faster\\n\")\n",
    "                f.write(\"- The generation task shows the largest speed improvement on GPU\\n\")\n",
    "            \n",
    "            f.write(\"\\n=== END OF REPORT ===\\n\")\n",
    "        \n",
    "        print(\"Performance log saved to performance_log.txt\")\n",
    "\n",
    "    def create_comparison_plots(self):\n",
    "        \"\"\"Create matplotlib comparison plots\"\"\"\n",
    "        if \"cpu\" not in self.devices or \"cuda\" not in self.devices or \"cuda\" not in self.results:\n",
    "            print(\"Both CPU and GPU results are needed for comparison plots\")\n",
    "            return\n",
    "        \n",
    "        # Set up the plots\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        \n",
    "        # 1. Processing Time Comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        \n",
    "        labels = ['Classification', 'Generation', 'Overall']\n",
    "        cpu_times = [self.summary['cpu']['avg_classification_time'], \n",
    "                    self.summary['cpu']['avg_generation_time'],\n",
    "                    self.summary['cpu']['avg_overall_time']]\n",
    "        \n",
    "        gpu_times = [self.summary['cuda']['avg_classification_time'], \n",
    "                    self.summary['cuda']['avg_generation_time'],\n",
    "                    self.summary['cuda']['avg_overall_time']]\n",
    "        \n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, cpu_times, width, label='CPU')\n",
    "        plt.bar(x + width/2, gpu_times, width, label='GPU')\n",
    "        \n",
    "        plt.xlabel('Task')\n",
    "        plt.ylabel('Time (seconds)')\n",
    "        plt.title('Average Processing Time Comparison')\n",
    "        plt.xticks(x, labels)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on the bars\n",
    "        for i, v in enumerate(cpu_times):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.2f}s\", ha='center')\n",
    "        \n",
    "        for i, v in enumerate(gpu_times):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.2f}s\", ha='center')\n",
    "        \n",
    "        # 2. Speed Improvement\n",
    "        plt.subplot(2, 2, 2)\n",
    "        \n",
    "        speedups = [\n",
    "            cpu_times[0] / gpu_times[0] if gpu_times[0] > 0 else 0,\n",
    "            cpu_times[1] / gpu_times[1] if gpu_times[1] > 0 else 0,\n",
    "            cpu_times[2] / gpu_times[2] if gpu_times[2] > 0 else 0\n",
    "        ]\n",
    "        \n",
    "        plt.bar(x, speedups, width=0.5)\n",
    "        plt.axhline(y=1.0, color='r', linestyle='-', alpha=0.3)\n",
    "        plt.xlabel('Task')\n",
    "        plt.ylabel('Times Faster on GPU')\n",
    "        plt.title('CPU vs GPU Speed Improvement')\n",
    "        plt.xticks(x, labels)\n",
    "        \n",
    "        # Add speed improvement labels\n",
    "        for i, v in enumerate(speedups):\n",
    "            plt.text(i, v + 0.1, f\"{v:.1f}x\", ha='center')\n",
    "        \n",
    "        # 3. Memory Usage\n",
    "        plt.subplot(2, 2, 3)\n",
    "        \n",
    "        memory_labels = ['CPU', 'GPU']\n",
    "        memory_usage = [self.summary['cpu']['avg_memory_usage'],\n",
    "                    self.summary['cuda']['avg_memory_usage']]\n",
    "        \n",
    "        plt.bar(memory_labels, memory_usage)\n",
    "        plt.xlabel('Device')\n",
    "        plt.ylabel('Memory Usage (MB)')\n",
    "        plt.title('Average Memory Usage')\n",
    "        \n",
    "        # Add memory usage labels\n",
    "        for i, v in enumerate(memory_usage):\n",
    "            plt.text(i, v + 0.1, f\"{v:.1f} MB\", ha='center')\n",
    "        \n",
    "        # 4. Time breakdown for both devices\n",
    "        plt.subplot(2, 2, 4)\n",
    "        \n",
    "        # CPU breakdown\n",
    "        cpu_breakdown = [\n",
    "            self.summary['cpu']['avg_classification_time'] / self.summary['cpu']['avg_overall_time'] * 100,\n",
    "            self.summary['cpu']['avg_generation_time'] / self.summary['cpu']['avg_overall_time'] * 100\n",
    "        ]\n",
    "        \n",
    "        # GPU breakdown  \n",
    "        gpu_breakdown = [\n",
    "            self.summary['cuda']['avg_classification_time'] / self.summary['cuda']['avg_overall_time'] * 100,\n",
    "            self.summary['cuda']['avg_generation_time'] / self.summary['cuda']['avg_overall_time'] * 100\n",
    "        ]\n",
    "        \n",
    "        task_labels = ['Classification', 'Generation']\n",
    "        \n",
    "        x = np.arange(len(memory_labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, [cpu_breakdown[0], gpu_breakdown[0]], width, label='Classification')\n",
    "        plt.bar(x - width/2, [cpu_breakdown[1], gpu_breakdown[1]], width, bottom=[cpu_breakdown[0], gpu_breakdown[0]], \n",
    "                label='Generation')\n",
    "        \n",
    "        plt.xlabel('Device')\n",
    "        plt.ylabel('Percentage of Overall Time (%)')\n",
    "        plt.title('Task Time Distribution')\n",
    "        plt.xticks(x - width/2, memory_labels)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for i, (c, g) in enumerate(zip(cpu_breakdown, gpu_breakdown)):\n",
    "            plt.text(0 - width/2, c/2 if i == 0 else cpu_breakdown[0] + c/2, \n",
    "                    f\"{c:.1f}%\", ha='center')\n",
    "            plt.text(1 - width/2, g/2 if i == 0 else gpu_breakdown[0] + g/2,\n",
    "                    f\"{g:.1f}%\", ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('performance_comparison.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Performance comparison plots saved to performance_comparison.png\")\n",
    "def main():\n",
    "    # Define paths\n",
    "    model_paths = {\n",
    "        \"hybrid_model_dir\": \"../../Hybrid_Cross_Attention_Freeze\",\n",
    "        \"generation_model_path\": \"../../text_generation_results_03-09-25/model\",\n",
    "        \"generation_tokenizer_path\": \"../../text_generation_results_03-09-25/tokenizer\"\n",
    "    }\n",
    "    \n",
    "    test_data_path = \"../../test.json\"\n",
    "    \n",
    "    # Verify and print paths\n",
    "    print(f\"Hybrid model directory: {model_paths['hybrid_model_dir']}\")\n",
    "    print(f\"Test data path: {test_data_path}\")\n",
    "    \n",
    "    # Number of samples to test\n",
    "    num_samples = 50  # You can adjust this number\n",
    "    \n",
    "    # Run the benchmark\n",
    "    benchmark = PerformanceTest(model_paths, test_data_path, num_samples)\n",
    "    benchmark.run_benchmark()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check available devices\n",
    "    device_info = \"CUDA\" if torch.cuda.is_available() else \"CPU\"\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"Running on: {device_info}\")\n",
    "    \n",
    "    # Run the benchmark\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
