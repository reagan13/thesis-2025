{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0+cu117)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (6.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement rogue-score (from versions: none)\n",
      "ERROR: No matching distribution found for rogue-score\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch psutil tqdm numpy rogue-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Running on: CUDA\n",
      "Baseline model dir: ../..//Baseline_freeze_v1\n",
      "Generation model path: ../../text_generation_results_03-09-25/model\n",
      "Test data path: ../../test.json\n",
      "CUDA available\n",
      "Running performance test on 50 samples\n",
      "\n",
      "================================================================================\n",
      "RUNNING BENCHMARK ON CPU\n",
      "================================================================================\n",
      "\n",
      "Loading models on CPU...\n",
      "Loading from: ../..//Baseline_freeze_v1\\label_encoders.json\n",
      "Initializing classification model...\n",
      "All GPT-2 layers remain frozen\n",
      "Classification model initialized with loss weights: {'intent': 0.3, 'category': 0.3, 'ner': 0.4}\n",
      "Attempting to load generation model from ../../text_generation_results_03-09-25/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   2%|▏         | 1/50 [00:26<21:49, 26.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1/50 (Device: CPU):\n",
      "Instruction: i do not know what i have to do to get my invoice #85632\n",
      "New Input: i do not know what i have to do to get my invoice #85632 [Classified: Intent is 'get_invoice', Category is 'invoice', Entities are # (refund_amount)]\n",
      "Generated Response: i understand that you're unsure about the steps to take in order to retrieve your invoice with the number #{{money amount}}. let me guide you through the process. first, please log in to your account on our website. once logged in, navigate to the \"billing\" or \"invoices\" section. there, you should be able to find a list of your invoices. look for a tab or option that says \"view invoice.\" click on that option, and you'll be directed to a page where you can view and download the invoice. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team. they will be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 6.8107                    | 19.9064                   | 26.7171                   | CPU        | 359.6094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   4%|▍         | 2/50 [00:39<14:55, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 2/50 (Device: CPU):\n",
      "Instruction: transfer to live agent\n",
      "New Input: transfer to live agent [Classified: Intent is 'create_account', Category is 'account', Entities are live (account_type), agent (account_type)]\n",
      "Generated Response: assuredly! i'm here to assist you in transferring to one of our {{account type}} agents. to get started, could you please provide me with your full name, email address, and a preferred username? once i have these details, i'll guide you through the process step by step, ensuring that you have a seamless experience. rest assured, your security and privacy are of utmost importance to us, so you can trust that your information will be handled with the highest level of care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4581                    | 12.5359                   | 12.9940                   | CPU        | 4.9922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   6%|▌         | 3/50 [00:49<11:26, 14.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 3/50 (Device: CPU):\n",
      "Instruction: i need help to see how long refunds take\n",
      "New Input: i need help to see how long refunds take [Classified: Intent is 'delivery_period', Category is 'delivery', Entities are none]\n",
      "Generated Response: we understand that you would like to know the estimated time it takes for refunds to be processed. to provide you with accurate information, could you please provide us with the {{order number}} or any other relevant details? this will enable us to track your refund and give you an estimated delivery date. thank you for your cooperation, and we look forward to assisting you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5217                    | 9.2667                   | 9.7884                   | CPU        | 0.8008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:   8%|▊         | 4/50 [01:00<09:59, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 4/50 (Device: CPU):\n",
      "Instruction: help to pseak to customer support\n",
      "New Input: help to pseak to customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are customer (account_type)]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to know if you need to speak to our customer assistance team. our dedicated team is available to assist you through various channels, including phone, email, and live chat. you can reach us through our website at {{website url}}. please let me know how i can help you with any questions or concerns you may have.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4125                    | 10.2032                   | 10.6157                   | CPU        | 0.4297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  10%|█         | 5/50 [01:22<12:17, 16.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 5/50 (Device: CPU):\n",
      "Instruction: i do not know what to do to swtch to the premium account\n",
      "New Input: i do not know what to do to swtch to the premium account [Classified: Intent is 'switch_account', Category is 'account', Entities are sw (person_name), premium (account_type)]\n",
      "Generated Response: i understand that you're unsure about the steps to switch to our {{account type}} account, and i'm here to guide you through the process. allow me to provide you with the information you need to make the switch smoothly: first, log in to your account on our website or mobile app using your credentials. once logged in, locate the account settings or profile section. look for an option that says \"switch account type\" or \"upgrade.\" click on that option. you'll be prompted to enter your email address or username associated with your subscription. make sure to double-check all the required fields, such as your name, email, phone number, or billing address, to ensure accurate information. if you encounter any difficulties or have further questions, please don't hesitate to let me know. it's my priority to assist you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.3845                    | 21.9488                   | 22.3333                   | CPU        | 1.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  12%|█▏        | 6/50 [01:43<13:11, 18.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 6/50 (Device: CPU):\n",
      "Instruction: i need assistance resetting the key of my user account\n",
      "New Input: i need assistance resetting the key of my user account [Classified: Intent is 'recover_password', Category is 'account', Entities are user (account_type)]\n",
      "Generated Response: i'm here to provide you with the assistance you need to reset your key to your account. rest assured, i'll guide you through the process step by step to ensure a smooth and secure recovery. to begin, please visit our platform's \"{{login page url}}\". once you're on the login page, navigate to the account settings or security settings section. look for the option that says \"reset key\" and click on it. you'll be prompted to enter the email address associated with your user profile when prompted. follow the on-screen instructions to verify your identity. if you encounter any difficulties or have further questions, feel free to reach out to our dedicated customer support team. they will be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4765                    | 20.6550                   | 21.1315                   | CPU        | 1.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  14%|█▍        | 7/50 [01:53<11:03, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 7/50 (Device: CPU):\n",
      "Instruction: i want help taking a quick look at the bills from john robertson\n",
      "New Input: i want help taking a quick look at the bills from john robertson [Classified: Intent is 'check_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: i'm on the same page, your desire to quickly view the invoices from {{salutation}} {{client last name}}. let's work together to find the information you need. could you please provide me with any additional details such as the date range, invoice number, or any other relevant information? with this information, i'll be able to assist you more effectively and provide you with the necessary assistance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4149                    | 9.7188                   | 10.1337                   | CPU        | 0.5742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  16%|█▌        | 8/50 [02:06<10:09, 14.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 8/50 (Device: CPU):\n",
      "Instruction: i can't enter a delivery address\n",
      "New Input: i can't enter a delivery address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are delivery (account_category)]\n",
      "Generated Response: i'm sorry to hear that you're having trouble entering a {{account category}} address. let's work together to resolve this issue. to better assist you, could you please provide more details about the specific problem you are facing? are you encountering any error messages or is the system not accepting your address? once i have more information, i'll be able to provide you with the most accurate guidance and support. your satisfaction is our top priority, and we're here to help you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4045                    | 12.1682                   | 12.5727                   | CPU        | 0.3047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  18%|█▊        | 9/50 [02:14<08:32, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 9/50 (Device: CPU):\n",
      "Instruction: can you help me place an order?\n",
      "New Input: can you help me place an order? [Classified: Intent is 'place_order', Category is 'delivery', Entities are none]\n",
      "Generated Response: i'll take care of it! i'm here to assist you in placing your order. please provide me with the details of the items you wish to order, such as their names, quantities, and any other relevant information. once i have this information, i will guide you through the process step by step, ensuring a smooth and satisfying experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4381                    | 7.6564                   | 8.0945                   | CPU        | 0.1914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  20%|██        | 10/50 [02:38<10:46, 16.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 10/50 (Device: CPU):\n",
      "Instruction: i do not know how i can set up my shipping address\n",
      "New Input: i do not know how i can set up my shipping address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are shipping (account_category)]\n",
      "Generated Response: i'm attuned to the idea that you're unsure about the process of setting up your shipping addresses. allow me to guide you through it step by step: 1. log in to your account on our website or app by visiting the \"my account\" or \"profile\" section. 2. look for the option that says \"add a new address\" and click on it. 3. fill in the required fields with the necessary details such as your name, email address, city, state, and zip code. 4. follow the on-screen prompts to verify your identity and provide any necessary information. 5. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team. they will be more than happy to assist you further and ensure a smooth setup process. remember, your satisfaction is our top priority, so please don't hesitate to ask any questions or share any concerns you may have.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.3769                    | 23.9686                   | 24.3454                   | CPU        | 0.3281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  22%|██▏       | 11/50 [02:48<09:08, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 11/50 (Device: CPU):\n",
      "Instruction: i need assistance to set my new delovery address up\n",
      "New Input: i need assistance to set my new delovery address up [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are new (account_type), del (person_name)]\n",
      "Generated Response: i understand your need for assistance in setting up your new delivery address. rest assured, i'm here to guide you through the process and ensure a seamless experience. to get started, could you please provide me with more details about the specific issue you're facing? this will help me tailor my guidance to your needs and provide you with the most accurate guidance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4521                    | 8.8277                   | 9.2799                   | CPU        | 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  24%|██▍       | 12/50 [02:56<07:51, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 12/50 (Device: CPU):\n",
      "Instruction: help me to check what payment methods are available\n",
      "New Input: help me to check what payment methods are available [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the payment options available to you. to provide you with accurate information, could you please provide me with your full name, email address, and a preferred payment method? once i have these details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4326                    | 8.1600                   | 8.5926                   | CPU        | 0.2461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  26%|██▌       | 13/50 [03:06<07:05, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 13/50 (Device: CPU):\n",
      "Instruction: i call to leave my feedback for ur services\n",
      "New Input: i call to leave my feedback for ur services [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: thank you for reaching out to us for feedback on our services. your feedback is highly valued and plays a crucial role in shaping our future offerings. i would be more than happy to assist you in leaving your valuable feedback. could you please provide me with more details about the specific service you would like to review? this will help me guide you through the process and ensure that your input reaches the appropriate department.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.3760                    | 9.0616                   | 9.4376                   | CPU        | 0.1641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  28%|██▊       | 14/50 [03:18<07:05, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 14/50 (Device: CPU):\n",
      "Instruction: open freemium account for wife\n",
      "New Input: open freemium account for wife [Classified: Intent is 'create_account', Category is 'account', Entities are fre (person_name), ium (account_type), wife (account_type)]\n",
      "Generated Response: i appreciate your interest in opening a {{account type}} account with us! to get started, could you please provide me with your wife's full name, email address, and a preferred username? once i have these details, i'll guide you through the account creation process step by step, ensuring a seamless experience for both of you. rest assured that we prioritize the security and privacy of our customers' information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.3868                    | 12.1628                   | 12.5496                   | CPU        | 0.1602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  30%|███       | 15/50 [03:29<06:38, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 15/50 (Device: CPU):\n",
      "Instruction: i am trying to see the status of order ord-1135283\n",
      "New Input: i am trying to see the status of order ord-1135283 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-1135283 (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the current status and estimated time of arrival (eta) for your order with the order number {{order number}}. to provide you with accurate information, could you please provide me with your account details or any other relevant information? once i have these details, i'll be able to guide you through the process and ensure a seamless experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4334                    | 9.9721                   | 10.4055                   | CPU        | 0.2578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  32%|███▏      | 16/50 [03:49<07:56, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 16/50 (Device: CPU):\n",
      "Instruction: wanna see ur payment modalities will yahelp me\n",
      "New Input: wanna see ur payment modalities will yahelp me [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking our available payment methods. we offer a wide range of payment options to cater to our customers' preferences. here's a comprehensive list of our accepted payment methods: - credit/debit card: we accept major credit and debit cards such as visa, mastercard, and american express. you can securely make payments using your card details. this allows you to make secure and hassle-free transactions with your credit or debit card payments. - paypal: a widely recognized and trusted online payment platform that provides a seamless and secure payment experience. if you have any specific questions or need further assistance, please don't hesitate to let me know. i'll be more than happy to provide you with the information you need.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4893                    | 19.6088                   | 20.0981                   | CPU        | 3.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  34%|███▍      | 17/50 [04:01<07:23, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 17/50 (Device: CPU):\n",
      "Instruction: what do i ned to do to change to the real estate account?\n",
      "New Input: what do i ned to do to change to the real estate account? [Classified: Intent is 'switch_account', Category is 'account', Entities are real (account_type), estate (account_category)]\n",
      "Generated Response: assuredly! i'm here to guide you through the process of switching to our {{account type}} account. to get started, could you please provide me with your full name, email address, and a preferred username? once i have these details, i'll be able to assist you step-by-step in making the switch. remember, your security and privacy are of utmost importance to us, so rest assured that your information will be handled with the utmost care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4722                    | 11.6784                   | 12.1506                   | CPU        | 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  36%|███▌      | 18/50 [04:11<06:35, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 18/50 (Device: CPU):\n",
      "Instruction: notify of errorw with online registration\n",
      "New Input: notify of errorw with online registration [Classified: Intent is 'registration_problems', Category is 'payment', Entities are online (account_type)]\n",
      "Generated Response: we understand that encountering errors during the registration process can be frustrating. rest assured, we are here to assist you in resolving this issue. could you please provide more details about the specific error you are facing? this will help us understand the situation better and provide you with the most appropriate solution. thank you for bringing this to our attention, and we appreciate your patience as we work together to resolve this matter.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4621                    | 9.3453                   | 9.8074                   | CPU        | 0.2773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  38%|███▊      | 19/50 [04:23<06:23, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 19/50 (Device: CPU):\n",
      "Instruction: another standard accounmt for father\n",
      "New Input: another standard accounmt for father [Classified: Intent is 'create_account', Category is 'account', Entities are standard (account_type), acc (delivery_country), father (account_type)]\n",
      "Generated Response: i'm here to assist you in creating another {{account type}} account for your father. to get started, could you please provide me with your dad's full name, email address, and a preferred username? once i have these details, i'll guide you through the account creation process step by step. rest assured, we prioritize the security and privacy of our customers' information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.3961                    | 11.9494                   | 12.3455                   | CPU        | 0.2422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  40%|████      | 20/50 [04:33<05:53, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 20/50 (Device: CPU):\n",
      "Instruction: i need infrmation about the cancellation of my enterprise account\n",
      "New Input: i need infrmation about the cancellation of my enterprise account [Classified: Intent is 'delete_account', Category is 'account', Entities are inf (person_name), enterprise (account_type)]\n",
      "Generated Response: i understand that you need assistance with canceling your {{account type}} account. i'm here to guide you through the process and ensure a smooth cancellation experience. could you please provide me with more details about your specific query or concern? this will help me provide you with the most accurate information and support.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4169                    | 10.0766                   | 10.4935                   | CPU        | 0.2969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  42%|████▏     | 21/50 [04:41<05:03, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 21/50 (Device: CPU):\n",
      "Instruction: i want help ordering a few of your product\n",
      "New Input: i want help ordering a few of your product [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for your interest in ordering our products! i'm here to assist you with the ordering process. to get started, could you please provide me with more details about the specific products you would like to order? this will help me guide you through the necessary steps and ensure a seamless experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4527                    | 6.8698                   | 7.3225                   | CPU        | 0.1523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  44%|████▍     | 22/50 [04:53<05:11, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 22/50 (Device: CPU):\n",
      "Instruction: checking status of order ord-6772250\n",
      "New Input: checking status of order ord-6772250 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-6772250 (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status and estimated time of arrival (eta) for your order with the order number {{order number}}. to provide you with accurate information, could you please provide me with your account details or any other relevant information? once i have these details, i'll be able to guide you through the process of tracking your purchase.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4351                    | 12.2436                   | 12.6787                   | CPU        | 0.1367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  46%|████▌     | 23/50 [05:04<04:57, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 23/50 (Device: CPU):\n",
      "Instruction: deleting government account\n",
      "New Input: deleting government account [Classified: Intent is 'delete_account', Category is 'account', Entities are government (account_type)]\n",
      "Generated Response: i understand that you would like to delete your {{account type}} account, and i'm here to assist you with that. to proceed with the account deletion, please provide me with your account details, such as your username or email address, so that i can locate and guide you through the necessary steps. rest assured, your request will be processed promptly and securely.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4025                    | 10.3171                   | 10.7196                   | CPU        | 6.1562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  48%|████▊     | 24/50 [05:14<04:33, 10.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 24/50 (Device: CPU):\n",
      "Instruction: my experience with your brand was bad, make a consumer claim\n",
      "New Input: my experience with your brand was bad, make a consumer claim [Classified: Intent is 'complaint', Category is 'feedback', Entities are consumer (account_type)]\n",
      "Generated Response: i apologize for any negative experience you may have had with our brand. your feedback is important to us, and we want to ensure that your concerns are addressed appropriately. could you please provide me with more details about the specific issue you encountered? this will help us understand the situation better and take the necessary steps to resolve it.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5509                    | 8.8397                   | 9.3906                   | CPU        | 0.3320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  50%|█████     | 25/50 [05:24<04:22, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 25/50 (Device: CPU):\n",
      "Instruction: wann locate order ord-3779640\n",
      "New Input: wann locate order ord-3779640 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-3779640 (order_number)]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to locate your order with the order number {{order number}}. to assist you further, could you please provide me with some additional information? specifically, i would need to know the platform or website where you made the purchase, the method of delivery used, and any other relevant details. this will help me provide you with accurate and up-to-date information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4145                    | 10.0214                   | 10.4358                   | CPU        | 0.1523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  52%|█████▏    | 26/50 [05:45<05:28, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 26/50 (Device: CPU):\n",
      "Instruction: i have to sde in what cases can i request a refund\n",
      "New Input: i have to sde in what cases can i request a refund [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm happy to help! i understand that you would like to know the specific cases in which you can request refunds. allow me to provide you with a comprehensive breakdown: 1. **product/service defect:** if you receive a product or service that has a defect or does not meet the description provided, you are typically eligible for a reimbursement. however, it's important to note that the exact nature of the defect may vary depending on the brand, product, and the particular circumstances surrounding the product's delivery to you or your location in the country where you made the purchase. if there are any specific conditions or requirements regarding the refund process, i recommend reaching out to our customer support team. they will be able to guide you through the necessary steps and provide any additional information or assistance you may need to request your refund.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4014                    | 20.7887                   | 21.1901                   | CPU        | 0.8047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  54%|█████▍    | 27/50 [05:56<04:52, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 27/50 (Device: CPU):\n",
      "Instruction: i'm trying to see at what time i can call customer support\n",
      "New Input: i'm trying to see at what time i can call customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are customer (account_type)]\n",
      "Generated Response: we're here to help! i take note that you would like to know the operating hours during which you can contact our support team. our dedicated team is available to assist you during our {{client support hours}}. please feel free to reach out to us during these hours, and we'll be more than happy to address any questions or concerns you may have. how else may i be of assistance to you today?\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.3989                    | 10.0262                   | 10.4251                   | CPU        | 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  56%|█████▌    | 28/50 [06:08<04:40, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 28/50 (Device: CPU):\n",
      "Instruction: assistance sending some feedback for ur proiducts\n",
      "New Input: assistance sending some feedback for ur proiducts [Classified: Intent is 'review', Category is 'feedback', Entities are pro (account_type)]\n",
      "Generated Response: thank you for choosing to provide feedback on our {{account type}} account! your feedback is incredibly valuable to us as it helps us understand your needs and preferences better. to send your valuable feedback, you can visit our website and navigate to the \"contact us\" or \"feedback\" section, where you'll find a form to fill out with your details and suggestions. we value your input and look forward to hearing from you!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4229                    | 12.3404                   | 12.7633                   | CPU        | 0.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  58%|█████▊    | 29/50 [06:37<06:10, 17.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 29/50 (Device: CPU):\n",
      "Instruction: need assistance to cancel order ord-8227420\n",
      "New Input: need assistance to cancel order ord-8227420 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8227420 (order_number)]\n",
      "Generated Response: i'm here to assist you with canceling your order with the order number {{order number}}. your satisfaction is our top priority, and we want to ensure that your request is handled promptly and efficiently. here's what you need to do: 1. sign into your account: access our platform by logging in to your {{online company portal info}} and navigating to the '{{online order interaction}}' section. you'll find a sign-up form where you can enter your registered email address and password to complete the cancellation process successfully.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4199                    | 28.6890                   | 29.1090                   | CPU        | 0.9219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  60%|██████    | 30/50 [06:46<05:00, 15.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 30/50 (Device: CPU):\n",
      "Instruction: need assistance cancelinv purchase ord-8042947\n",
      "New Input: need assistance cancelinv purchase ord-8042947 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8042947 (order_number)]\n",
      "Generated Response: i'm here to assist you with canceling your purchase with the order number {{order number}}. your satisfaction is our top priority, and we want to ensure that you have a seamless experience with us. please provide me with some additional information, such as your account details or any specific concerns you may have, so i can guide you through the cancellation process smoothly.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4021                    | 8.5634                   | 8.9654                   | CPU        | 0.1719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  62%|██████▏   | 31/50 [07:07<05:15, 16.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 31/50 (Device: CPU):\n",
      "Instruction: i don't know how i can download my bill #12588\n",
      "New Input: i don't know how i can download my bill #12588 [Classified: Intent is 'get_invoice', Category is 'invoice', Entities are # (refund_amount)]\n",
      "Generated Response: i understand that you're unsure about how to download your bill with the number #{{salutation}} {{client last name}}. let me guide you through the process step by step. first, please log in to your account on our website. once logged in, navigate to the \"billing\" or \"invoices\" section. there, you should be able to find a list of your invoices. look for a tab or link that says \"download invoice.\" click on that, and it will take you to a page where you can view and download the bill. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.3878                    | 19.8819                   | 20.2697                   | CPU        | 0.1523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  64%|██████▍   | 32/50 [07:18<04:28, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 32/50 (Device: CPU):\n",
      "Instruction: i need help to see purchase ord-0521414 status\n",
      "New Input: i need help to see purchase ord-0521414 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-0521414 (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status of your purchase with the purchase number {{order number}}. to provide you with accurate information, could you please provide me with your account details or any other relevant information? once i have the necessary details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4166                    | 10.5276                   | 10.9442                   | CPU        | 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  66%|██████▌   | 33/50 [07:31<04:07, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 33/50 (Device: CPU):\n",
      "Instruction: am i entitled to a refund according to your policy?\n",
      "New Input: am i entitled to a refund according to your policy? [Classified: Intent is 'get_refund', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm happy to help! i understand that you would like to know the status of your refund. your satisfaction is of utmost importance to us, and we want to ensure that your concerns are addressed promptly and effectively. to provide you with accurate information, could you please provide me with your order number or any other relevant details? this will enable me to assist you more effectively and guide you through the refund process.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6947                    | 13.1296                   | 13.8244                   | CPU        | 1.3633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  68%|██████▊   | 34/50 [08:29<07:17, 27.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 34/50 (Device: CPU):\n",
      "Instruction: need to check what methods for delivery i can choose help me\n",
      "New Input: need to check what methods for delivery i can choose help me [Classified: Intent is 'delivery_options', Category is 'delivery', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in exploring the various delivery methods available to you. let's dive into the possibilities: 1. standard shipping: this method typically takes {{date range}} business days for your package to arrive at your doorstep. it's perfect for non-urgent items when you have a bit more time to spare before you're ready to receive your desired items. you can also opt for expedited shipping if you prefer a quicker delivery, and we'll make sure to provide you with a {{order number}} or {{tracking number}}, whichever method suits your needs best. 2. express delivery: for those urgent needs, we offer express shipping for a faster delivery. whether you need your items urgently or need them in a matter of moments, our express service guarantees next business day delivery to your location in {{delivery country}}. we've got you covered every step of the way, ensuring a seamless and hassle-free experience. 3. in-store pickup: to ensure your order is delivered to the right location, please reach out to our customer support team during {{customer support hours}} to initiate the pickup process. they'll be more than happy to guide you through the necessary steps and provide any additional assistance you may need. feel free to let me know if there's anything specific you'd like to know about our delivery options. your satisfaction is our top priority, so please don't hesitate to ask.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5054                    | 56.6993                   | 57.2047                   | CPU        | 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  70%|███████   | 35/50 [08:42<05:46, 23.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 35/50 (Device: CPU):\n",
      "Instruction: ya have an address to submit some feednack\n",
      "New Input: ya have an address to submit some feednack [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in submitting your delivery address. to ensure a smooth process, could you please provide me with more details about the platform or service you are using? this will help me guide you through the necessary steps and provide you with accurate information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 1.1882                    | 11.8452                   | 13.0334                   | CPU        | 224.4141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  72%|███████▏  | 36/50 [09:09<05:41, 24.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 36/50 (Device: CPU):\n",
      "Instruction: show me in which situations  can i ask for refunds\n",
      "New Input: show me in which situations  can i ask for refunds [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the situations where you can request a refund. let me provide you with a comprehensive breakdown of the common scenarios: 1. **product/service defect:** if the product or service you received has a defect or does not meet the description provided, you may be eligible for a reimbursement. However, i recommend reaching out to our customer support team for further assistance. they will be able to guide you through the refund process and provide any additional information or documentation required to initiate your refund request. remember, your satisfaction is our top priority, and we want to ensure that you have a seamless experience with our products and services.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5047                    | 27.0219                   | 27.5266                   | CPU        | 0.2812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  74%|███████▍  | 37/50 [09:29<05:00, 23.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 37/50 (Device: CPU):\n",
      "Instruction: help mke chatting with a person\n",
      "New Input: help mke chatting with a person [Classified: Intent is 'contact_human_agent', Category is 'contact', Entities are mke (person_name)]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to chat with one of our representatives. our dedicated team is available during {{customer support hours}} to assist you with any questions or concerns you may have. please feel free to reach out to us during these hours, and we'll be more than happy to help you. is there anything specific you need assistance with?\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 1.9476                    | 18.2427                   | 20.1903                   | CPU        | 264.3984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  76%|███████▌  | 38/50 [09:53<04:38, 23.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 38/50 (Device: CPU):\n",
      "Instruction: there are errors trying to update my shipping address\n",
      "New Input: there are errors trying to update my shipping address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are shipping (account_category)]\n",
      "Generated Response: i'm sorry to hear that you're encountering errors while updating your delivery address. let's work together to resolve this issue. could you please provide more details about the specific errors you are facing? this will help us identify the root cause and provide you with the most accurate solution. thank you for bringing this to our attention, and rest assured, we are here to assist you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 1.2717                    | 22.0360                   | 23.3077                   | CPU        | 12.1055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  78%|███████▊  | 39/50 [10:11<03:58, 21.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 39/50 (Device: CPU):\n",
      "Instruction: i wnt help swapping several items of order ord-9073141\n",
      "New Input: i wnt help swapping several items of order ord-9073141 [Classified: Intent is 'change_order', Category is 'order', Entities are w (currency_symbol), ord-9073141 (order_number)]\n",
      "Generated Response: i understand that you need assistance with swapping multiple items in order number {{order number}}. i'm here to help you with that. could you please provide me with the specific details of the items you would like to swap? this will allow me to guide you through the process and ensure that your order is just as you desire.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 1.1244                    | 17.0783                   | 18.2028                   | CPU        | 3.5117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  80%|████████  | 40/50 [10:43<04:08, 24.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 40/50 (Device: CPU):\n",
      "Instruction: i don't know how i can order some of your article\n",
      "New Input: i don't know how i can order some of your article [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for reaching out! i completely understand that you're unsure about how to place an order for our articles. let me guide you through the process step by step. first, you can visit our website and browse through our wide range of products. once you find the articles you want to order, simply add them to your cart. after that, proceed to the checkout page, where you'll need to provide your shipping and payment information. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team, and they will be more than happy to assist you further. we're here to help you every step of the way!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.9981                    | 31.0681                   | 32.0663                   | CPU        | 5.6094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  82%|████████▏ | 41/50 [11:07<03:40, 24.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 41/50 (Device: CPU):\n",
      "Instruction: i don't know how i could modify my address\n",
      "New Input: i don't know how i could modify my address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about how to modify your address. let me guide you through the process step by step. first, please log in to your account and navigate to the \"my account\" or \"profile\" section. from there, you should be able to find an option to edit your shipping address or add a new one. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team for assistance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.9689                    | 22.8155                   | 23.7843                   | CPU        | 0.5273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  84%|████████▍ | 42/50 [11:30<03:12, 24.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 42/50 (Device: CPU):\n",
      "Instruction: order ord-9186353 status\n",
      "New Input: order ord-9186353 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-9186353 status (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status of your order with the order number {{order number}}. to provide you with accurate information, could you please provide me with your email address or phone number associated with this order? this will help me locate the specific order and give you the most up-to-date status.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.9512                    | 22.1919                   | 23.1431                   | CPU        | 0.3984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  86%|████████▌ | 43/50 [11:49<02:37, 22.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 43/50 (Device: CPU):\n",
      "Instruction: help me making a claim against your business\n",
      "New Input: help me making a claim against your business [Classified: Intent is 'complaint', Category is 'feedback', Entities are business (account_type)]\n",
      "Generated Response: i'll take care of it! i'm here to guide you through the process of filing a complaint against our business. your feedback is incredibly valuable to us, and we want to ensure that your concerns are addressed appropriately. to begin, could you please provide me with more details about the nature of your claim? this will allow me to better understand your situation and provide you with the most appropriate assistance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.9103                    | 18.0436                   | 18.9539                   | CPU        | 0.6328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  88%|████████▊ | 44/50 [12:24<02:38, 26.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 44/50 (Device: CPU):\n",
      "Instruction: i have got to see in which cases can i request my money back\n",
      "New Input: i have got to see in which cases can i request my money back [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm happy to help! i completely understand your need to have clarity on the situations where you can request a refund. allow me to provide you with a comprehensive breakdown: 1. **product/service defect:** if you receive a product or service that has a defect or does not meet the description provided, you are generally eligible for a reimbursement. however, it's important to note that the exact nature of the defect may vary based on several factors, such as the product's quality, the vendor's policies, and the specific circumstances surrounding the purchase. make sure to review the terms and conditions section of your order or contact our customer support team directly for detailed information on how to proceed with your refund request.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.9583                    | 34.5677                   | 35.5260                   | CPU        | 1.1484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  90%|█████████ | 45/50 [12:58<02:22, 28.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 45/50 (Device: CPU):\n",
      "Instruction: can you help me report troubles with payment?\n",
      "New Input: can you help me report troubles with payment? [Classified: Intent is 'payment_issue', Category is 'payment', Entities are none]\n",
      "Generated Response: unquestionably! i'm here to assist you in reporting any troubles you're facing with your payment. your feedback is incredibly valuable to us, as it helps us identify areas for improvement and ensure a seamless payment experience for all our customers. to report any payment issues, i recommend reaching out to our dedicated customer support team. they have the expertise to guide you through the process and address any concerns you may have.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.9794                    | 32.5653                   | 33.5447                   | CPU        | 0.4414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  92%|█████████▏| 46/50 [13:14<01:39, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 46/50 (Device: CPU):\n",
      "Instruction: i try to notify of an error with registrations\n",
      "New Input: i try to notify of an error with registrations [Classified: Intent is 'registration_problems', Category is 'payment', Entities are none]\n",
      "Generated Response: we understand that encountering errors during the registration process can be frustrating, and we apologize for any inconvenience caused. to assist you further, could you please provide more details about the specific error you are facing? this information will help us investigate and resolve the issue promptly. thank you for bringing this to our attention.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.8879                    | 15.0319                   | 15.9197                   | CPU        | 0.2969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  94%|█████████▍| 47/50 [13:34<01:10, 23.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 47/50 (Device: CPU):\n",
      "Instruction: what do i have to do to cancel my platinum account?\n",
      "New Input: what do i have to do to cancel my platinum account? [Classified: Intent is 'newsletter_subscription', Category is 'subscription', Entities are platinum (account_type)]\n",
      "Generated Response: i'm here to assist you with canceling your {{account type}} account. to proceed with the cancellation, i kindly request you to provide me with your email address or any other relevant information related to your subscription. rest assured, your privacy and security are of utmost importance to us, and we will handle your request promptly and securely. if you have any further questions or concerns, feel free to let me know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.9534                    | 19.4594                   | 20.4128                   | CPU        | 6.7578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  96%|█████████▌| 48/50 [13:54<00:44, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 48/50 (Device: CPU):\n",
      "Instruction: how can i submit some feedback abot ur company\n",
      "New Input: how can i submit some feedback abot ur company [Classified: Intent is 'review', Category is 'feedback', Entities are ab (person_name)]\n",
      "Generated Response: thank you for expressing your interest in providing feedback on our company. to submit your feedback, you can visit our website and navigate to the \"contact us\" or \"feedback\" section. there you will find a form or contact information to reach out to our dedicated customer support team. they will be more than happy to assist you with the feedback submission process. we appreciate your time and look forward to hearing from you!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.8852                    | 18.9753                   | 19.8605                   | CPU        | 0.4570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU:  98%|█████████▊| 49/50 [14:13<00:21, 21.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 49/50 (Device: CPU):\n",
      "Instruction: change  order ord-8335461\n",
      "New Input: change  order ord-8335461 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-8335461 (order_number)]\n",
      "Generated Response: i'm sorry to hear that you need to change order number {{order number}}. i understand how important it is to have accurate information about your order. to assist you further, could you please provide me with the specific details of the changes you would like to make? this will help me guide you through the process accurately and efficiently.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.8880                    | 17.5057                   | 18.3937                   | CPU        | 0.0898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CPU: 100%|██████████| 50/50 [14:29<00:00, 17.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 50/50 (Device: CPU):\n",
      "Instruction: i need assistance to send some feedback for ur produucts\n",
      "New Input: i need assistance to send some feedback for ur produucts [Classified: Intent is 'review', Category is 'feedback', Entities are produ (person_name)]\n",
      "Generated Response: i'll take care of it! i'm here to assist you in sending your feedback about our products and services. your opinion matters to us, and we appreciate your willingness to share your thoughts with us. please let me know if you have any specific questions or if there's anything else i can help you with.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.9115                    | 15.1799                   | 16.0913                   | CPU        | 0.3125\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK SUMMARY FOR CPU\n",
      "================================================================================\n",
      "Average Classification Time: 0.7530 seconds\n",
      "Average Generation Time: 16.6287 seconds\n",
      "Average Overall Time: 17.3817 seconds\n",
      "Average Memory Usage: 18.1313 MB\n",
      "\n",
      "================================================================================\n",
      "RUNNING BENCHMARK ON CUDA\n",
      "================================================================================\n",
      "\n",
      "Loading models on CUDA...\n",
      "Loading from: ../..//Baseline_freeze_v1\\label_encoders.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing classification model...\n",
      "All GPT-2 layers remain frozen\n",
      "Classification model initialized with loss weights: {'intent': 0.3, 'category': 0.3, 'ner': 0.4}\n",
      "Attempting to load generation model from ../../text_generation_results_03-09-25/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   2%|▏         | 1/50 [00:15<13:00, 15.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1/50 (Device: CUDA):\n",
      "Instruction: i do not know what i have to do to get my invoice #85632\n",
      "New Input: i do not know what i have to do to get my invoice #85632 [Classified: Intent is 'get_invoice', Category is 'invoice', Entities are # (refund_amount)]\n",
      "Generated Response: i understand that you're unsure about the steps to take in order to retrieve your invoice with the number #{{invoices number}}. don't worry, i'm here to guide you through the process. first, you'll need to log in to your account on our website. once logged in, navigate to the \"billing\" or \"invoices\" section, where you should be able to find a list of your available invoice types. look for a tab or link that says \"view invoice,\" and click on it to access the invoice details. if you encounter any difficulties or have further questions, feel free to let me know, and i'll be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.7829                    | 15.1416                   | 15.9245                   | CUDA        | 2237.2349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   4%|▍         | 2/50 [00:27<10:42, 13.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 2/50 (Device: CUDA):\n",
      "Instruction: transfer to live agent\n",
      "New Input: transfer to live agent [Classified: Intent is 'create_account', Category is 'account', Entities are live (account_type), agent (account_type)]\n",
      "Generated Response: i appreciate your interest in transferring to our {{account type}} agent. i'm here to assist you every step of the way. to get started, could you please provide me with your full name, email address, and a preferred username? once i have these details, i'll guide you through the process step by step, ensuring a seamless experience for you. rest assured, your security and privacy are of utmost importance to us, so your information will be handled with the utmost care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5961                    | 11.0089                   | 11.6050                   | CUDA        | 2178.1631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   6%|▌         | 3/50 [00:37<09:07, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 3/50 (Device: CUDA):\n",
      "Instruction: i need help to see how long refunds take\n",
      "New Input: i need help to see how long refunds take [Classified: Intent is 'delivery_period', Category is 'delivery', Entities are none]\n",
      "Generated Response: i'll take care of it! i understand that you would like to know the estimated time it takes for refunds to be processed. to provide you with accurate information, could you please provide me with the {{order number}} or any other relevant details? this will enable me to check the status of your refund and give you an estimated delivery date. thank you for your cooperation, and i'm here to assist you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5658                    | 9.0262                   | 9.5920                   | CUDA        | 2159.3882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:   8%|▊         | 4/50 [00:47<08:35, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 4/50 (Device: CUDA):\n",
      "Instruction: help to pseak to customer support\n",
      "New Input: help to pseak to customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are customer (account_type)]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to know how to contact our dedicated customer assistance team. to get in touch with us, you can reach out to us through various channels such as phone, email, or live chat on our website at {{website url}}. our team is ready to assist you with any questions or concerns you may have. feel free to let me know if there's anything else i can do to help you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6429                    | 9.8592                   | 10.5021                   | CUDA        | 2175.4072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  10%|█         | 5/50 [01:05<10:17, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 5/50 (Device: CUDA):\n",
      "Instruction: i do not know what to do to swtch to the premium account\n",
      "New Input: i do not know what to do to swtch to the premium account [Classified: Intent is 'switch_account', Category is 'account', Entities are sw (person_name), premium (account_type)]\n",
      "Generated Response: i understand that you're unsure about the steps to switch to our {{account type}} account, and i'm here to guide you through the process. allow me to provide you with the information you need to make the switch smoothly: first, log in to your account on our platform using your username and password. this will allow you to access the account settings, preferences, or any other relevant information. once logged in, locate the \"account settings\" or \"profile\" section, usually located in the top right corner of the page. you should see an option to \"switch account type.\" click on that option and you'll be directed to a page where you can select or select a different type of account. if you encounter any difficulties or have further questions along the way, please don't hesitate to let me know. it's my goal to ensure a seamless experience for you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6152                    | 17.5938                   | 18.2090                   | CUDA        | 2269.5508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  12%|█▏        | 6/50 [01:20<10:11, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 6/50 (Device: CUDA):\n",
      "Instruction: i need assistance resetting the key of my user account\n",
      "New Input: i need assistance resetting the key of my user account [Classified: Intent is 'recover_password', Category is 'account', Entities are user (account_type)]\n",
      "Generated Response: i'm here to provide you with the assistance you need to reset your key to your account. rest assured, i'll guide you through the process step by step to ensure a smooth and secure recovery. to get started, could you please provide me with some additional information? specifically, it would be helpful to know the platform or service you are using and any specific instructions or steps you have already followed. this will help me narrow down the search and find the best solution for you.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6112                    | 13.6318                   | 14.2429                   | CUDA        | 2210.7925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  14%|█▍        | 7/50 [01:30<09:09, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 7/50 (Device: CUDA):\n",
      "Instruction: i want help taking a quick look at the bills from john robertson\n",
      "New Input: i want help taking a quick look at the bills from john robertson [Classified: Intent is 'check_invoice', Category is 'invoice', Entities are none]\n",
      "Generated Response: i'm happy to help! i understand that you would like to quickly view the invoices from {{salutation}} {{client last name}}. to assist you with this, could you please provide me with some additional information such as the {{date range}} or any other relevant details? this will enable me to locate the specific bills you are looking for more efficiently. rest assured, i am here to guide you every step of the way and ensure a seamless experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5252                    | 9.9199                   | 10.4452                   | CUDA        | 2175.3926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  16%|█▌        | 8/50 [01:49<10:25, 14.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 8/50 (Device: CUDA):\n",
      "Instruction: i can't enter a delivery address\n",
      "New Input: i can't enter a delivery address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are delivery (account_category)]\n",
      "Generated Response: i'm sorry to hear that you're having trouble entering a {{account category}} address. let's work together to resolve this issue. here's what you can do: 1. double-check the format: make sure you have entered all the required information correctly, including the street name, house number, city, state, zip code, and any other relevant details. ensure that the address you've entered is accurate and matches the provided format. sometimes, typos or missing information can prevent a successful address entry, but don't worry, we're here to assist you every step of the way to ensure your address is entered correctly and in line with your needs. btw, if you encounter any difficulties during the process, feel free to reach out to our customer support team for further assistance. they will be more than happy to guide you through the necessary steps and address any concerns you may have.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6930                    | 18.7108                   | 19.4037                   | CUDA        | 2266.3428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  18%|█▊        | 9/50 [01:58<08:53, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 9/50 (Device: CUDA):\n",
      "Instruction: can you help me place an order?\n",
      "New Input: can you help me place an order? [Classified: Intent is 'place_order', Category is 'delivery', Entities are none]\n",
      "Generated Response: i'll take care of it! i'm here to assist you in placing your order. please provide me with the details of the items you would like to order, such as their names, quantities, and any other relevant information. once i have this information, i will guide you through the process step by step, ensuring a seamless and satisfying experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6270                    | 8.2739                   | 8.9009                   | CUDA        | 2152.4204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  20%|██        | 10/50 [02:32<12:58, 19.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 10/50 (Device: CUDA):\n",
      "Instruction: i do not know how i can set up my shipping address\n",
      "New Input: i do not know how i can set up my shipping address [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are shipping (account_category)]\n",
      "Generated Response: i'm attuned to the idea that you're unsure about the process of setting up your shipping addresses. allow me to guide you through it step by step: 1. log in to your account on our website or mobile app using your username and password. 2. look for the \"my account\" or \"profile\" section, usually located in the top right corner of the page. 3. within the account settings or profile settings, you should find an option to \"add a new address.\" 4. click on that option, and you'll be prompted to enter the email address associated with your {{account category}} account. 5. fill out the required fields with the necessary information, such as your name, contact details, city, state, postal code, or any other relevant details. 6. once you've completed the form fields, keep an eye on your inbox for an email with detailed instructions on how to update your address. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team for assistance. they are available {{customer support hours}} at {{toll-free number}} or through the live chat on {{website url}}. we're here to help you with any questions or concerns you may have. how else may i assist you today?\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6111                    | 33.2749                   | 33.8861                   | CUDA        | 2374.8301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  22%|██▏       | 11/50 [02:43<10:51, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 11/50 (Device: CUDA):\n",
      "Instruction: i need assistance to set my new delovery address up\n",
      "New Input: i need assistance to set my new delovery address up [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are new (account_type), del (person_name)]\n",
      "Generated Response: i understand your need for assistance in setting up your new delivery address. rest assured, i'm here to guide you through the process and ensure a seamless experience. to get started, could you please provide me with more details about the specific issue or question you have? this will enable me to provide you with the most accurate and tailored assistance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6661                    | 9.7934                   | 10.4594                   | CUDA        | 2161.8745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  24%|██▍       | 12/50 [02:53<09:22, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 12/50 (Device: CUDA):\n",
      "Instruction: help me to check what payment methods are available\n",
      "New Input: help me to check what payment methods are available [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the available payment options. to provide you with accurate information, could you please provide me with your preferred payment method? once i have that, i'll be able to guide you through the process step by step. your satisfaction is our top priority, and we want to ensure that you have a seamless experience with us.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6414                    | 9.7921                   | 10.4335                   | CUDA        | 2154.0610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  26%|██▌       | 13/50 [03:05<08:29, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 13/50 (Device: CUDA):\n",
      "Instruction: i call to leave my feedback for ur services\n",
      "New Input: i call to leave my feedback for ur services [Classified: Intent is 'review', Category is 'feedback', Entities are none]\n",
      "Generated Response: thank you for choosing to provide feedback on our services! your feedback is incredibly valuable to us as it helps us understand what we are doing well and areas where we can improve. i would be more than happy to assist you in leaving your valuable feedback. please let me know if you have any specific questions or if there's anything else i can help you with.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6651                    | 10.7240                   | 11.3891                   | CUDA        | 2152.1489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  28%|██▊       | 14/50 [03:17<08:01, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 14/50 (Device: CUDA):\n",
      "Instruction: open freemium account for wife\n",
      "New Input: open freemium account for wife [Classified: Intent is 'create_account', Category is 'account', Entities are fre (person_name), ium (account_type), wife (account_type)]\n",
      "Generated Response: i appreciate your interest in opening a {{account type}} account with us! to get started, could you please provide me with your wife's full name, email address, and a preferred username? once i have these details, i will guide you through the account creation process step by step, ensuring a seamless experience for both of you. rest assured, we value your privacy and will handle your information with the utmost care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5769                    | 11.8611                   | 12.4380                   | CUDA        | 2178.4561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  30%|███       | 15/50 [03:27<07:16, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 15/50 (Device: CUDA):\n",
      "Instruction: i am trying to see the status of order ord-1135283\n",
      "New Input: i am trying to see the status of order ord-1135283 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-1135283 (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the current status and estimated time of arrival (eta) for your order with the order number {{order number}}. to provide you with accurate information, could you please provide me with your account details or any other relevant information? once i have these details, i will be able to guide you through the process and ensure that you receive the most up-to-date status.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6007                    | 9.7729                   | 10.3736                   | CUDA        | 2175.4814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  32%|███▏      | 16/50 [03:41<07:13, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 16/50 (Device: CUDA):\n",
      "Instruction: wanna see ur payment modalities will yahelp me\n",
      "New Input: wanna see ur payment modalities will yahelp me [Classified: Intent is 'check_payment_methods', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in exploring our available payment methods. to view our accepted payment options, you can visit our website and navigate to the \"payment methods\" or \"checkout\" section. there you will find a comprehensive list of all the payment modifications we offer, including credit/debit cards, paypal, bank transfers, apple pay, and google wallet. if you have any specific questions or need further assistance, please don't hesitate to let me know. i'll be more than happy to help you every step of the way!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6778                    | 12.7466                   | 13.4243                   | CUDA        | 2194.8042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  34%|███▍      | 17/50 [03:52<06:49, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 17/50 (Device: CUDA):\n",
      "Instruction: what do i ned to do to change to the real estate account?\n",
      "New Input: what do i ned to do to change to the real estate account? [Classified: Intent is 'switch_account', Category is 'account', Entities are real (account_type), estate (account_category)]\n",
      "Generated Response: i appreciate your interest in upgrading to our {{account type}} account, and i'm here to guide you through the process step by step. to get started, could you please provide me with your account details, such as your username or email address? once i have this information, i'll be able to assist you further and ensure a smooth transition. remember, your security and privacy are of utmost importance to us, so rest assured that your information will be handled with the highest level of care.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5390                    | 11.0911                   | 11.6300                   | CUDA        | 2187.6567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  36%|███▌      | 18/50 [04:01<06:01, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 18/50 (Device: CUDA):\n",
      "Instruction: notify of errorw with online registration\n",
      "New Input: notify of errorw with online registration [Classified: Intent is 'registration_problems', Category is 'payment', Entities are online (account_type)]\n",
      "Generated Response: we understand that encountering errors during the registration process can be frustrating. rest assured, we are here to assist you in resolving this issue. could you please provide more details about the specific error you are facing? this will help us understand the situation better and provide you with the most appropriate solution. thank you for bringing this to our attention, and we appreciate your patience as we work together to resolve this matter.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5337                    | 8.1818                   | 8.7155                   | CUDA        | 2156.5977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  38%|███▊      | 19/50 [04:14<06:01, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 19/50 (Device: CUDA):\n",
      "Instruction: another standard accounmt for father\n",
      "New Input: another standard accounmt for father [Classified: Intent is 'create_account', Category is 'account', Entities are standard (account_type), acc (delivery_country), father (account_type)]\n",
      "Generated Response: assuredly! i'm here to assist you in creating another {{account type}} account for your father. to get started, could you please provide me with your dad's full name, email address, and a preferred username? once i have these details, i'll guide you through the account creation process step by step, ensuring a seamless experience for both of you. rest assured, we prioritize the security and privacy of our customers' information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6224                    | 11.8116                   | 12.4339                   | CUDA        | 2196.2715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  40%|████      | 20/50 [04:26<05:59, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 20/50 (Device: CUDA):\n",
      "Instruction: i need infrmation about the cancellation of my enterprise account\n",
      "New Input: i need infrmation about the cancellation of my enterprise account [Classified: Intent is 'delete_account', Category is 'account', Entities are inf (person_name), enterprise (account_type)]\n",
      "Generated Response: i understand that you need assistance with canceling your {{account type}} account. i'm here to help you with that. could you please provide me with some additional information? specifically, i would need to know the platform or service you are referring to, the reason for cancellation, and any additional details you may have. this will allow me to guide you through the process and ensure a smooth cancellation experience. remember, your satisfaction is our top priority, so please don't hesitate to reach out if you have any further questions or concerns.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6131                    | 12.1209                   | 12.7341                   | CUDA        | 2195.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  42%|████▏     | 21/50 [04:34<05:07, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 21/50 (Device: CUDA):\n",
      "Instruction: i want help ordering a few of your product\n",
      "New Input: i want help ordering a few of your product [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in placing an order for our products. to get started, could you please provide me with more details about the specific products you would like to order? this will help me guide you through the ordering process and ensure a seamless experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6047                    | 6.7799                   | 7.3846                   | CUDA        | 2144.0498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  44%|████▍     | 22/50 [04:43<04:46, 10.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 22/50 (Device: CUDA):\n",
      "Instruction: checking status of order ord-6772250\n",
      "New Input: checking status of order ord-6772250 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-6772250 (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status and estimated time of arrival (eta) for your order with the order number {{order number}}. to provide you with accurate information, could you please provide me with your full name, email address, or any other relevant details? this will help me locate the specific order and give you the most up-to-date status.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5445                    | 8.7797                   | 9.3242                   | CUDA        | 2161.8730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  46%|████▌     | 23/50 [04:53<04:36, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 23/50 (Device: CUDA):\n",
      "Instruction: deleting government account\n",
      "New Input: deleting government account [Classified: Intent is 'delete_account', Category is 'account', Entities are government (account_type)]\n",
      "Generated Response: i understand that you would like to delete your {{account type}} account. i apologize for any inconvenience this may have caused you. to assist you further, could you please provide me with your account details or any specific concerns you have regarding the deletion process? this will enable me to guide you through the necessary steps and ensure a smooth account deletion experience. your satisfaction is our top priority, and i'm here to support you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5385                    | 9.8001                   | 10.3385                   | CUDA        | 2159.4756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  48%|████▊     | 24/50 [05:02<04:10,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 24/50 (Device: CUDA):\n",
      "Instruction: my experience with your brand was bad, make a consumer claim\n",
      "New Input: my experience with your brand was bad, make a consumer claim [Classified: Intent is 'complaint', Category is 'feedback', Entities are consumer (account_type)]\n",
      "Generated Response: i apologize for the negative experience you've had with our brand. your feedback is valuable to us, and we want to address your concerns promptly and effectively. could you please provide me with more details about the specific issue you encountered? this will help us understand the situation better and take appropriate actions to resolve it. thank you for bringing this to our attention.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5719                    | 7.6432                   | 8.2151                   | CUDA        | 2153.6304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  50%|█████     | 25/50 [05:14<04:18, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 25/50 (Device: CUDA):\n",
      "Instruction: wann locate order ord-3779640\n",
      "New Input: wann locate order ord-3779640 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-3779640 (order_number)]\n",
      "Generated Response: thank you for trusting us! i'm fully aware of you would like to locate your order with the order number {{order number}}. to assist you further, could you please provide me with some additional information such as the platform or website you used to make the purchase? this will help me provide you with accurate and up-to-date information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5962                    | 11.3961                   | 11.9922                   | CUDA        | 2181.3271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  52%|█████▏    | 26/50 [05:27<04:33, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 26/50 (Device: CUDA):\n",
      "Instruction: i have to sde in what cases can i request a refund\n",
      "New Input: i have to sde in what cases can i request a refund [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the cases in which you can request refunds. here are some common scenarios where you may be eligible for a reimbursement: 1. **product/service defect:** if the product or service you received has a defect or does not meet the description provided, you usually have grounds to request compensation.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5214                    | 13.2764                   | 13.7978                   | CUDA        | 2200.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  54%|█████▍    | 27/50 [05:34<03:50, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 27/50 (Device: CUDA):\n",
      "Instruction: i'm trying to see at what time i can call customer support\n",
      "New Input: i'm trying to see at what time i can call customer support [Classified: Intent is 'contact_customer_service', Category is 'contact', Entities are customer (account_type)]\n",
      "Generated Response: we're here to help! i take note that you would like to know the operating hours during which you can reach our dedicated customer assistance team. our team is available to assist you during the following hours: - {{client support hours}}- how else may i be of assistance to you?\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5904                    | 6.3109                   | 6.9013                   | CUDA        | 2150.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  56%|█████▌    | 28/50 [05:43<03:33,  9.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 28/50 (Device: CUDA):\n",
      "Instruction: assistance sending some feedback for ur proiducts\n",
      "New Input: assistance sending some feedback for ur proiducts [Classified: Intent is 'review', Category is 'feedback', Entities are pro (account_type)]\n",
      "Generated Response: thank you for choosing to provide feedback on our {{account type}} account! your feedback is incredibly valuable to us as it helps us understand your needs and preferences better. i'm here to guide you through the process of sending your valuable feedback. please let me know if you have any specific questions or if there's anything else i can assist you with.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6087                    | 8.3540                   | 8.9627                   | CUDA        | 2153.9888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  58%|█████▊    | 29/50 [05:52<03:20,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 29/50 (Device: CUDA):\n",
      "Instruction: need assistance to cancel order ord-8227420\n",
      "New Input: need assistance to cancel order ord-8227420 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8227420 (order_number)]\n",
      "Generated Response: i'm sorry to hear that you're having trouble canceling your order with the order number {{order number}}. our team is here to assist you every step of the way. please provide us with more details about the specific issue or error you encountered during the cancellation process. your satisfaction is our top priority, and we're committed to resolving this matter for you as quickly as possible.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5942                    | 8.4988                   | 9.0930                   | CUDA        | 2161.1553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  60%|██████    | 30/50 [06:14<04:24, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 30/50 (Device: CUDA):\n",
      "Instruction: need assistance cancelinv purchase ord-8042947\n",
      "New Input: need assistance cancelinv purchase ord-8042947 [Classified: Intent is 'cancel_order', Category is 'order', Entities are ord-8042947 (order_number)]\n",
      "Generated Response: i'm sorry to hear that you're having trouble canceling your purchase with the order number {{order number}}. your satisfaction is our top priority, and we're here to assist you every step of the way. to cancel your order, please follow these instructions: 1. sign in to your account: access our platform by logging into your {{online company portal info}} and navigating to the '{{online order interaction}}' section. you'll find a sign-up form where you can enter your registered email address to complete the cancellation process. 2. locate your orders: once logged in, locate the section that provides detailed information about your purchases, including the purchase number. this will help you find the specific order you need. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5938                    | 21.1772                   | 21.7710                   | CUDA        | 2287.8667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  62%|██████▏   | 31/50 [06:32<04:37, 14.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 31/50 (Device: CUDA):\n",
      "Instruction: i don't know how i can download my bill #12588\n",
      "New Input: i don't know how i can download my bill #12588 [Classified: Intent is 'get_invoice', Category is 'invoice', Entities are # (refund_amount)]\n",
      "Generated Response: i'm sorry to hear that you're unsure about how to download your bill with the number #{{salutation}} {{client last name}}. let me guide you through the process step by step. first, please log in to your account on our website. once logged in, navigate to the \"billing\" or \"invoices\" section. there, you should be able to find a list of your invoices. look for a download button or link that says \"download invoice\" and click on it. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team. they will be more than happy to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6154                    | 17.2804                   | 17.8958                   | CUDA        | 2242.5566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  64%|██████▍   | 32/50 [06:41<03:52, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 32/50 (Device: CUDA):\n",
      "Instruction: i need help to see purchase ord-0521414 status\n",
      "New Input: i need help to see purchase ord-0521414 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-0521414 (order_number)]\n",
      "Generated Response: thank you for reaching out! i'm here to assist you in checking the status of your purchase with the purchase number {{order number}}. to provide you with accurate information, could you please provide me with your account details or any other relevant information? once i have the necessary details, i'll be able to guide you through the process step by step.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6231                    | 8.3265                   | 8.9496                   | CUDA        | 2162.4424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  66%|██████▌   | 33/50 [06:51<03:25, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 33/50 (Device: CUDA):\n",
      "Instruction: am i entitled to a refund according to your policy?\n",
      "New Input: am i entitled to a refund according to your policy? [Classified: Intent is 'get_refund', Category is 'refund', Entities are none]\n",
      "Generated Response: i'll take care of it! i understand that you would like to know the status of your refund. your satisfaction is our top priority, and i'm here to assist you every step of the way. to provide you with accurate information, could you please provide me with your order number or any other relevant details? this will help me locate your case and guide you through the refund process.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5751                    | 9.6307                   | 10.2058                   | CUDA        | 2160.9146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  68%|██████▊   | 34/50 [07:21<04:36, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 34/50 (Device: CUDA):\n",
      "Instruction: need to check what methods for delivery i can choose help me\n",
      "New Input: need to check what methods for delivery i can choose help me [Classified: Intent is 'delivery_options', Category is 'delivery', Entities are none]\n",
      "Generated Response: i'll take care of it! i'm here to assist you in exploring the delivery methods available to you. let's dive into the possibilities: 1. standard shipping: this method typically takes {{date range}} business days for your package to arrive at your doorstep. it's perfect for non-urgent items when you have a bit more time to spare before heading to the next destination. you can also choose this option if you prefer a cost-effective delivery method that is faster and more likely to reach its destination in a matter of a few days or a couple of business nights. if your items are not in your reach-out inbox within this timeframe, i recommend reaching out to our customer support team. they will be more than happy to guide you through the process and provide you with all the necessary information and support to make your delivery experience seamless and efficient.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4588                    | 28.8712                   | 29.3299                   | CUDA        | 2358.5386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  70%|███████   | 35/50 [07:28<03:33, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 35/50 (Device: CUDA):\n",
      "Instruction: ya have an address to submit some feednack\n",
      "New Input: ya have an address to submit some feednack [Classified: Intent is 'set_up_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in submitting your delivery address. to ensure a smooth process, could you please provide me with more details about the platform or service you are using? this will help me guide you through the necessary steps and provide you with accurate information.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6205                    | 6.5144                   | 7.1349                   | CUDA        | 2146.0566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  72%|███████▏  | 36/50 [07:53<04:06, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 36/50 (Device: CUDA):\n",
      "Instruction: show me in which situations  can i ask for refunds\n",
      "New Input: show me in which situations  can i ask for refunds [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm here to assist you in understanding the situations where you can request a refund. let me provide you with a comprehensive breakdown: 1. **product/service defect:** if you receive a product or service that has a defect or does not meet the description provided, you are generally eligible for a reimbursement. however, it's important to note that the exact nature of the defect may vary depending on the brand, service provider, and the specific circumstances surrounding the product. it would be helpful to have all the relevant details, such as the date of purchase or any other relevant information, so that i can guide you through the refund process more effectively and ensure a smooth resolution.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5919                    | 24.8026                   | 25.3945                   | CUDA        | 2329.0801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  74%|███████▍  | 37/50 [08:01<03:12, 14.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 37/50 (Device: CUDA):\n",
      "Instruction: help mke chatting with a person\n",
      "New Input: help mke chatting with a person [Classified: Intent is 'contact_human_agent', Category is 'contact', Entities are mke (person_name)]\n",
      "Generated Response: thank you for reaching out! i understand that you would like to chat with someone. our team of dedicated human representatives is here to assist you. please allow me a moment to connect you with one of our representatives who will be more than happy to help you further. your satisfaction is our top priority, and we appreciate your patience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6012                    | 7.7500                   | 8.3512                   | CUDA        | 2152.5171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  76%|███████▌  | 38/50 [08:08<02:29, 12.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 38/50 (Device: CUDA):\n",
      "Instruction: there are errors trying to update my shipping address\n",
      "New Input: there are errors trying to update my shipping address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are shipping (account_category)]\n",
      "Generated Response: i'm sorry to hear that you're encountering errors while updating your delivery address. let's work together to resolve this issue. could you please provide more details about the specific errors you are facing? this will help us better understand the problem and provide you with the most accurate solution.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.4378                    | 6.6073                   | 7.0451                   | CUDA        | 2146.4746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  78%|███████▊  | 39/50 [08:18<02:07, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 39/50 (Device: CUDA):\n",
      "Instruction: i wnt help swapping several items of order ord-9073141\n",
      "New Input: i wnt help swapping several items of order ord-9073141 [Classified: Intent is 'change_order', Category is 'order', Entities are w (currency_symbol), ord-9073141 (order_number)]\n",
      "Generated Response: i understand that you need assistance with swapping multiple items in your order with the number {{order number}}. i apologize for any inconvenience this may have caused. to better assist you, could you please provide more details about the items you would like to swap? this will help us ensure that we process your request accurately and efficiently. your satisfaction is our top priority, and we are here to support you every step of the way.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5833                    | 9.0279                   | 9.6112                   | CUDA        | 2177.6973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  80%|████████  | 40/50 [08:25<01:42, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 40/50 (Device: CUDA):\n",
      "Instruction: i don't know how i can order some of your article\n",
      "New Input: i don't know how i can order some of your article [Classified: Intent is 'place_order', Category is 'order', Entities are none]\n",
      "Generated Response: i'm here to guide you through the process of placing an order for our articles. to get started, could you please provide me with more details about the specific articles you would like to order? this will help me provide you with accurate information and ensure a seamless ordering experience.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6104                    | 6.4745                   | 7.0849                   | CUDA        | 2145.0908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  82%|████████▏ | 41/50 [08:36<01:34, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 41/50 (Device: CUDA):\n",
      "Instruction: i don't know how i could modify my address\n",
      "New Input: i don't know how i could modify my address [Classified: Intent is 'change_shipping_address', Category is 'shipping', Entities are none]\n",
      "Generated Response: i'm sorry to hear that you're unsure about how to modify your address. let me guide you through the process step by step. first, log in to your account on our website. then, navigate to the \"my account\" or \"profile\" section. look for the option to edit your shipping address and click on it. make the necessary changes and save the changes. if you encounter any difficulties or have further questions, feel free to reach out to our customer support team for assistance.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5412                    | 10.5980                   | 11.1392                   | CUDA        | 2175.4204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  84%|████████▍ | 42/50 [08:46<01:23, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 42/50 (Device: CUDA):\n",
      "Instruction: order ord-9186353 status\n",
      "New Input: order ord-9186353 status [Classified: Intent is 'track_order', Category is 'order', Entities are ord-9186353 status (order_number)]\n",
      "Generated Response: thank you for reaching out! i understand that you would like to check the status of your order with the order number {{order number}}. to provide you with accurate information, could you please provide me with your email address or phone number associated with this order? once i have that, i will be able to assist you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5947                    | 9.5360                   | 10.1308                   | CUDA        | 2168.6108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  86%|████████▌ | 43/50 [08:59<01:16, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 43/50 (Device: CUDA):\n",
      "Instruction: help me making a claim against your business\n",
      "New Input: help me making a claim against your business [Classified: Intent is 'complaint', Category is 'feedback', Entities are business (account_type)]\n",
      "Generated Response: i'm here to provide you with the assistance you need to make a complaint against our business. your feedback is important to us, and i want to ensure that your concerns are addressed promptly and effectively. to begin, could you please provide me with more details about the nature of your claim? this will enable me to guide you through the appropriate channels and work towards a resolution.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5846                    | 11.7573                   | 12.3419                   | CUDA        | 2169.4341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  88%|████████▊ | 44/50 [09:31<01:43, 17.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 44/50 (Device: CUDA):\n",
      "Instruction: i have got to see in which cases can i request my money back\n",
      "New Input: i have got to see in which cases can i request my money back [Classified: Intent is 'check_refund_policy', Category is 'refund', Entities are none]\n",
      "Generated Response: i'm happy to help! i understand that you would like to have a clear understanding of the situations where you can request a refund. let me provide you with a comprehensive breakdown: 1. **product/service defect:** if you receive a product or service that has a defect or does not meet the description provided, you are generally eligible for a reimbursement. however, it's important to note that it may take additional time for the product to reflect in your hands, and this can vary depending on the specific product(s) you received and the company's policies or procedures. in most cases, refunds are processed within a few business days after the return or cancellation request has been approved by the relevant department or the customer support team for review or review purposes.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.7982                    | 31.1960                   | 31.9943                   | CUDA        | 2307.5459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  90%|█████████ | 45/50 [09:52<01:31, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 45/50 (Device: CUDA):\n",
      "Instruction: can you help me report troubles with payment?\n",
      "New Input: can you help me report troubles with payment? [Classified: Intent is 'payment_issue', Category is 'payment', Entities are none]\n",
      "Generated Response: assuredly! i'm here to assist you in reporting any troubles you're facing with your payment. your feedback is incredibly valuable to us, as it helps us identify areas for improvement and ensure a seamless payment experience for all our customers. to report any payment issues, i recommend reaching out to our dedicated customer support team. they have the expertise to guide you through the process and address any concerns you may have. you can contact them through our helpline at {{customer support phone number}} or by visiting our website and initiating a live chat session with one of our representatives. rest assured, we're committed to resolving this matter promptly and ensuring your satisfaction.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6682                    | 20.0614                   | 20.7296                   | CUDA        | 2224.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  92%|█████████▏| 46/50 [10:04<01:05, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 46/50 (Device: CUDA):\n",
      "Instruction: i try to notify of an error with registrations\n",
      "New Input: i try to notify of an error with registrations [Classified: Intent is 'registration_problems', Category is 'payment', Entities are none]\n",
      "Generated Response: thank you for bringing this to our attention. we understand that encountering errors during the registration process can be frustrating, and we apologize for any inconvenience caused. to assist you further, could you please provide more details about the specific error you are facing? this will help us investigate the issue thoroughly and provide you with an appropriate solution. your feedback is highly valuable to us as it helps us improve our services and ensure a seamless registration experience for all our users.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.6485                    | 11.5295                   | 12.1780                   | CUDA        | 2161.8101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  94%|█████████▍| 47/50 [10:19<00:48, 16.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 47/50 (Device: CUDA):\n",
      "Instruction: what do i have to do to cancel my platinum account?\n",
      "New Input: what do i have to do to cancel my platinum account? [Classified: Intent is 'newsletter_subscription', Category is 'subscription', Entities are platinum (account_type)]\n",
      "Generated Response: i appreciate your interest in canceling your {{account type}} account. to proceed with the cancellation, i kindly request you to provide me with your email address or any other relevant information. this will enable me to locate your account and guide you through the necessary steps. rest assured, your privacy and security are of utmost importance to us, and we will handle your request promptly and securely. if you have any further questions or if there's anything else i can assist you with, please don't hesitate to let me know.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.5988                    | 14.8351                   | 15.4339                   | CUDA        | 2182.3823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  96%|█████████▌| 48/50 [10:34<00:31, 15.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 48/50 (Device: CUDA):\n",
      "Instruction: how can i submit some feedback abot ur company\n",
      "New Input: how can i submit some feedback abot ur company [Classified: Intent is 'review', Category is 'feedback', Entities are ab (person_name)]\n",
      "Generated Response: thank you for expressing your interest in providing feedback on our company! to submit your feedback, you can visit our website and navigate to the \"contact us\" or \"feedback\" section. there, there you will find a form or contact information to share your thoughts and experiences with our product or service. if you have any specific questions or need further assistance, please don't hesitate to let me know. i'm here to help!\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.8045                    | 14.2381                   | 15.0426                   | CUDA        | 2161.5894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA:  98%|█████████▊| 49/50 [11:09<00:21, 21.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 49/50 (Device: CUDA):\n",
      "Instruction: change  order ord-8335461\n",
      "New Input: change  order ord-8335461 [Classified: Intent is 'track_order', Category is 'order', Entities are ord-8335461 (order_number)]\n",
      "Generated Response: we understand that you would like to know the status of your order with the order number {{order number}}. to provide you with accurate information, could you please provide us with some additional details such as your name or email address associated with this order? this will enable us to locate the specific order and give you the most up-to-date status. thank you for your cooperation, and we look forward to assisting you further.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 0.7556                    | 34.2935                   | 35.0492                   | CUDA        | 2163.8970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on CUDA: 100%|██████████| 50/50 [11:35<00:00, 13.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 50/50 (Device: CUDA):\n",
      "Instruction: i need assistance to send some feedback for ur produucts\n",
      "New Input: i need assistance to send some feedback for ur produucts [Classified: Intent is 'review', Category is 'feedback', Entities are produ (person_name)]\n",
      "Generated Response: i'll take care of it! i'm here to assist you in sending your feedback about our products and services. your opinion matters to us, and we appreciate your willingness to share your thoughts. could you please provide me with more details about the specific product or service you would like to provide feedback on? this will help me guide you through the process more effectively.\n",
      "Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\n",
      "GPT2-Baseline Pipeline| 1.7705                    | 24.3571                   | 26.1275                   | CUDA        | 2158.7026\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK SUMMARY FOR CUDA\n",
      "================================================================================\n",
      "Average Classification Time: 0.6331 seconds\n",
      "Average Generation Time: 13.2808 seconds\n",
      "Average Overall Time: 13.9139 seconds\n",
      "Average Memory Usage: 2192.0493 MB\n",
      "CPU results saved to performance_results_baseline_cpu.json\n",
      "GPU results saved to performance_results_baseline_gpu.json\n",
      "Performance log saved to baseline_performance_log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance comparison plots saved to baseline_performance_comparison.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2Model, GPT2TokenizerFast, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def log_to_file(message):\n",
    "    \"\"\"Helper function for logging\"\"\"\n",
    "    print(message)\n",
    "\n",
    "class BaselineGPT2MultiTask(nn.Module):\n",
    "    def __init__(self, num_intents: int, num_categories: int, num_ner_labels: int,\n",
    "                 dropout_rate: float, loss_weights: Optional[Dict[str, float]] = None,\n",
    "                 ner_class_weights: Optional[torch.Tensor] = None,\n",
    "                 category_class_weights: Optional[torch.Tensor] = None,\n",
    "                 intent_class_weights: Optional[torch.Tensor] = None):\n",
    "        super().__init__()\n",
    "        log_to_file(\"Initializing classification model...\")\n",
    "        self.config = GPT2Config.from_pretrained('gpt2')\n",
    "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "        hidden_size = self.config.n_embd\n",
    "\n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False\n",
    "        log_to_file(\"All GPT-2 layers remain frozen\")\n",
    "\n",
    "        self.intent_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_intents)\n",
    "        )\n",
    "        self.category_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_categories)\n",
    "        )\n",
    "        self.ner_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_ner_labels)\n",
    "        )\n",
    "\n",
    "        self.loss_weights = loss_weights or {'intent': 0.3, 'category': 0.3, 'ner': 0.4}\n",
    "        self.intent_loss_fn = nn.CrossEntropyLoss(weight=intent_class_weights) if intent_class_weights is not None else nn.CrossEntropyLoss()\n",
    "        self.category_loss_fn = nn.CrossEntropyLoss(weight=category_class_weights) if category_class_weights is not None else nn.CrossEntropyLoss()\n",
    "        self.ner_loss_fn = nn.CrossEntropyLoss(weight=ner_class_weights) if ner_class_weights is not None else nn.CrossEntropyLoss()\n",
    "\n",
    "        log_to_file(f\"Classification model initialized with loss weights: {self.loss_weights}\")\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
    "                intent_labels: Optional[torch.Tensor] = None,\n",
    "                category_labels: Optional[torch.Tensor] = None,\n",
    "                ner_labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        masked_features = sequence_output * attention_mask.unsqueeze(-1)\n",
    "        sequence_repr = masked_features.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "\n",
    "        intent_logits = self.intent_head(sequence_repr)\n",
    "        category_logits = self.category_head(sequence_repr)\n",
    "        ner_logits = self.ner_head(sequence_output)\n",
    "\n",
    "        output_dict = {\n",
    "            'intent_logits': intent_logits,\n",
    "            'category_logits': category_logits,\n",
    "            'ner_logits': ner_logits\n",
    "        }\n",
    "\n",
    "        if all(label is not None for label in [intent_labels, category_labels, ner_labels]):\n",
    "            intent_loss = self.intent_loss_fn(intent_logits, intent_labels)\n",
    "            category_loss = self.category_loss_fn(category_logits, category_labels)\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = ner_logits.view(-1, ner_logits.size(-1))[active_loss]\n",
    "            active_labels = ner_labels.view(-1)[active_loss]\n",
    "            ner_loss = self.ner_loss_fn(active_logits, active_labels)\n",
    "\n",
    "            total_loss = (self.loss_weights['intent'] * intent_loss +\n",
    "                          self.loss_weights['category'] * category_loss +\n",
    "                          self.loss_weights['ner'] * ner_loss)\n",
    "\n",
    "            output_dict.update({\n",
    "                'loss': total_loss,\n",
    "                'intent_loss': intent_loss,\n",
    "                'category_loss': category_loss,\n",
    "                'ner_loss': ner_loss\n",
    "            })\n",
    "        return output_dict\n",
    "    \n",
    "def inference_baseline(model, text, tokenizer, label_encoders, max_length, device):\n",
    "        \"\"\"Run inference with the BaselineGPT2MultiTask model\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Tokenize input\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].to(device),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].to(device)\n",
    "        }\n",
    "\n",
    "        # Run model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Get softmax probabilities for confidence scores\n",
    "        intent_logits = outputs[\"intent_logits\"]\n",
    "        category_logits = outputs[\"category_logits\"]\n",
    "        ner_logits = outputs[\"ner_logits\"]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        intent_probs = torch.nn.functional.softmax(intent_logits, dim=-1)[0]\n",
    "        category_probs = torch.nn.functional.softmax(category_logits, dim=-1)[0]\n",
    "        ner_probs = torch.nn.functional.softmax(ner_logits, dim=-1)\n",
    "\n",
    "        # Get top predictions and their confidences\n",
    "        intent_pred = torch.argmax(intent_probs).cpu().item()\n",
    "        intent_confidence = intent_probs[intent_pred].cpu().item()  # As decimal (0.0-1.0)\n",
    "\n",
    "        category_pred = torch.argmax(category_probs).cpu().item()\n",
    "        category_confidence = category_probs[category_pred].cpu().item()  # As decimal (0.0-1.0)\n",
    "\n",
    "        ner_preds = torch.argmax(ner_probs, dim=-1).cpu().numpy()[0]\n",
    "        ner_confidences = torch.max(ner_probs, dim=-1)[0][0].cpu().numpy()  # Get max probability for each token\n",
    "\n",
    "        # Map to labels\n",
    "        intent_decoder = {v: k for k, v in label_encoders[\"intent_encoder\"].items()}\n",
    "        category_decoder = {v: k for k, v in label_encoders[\"category_encoder\"].items()}\n",
    "        ner_decoder = {v: k for k, v in label_encoders[\"ner_label_encoder\"].items()}\n",
    "\n",
    "        intent_label = intent_decoder[intent_pred]\n",
    "        category_label = category_decoder[category_pred]\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0].tolist())\n",
    "        seq_len = int(inputs[\"attention_mask\"][0].sum().item())\n",
    "        ner_labels = [ner_decoder[pred] for pred in ner_preds[:seq_len]]\n",
    "\n",
    "        # Extract entities from NER labels with confidence\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        entity_tokens = []\n",
    "        entity_confidences = []\n",
    "        entity_type = None\n",
    "\n",
    "        for i, (token, label, confidence) in enumerate(zip(tokens[:seq_len], ner_labels, ner_confidences[:seq_len])):\n",
    "            if label.startswith(\"B-\"):\n",
    "                # If we were tracking an entity, save it before starting a new one\n",
    "                if current_entity is not None:\n",
    "                    entity_text = tokenizer.convert_tokens_to_string(entity_tokens).strip()\n",
    "                    if entity_text:\n",
    "                        # Calculate average confidence for the entity (as decimal)\n",
    "                        avg_confidence = sum(entity_confidences) / len(entity_confidences)\n",
    "                        entities.append({\n",
    "                            \"entity\": entity_text,\n",
    "                            \"label\": entity_type,\n",
    "                            \"confidence\": avg_confidence\n",
    "                        })\n",
    "\n",
    "                # Start a new entity\n",
    "                current_entity = label[2:]\n",
    "                entity_type = label[2:]\n",
    "                entity_tokens = [token]\n",
    "                entity_confidences = [confidence]\n",
    "\n",
    "            elif label.startswith(\"I-\") and current_entity == label[2:]:\n",
    "                # Continue current entity\n",
    "                entity_tokens.append(token)\n",
    "                entity_confidences.append(confidence)\n",
    "\n",
    "            elif current_entity is not None:\n",
    "                # End of an entity\n",
    "                entity_text = tokenizer.convert_tokens_to_string(entity_tokens).strip()\n",
    "                if entity_text:\n",
    "                    # Calculate average confidence for the entity (as decimal)\n",
    "                    avg_confidence = sum(entity_confidences) / len(entity_confidences)\n",
    "                    entities.append({\n",
    "                        \"entity\": entity_text,\n",
    "                        \"label\": entity_type,\n",
    "                        \"confidence\": avg_confidence\n",
    "                    })\n",
    "                current_entity = None\n",
    "                entity_tokens = []\n",
    "                entity_confidences = []\n",
    "                entity_type = None\n",
    "\n",
    "        # Check for unfinished entity\n",
    "        if current_entity is not None:\n",
    "            entity_text = tokenizer.convert_tokens_to_string(entity_tokens).strip()\n",
    "            if entity_text:\n",
    "                # Calculate average confidence for the entity (as decimal)\n",
    "                avg_confidence = sum(entity_confidences) / len(entity_confidences)\n",
    "                entities.append({\n",
    "                    \"entity\": entity_text,\n",
    "                    \"label\": entity_type,\n",
    "                    \"confidence\": avg_confidence\n",
    "                })\n",
    "\n",
    "        return {\n",
    "            \"intent\": {\"label\": intent_label, \"confidence\": intent_confidence},\n",
    "            \"category\": {\"label\": category_label, \"confidence\": category_confidence},\n",
    "            \"ner\": entities\n",
    "        }\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, instruction, classification_result=None, max_length=512, device=\"cuda\"):\n",
    "    \"\"\"Generate a response using GPT-2 model based on instruction and classification results\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a prompt for generating response\n",
    "    if classification_result:\n",
    "        intent = classification_result[\"intent\"][\"label\"]\n",
    "        category = classification_result[\"category\"][\"label\"]\n",
    "        entities_text = \", \".join([f\"{entity['entity']} ({entity['label']})\" for entity in classification_result[\"ner\"]]) if classification_result[\"ner\"] else \"none\"\n",
    "        input_text = f\"[INST] User query: {instruction}\\nIntent: {intent}\\nCategory: {category}\\nEntities: {entities_text}\\n\\nProvide a helpful customer service response: [RESP]\"\n",
    "    else:\n",
    "        input_text = f\"[INST] User query: {instruction}\\n\\nProvide a helpful customer service response: [RESP]\"\n",
    "        \n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "    try:\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                num_beams=5,\n",
    "                no_repeat_ngram_size=2,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "\n",
    "        # Extract response part\n",
    "        if \"[RESP]\" in generated_text:\n",
    "            response = generated_text.split(\"[RESP]\")[1].strip()\n",
    "            # Clean up any trailing tokens\n",
    "            if \"[EOS]\" in response:\n",
    "                response = response.split(\"[EOS]\")[0].strip()\n",
    "        else:\n",
    "            # Fallback extraction\n",
    "            response = generated_text[len(input_text):].strip()\n",
    "            \n",
    "        # Format steps if present\n",
    "        steps_pattern = re.search(r'(\\d+)\\.\\s+([A-Z])', response)\n",
    "        if steps_pattern or \"step\" in response.lower() or \"follow\" in response.lower():\n",
    "            # Format steps to be on separate lines\n",
    "            for i in range(1, 10):\n",
    "                step_marker = f\"{i}. \"\n",
    "                if step_marker in response and f\"\\n{i}. \" not in response:\n",
    "                    response = response.replace(step_marker, f\"\\n{i}. \")\n",
    "\n",
    "            # Clean up any excess newlines\n",
    "            response = re.sub(r'\\n\\s*\\n', '\\n\\n', response)\n",
    "            response = response.lstrip('\\n')\n",
    "\n",
    "        # Clean any technical artifacts\n",
    "        response = re.sub(r'https?://\\S+', '', response)  # Remove URLs\n",
    "        response = re.sub(r'<[^>]*>', '', response)  # Remove HTML tags\n",
    "        response = re.sub(r'\\{\\s*\"[^\"]*\":', '', response)  # Remove JSON-like content\n",
    "        response = re.sub(r'\\s+', ' ', response).strip()  # Clean up whitespace\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_response: {e}\")\n",
    "        return f\"I apologize, but I couldn't generate a response. Error: {str(e)}\"\n",
    "\n",
    "# Memory measurement functions\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / 1024 / 1024  # Convert bytes to MB\n",
    "\n",
    "def get_peak_memory_usage(func, *args, **kwargs):\n",
    "    device_param = kwargs.pop('device', None)  # Remove device parameter before calling func\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    start_mem = get_memory_usage()\n",
    "    result = func(*args, **kwargs)  # Call without device parameter\n",
    "    end_mem = get_memory_usage()\n",
    "    \n",
    "    peak_gpu_mem = 0\n",
    "    if torch.cuda.is_available() and device_param == \"cuda\":\n",
    "        peak_gpu_mem = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB\n",
    "        return result, peak_gpu_mem\n",
    "    else:\n",
    "        return result, max(0, end_mem - start_mem)\n",
    "    \n",
    "class BaselinePerformanceTest:\n",
    "    def __init__(self, model_paths, test_data_path, num_samples=50):\n",
    "        self.output_dir = model_paths[\"baseline_model_dir\"]\n",
    "        self.generation_model_path = model_paths[\"generation_model_path\"]\n",
    "        self.generation_tokenizer_path = model_paths[\"generation_tokenizer_path\"]\n",
    "        self.test_data_path = test_data_path\n",
    "        self.num_samples = num_samples\n",
    "        self.devices = [\"cpu\", \"cuda\"] if torch.cuda.is_available() else [\"cpu\"]\n",
    "        self.results = {device: [] for device in self.devices}\n",
    "        self.summary = {device: {\"classification_time\": [], \"generation_time\": [], \"overall_time\": [], \"memory_usage\": []} for device in self.devices}\n",
    "    \n",
    "    def load_models(self, device):\n",
    "        print(f\"\\nLoading models on {device.upper()}...\")\n",
    "        \n",
    "        # Load baseline model for classification\n",
    "        encoders_path = os.path.join(self.output_dir, \"label_encoders.json\")\n",
    "        hyperparams_path = os.path.join(self.output_dir, \"hyperparameters.json\")\n",
    "        model_path = os.path.join(self.output_dir, \"baseline_model.pth\")\n",
    "        \n",
    "        print(f\"Loading from: {encoders_path}\")\n",
    "        with open(encoders_path, 'r', encoding='utf-8') as f:\n",
    "            self.label_encoders = json.load(f)\n",
    "            \n",
    "        with open(hyperparams_path, 'r', encoding='utf-8') as f:\n",
    "            self.hyperparameters = json.load(f)\n",
    "                \n",
    "        self.gpt2_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        \n",
    "        if self.gpt2_tokenizer.pad_token is None:\n",
    "            self.gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                \n",
    "        self.classification_model = BaselineGPT2MultiTask(\n",
    "            num_intents=len(self.label_encoders[\"intent_encoder\"]),\n",
    "            num_categories=len(self.label_encoders[\"category_encoder\"]),\n",
    "            num_ner_labels=len(self.label_encoders[\"ner_label_encoder\"]),\n",
    "            dropout_rate=self.hyperparameters[\"dropout_rate\"]\n",
    "        )\n",
    "        \n",
    "        if self.gpt2_tokenizer.pad_token_id is not None:\n",
    "            self.classification_model.gpt2.resize_token_embeddings(len(self.gpt2_tokenizer))\n",
    "        \n",
    "        self.classification_model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "        self.classification_model.to(device)\n",
    "        self.classification_model.eval()\n",
    "        \n",
    "        # Load generation model using from_pretrained instead of torch.load\n",
    "        try:\n",
    "            print(f\"Attempting to load generation model from {self.generation_model_path}\")\n",
    "            self.generation_model = GPT2LMHeadModel.from_pretrained(self.generation_model_path).to(device)\n",
    "            self.generation_tokenizer = GPT2Tokenizer.from_pretrained(self.generation_tokenizer_path)\n",
    "            self.generation_tokenizer.pad_token = self.generation_tokenizer.eos_token\n",
    "            self.generation_tokenizer.add_special_tokens({'additional_special_tokens': ['[INST]', '[RESP]', '[EOS]']})\n",
    "            self.generation_model.resize_token_embeddings(len(self.generation_tokenizer))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading generation model: {e}\")\n",
    "            print(\"Falling back to default GPT2...\")\n",
    "            self.generation_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "            self.generation_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            self.generation_tokenizer.pad_token = self.generation_tokenizer.eos_token\n",
    "            self.generation_tokenizer.add_special_tokens({'additional_special_tokens': ['[INST]', '[RESP]', '[EOS]']})\n",
    "            self.generation_model.resize_token_embeddings(len(self.generation_tokenizer))\n",
    "        \n",
    "        self.generation_model.eval()\n",
    "        \n",
    "        return self.classification_model, self.generation_model, self.gpt2_tokenizer, self.generation_tokenizer\n",
    "    \n",
    "    def run_benchmark(self):\n",
    "        # Load test data\n",
    "        with open(self.test_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "            \n",
    "        # Select samples for testing\n",
    "        if len(test_data) > self.num_samples:\n",
    "            test_samples = test_data[:self.num_samples]\n",
    "        else:\n",
    "            test_samples = test_data\n",
    "            \n",
    "        print(f\"Running performance test on {len(test_samples)} samples\")\n",
    "        \n",
    "        # Run tests on each device\n",
    "        for device in self.devices:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"RUNNING BENCHMARK ON {device.upper()}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            classification_model, generation_model, gpt2_tokenizer, generation_tokenizer = self.load_models(device)\n",
    "            \n",
    "            for idx, sample in enumerate(tqdm(test_samples, desc=f\"Testing on {device.upper()}\")):\n",
    "                instruction = sample[\"instruction\"]\n",
    "                \n",
    "                # Measure classification time and memory\n",
    "                def run_classification():\n",
    "                    return inference_baseline(\n",
    "                        classification_model, \n",
    "                        instruction, \n",
    "                        gpt2_tokenizer, \n",
    "                        self.label_encoders, \n",
    "                        self.hyperparameters[\"max_length\"],\n",
    "                        device\n",
    "                    )\n",
    "                \n",
    "                classification_start = time.time()\n",
    "                classification_result, classification_memory = get_peak_memory_usage(\n",
    "                    run_classification, device=device\n",
    "                )\n",
    "                classification_end = time.time()\n",
    "                classification_time = classification_end - classification_start\n",
    "                \n",
    "                # Create new input with classification results\n",
    "                intent = classification_result[\"intent\"][\"label\"]\n",
    "                category = classification_result[\"category\"][\"label\"]\n",
    "                entities_text = \", \".join([f\"{entity['entity']} ({entity['label']})\" for entity in classification_result[\"ner\"]]) if classification_result[\"ner\"] else \"none\"\n",
    "                new_input = f\"{instruction} [Classified: Intent is '{intent}', Category is '{category}', Entities are {entities_text}]\"\n",
    "                \n",
    "                # Measure generation time and memory\n",
    "                def run_generation():\n",
    "                    return generate_response(\n",
    "                        generation_model,\n",
    "                        generation_tokenizer,\n",
    "                        instruction,\n",
    "                        classification_result,\n",
    "                        device=device\n",
    "                    )\n",
    "                \n",
    "                generation_start = time.time()\n",
    "                generated_response, generation_memory = get_peak_memory_usage(\n",
    "                    run_generation, device=device\n",
    "                )\n",
    "                generation_end = time.time()\n",
    "                generation_time = generation_end - generation_start\n",
    "                \n",
    "                # Calculate overall time and memory\n",
    "                overall_time = classification_time + generation_time\n",
    "                overall_memory = classification_memory + generation_memory\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    \"Model\": \"GPT2-Baseline Pipeline\",\n",
    "                    \"Classification Time (s)\": classification_time,\n",
    "                    \"Text Generation Time (s)\": generation_time,\n",
    "                    \"Overall Inference Time (s)\": overall_time,\n",
    "                    \"Device\": \"GPU\" if device == \"cuda\" else \"CPU\",\n",
    "                    \"Memory Usage (MB)\": overall_memory,\n",
    "                    \"Instruction\": instruction,\n",
    "                    \"New Input\": new_input,\n",
    "                    \"Generated Response\": generated_response\n",
    "                }\n",
    "                self.results[device].append(result)\n",
    "                \n",
    "                # Update summary statistics\n",
    "                self.summary[device][\"classification_time\"].append(classification_time)\n",
    "                self.summary[device][\"generation_time\"].append(generation_time)\n",
    "                self.summary[device][\"overall_time\"].append(overall_time)\n",
    "                self.summary[device][\"memory_usage\"].append(overall_memory)\n",
    "                \n",
    "                # Print sample result\n",
    "                print(f\"\\nSample {idx+1}/{len(test_samples)} (Device: {device.upper()}):\")\n",
    "                print(f\"Instruction: {instruction}\")\n",
    "                print(f\"New Input: {new_input}\")\n",
    "                print(f\"Generated Response: {generated_response}\")\n",
    "                print(f\"Model                | Classification Time (s)   | Text Generation Time (s)  | Overall Inference Time (s) | Device     | Memory Usage (MB)\")\n",
    "                print(f\"GPT2-Baseline Pipeline| {classification_time:.4f}                    | {generation_time:.4f}                   | {overall_time:.4f}                   | {device.upper()}        | {overall_memory:.4f}\")\n",
    "            \n",
    "            # Calculate average times\n",
    "            self.summary[device][\"avg_classification_time\"] = sum(self.summary[device][\"classification_time\"]) / len(test_samples)\n",
    "            self.summary[device][\"avg_generation_time\"] = sum(self.summary[device][\"generation_time\"]) / len(test_samples)\n",
    "            self.summary[device][\"avg_overall_time\"] = sum(self.summary[device][\"overall_time\"]) / len(test_samples)\n",
    "            self.summary[device][\"avg_memory_usage\"] = sum(self.summary[device][\"memory_usage\"]) / len(test_samples)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"BENCHMARK SUMMARY FOR {device.upper()}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Average Classification Time: {self.summary[device]['avg_classification_time']:.4f} seconds\")\n",
    "            print(f\"Average Generation Time: {self.summary[device]['avg_generation_time']:.4f} seconds\")\n",
    "            print(f\"Average Overall Time: {self.summary[device]['avg_overall_time']:.4f} seconds\")\n",
    "            print(f\"Average Memory Usage: {self.summary[device]['avg_memory_usage']:.4f} MB\")\n",
    "\n",
    "        # Save results after all devices are benchmarked\n",
    "        self.save_results()\n",
    "        \n",
    "    def save_results(self):\n",
    "        \"\"\"Save benchmark results to JSON files in the specified format\"\"\"\n",
    "        \n",
    "        # Save CPU results\n",
    "        if \"cpu\" in self.devices and self.results[\"cpu\"]:\n",
    "            with open(\"performance_results_baseline_cpu.json\", \"w\") as f:\n",
    "                json.dump(self.results[\"cpu\"], f, indent=4)\n",
    "            print(\"CPU results saved to performance_results_baseline_cpu.json\")\n",
    "        \n",
    "        # Save GPU results\n",
    "        if \"cuda\" in self.devices and self.results[\"cuda\"]:\n",
    "            with open(\"performance_results_baseline_gpu.json\", \"w\") as f:\n",
    "                json.dump(self.results[\"cuda\"], f, indent=4)\n",
    "            print(\"GPU results saved to performance_results_baseline_gpu.json\")\n",
    "        \n",
    "        # Create performance log and comparison plots\n",
    "        self.create_performance_log()\n",
    "        if \"cpu\" in self.devices and \"cuda\" in self.devices:\n",
    "            self.create_comparison_plots()\n",
    "\n",
    "    def create_performance_log(self):\n",
    "        \"\"\"Create a text file with performance comparisons\"\"\"\n",
    "        with open(\"baseline_performance_log.txt\", \"w\") as f:\n",
    "            f.write(\"=== BASELINE PIPELINE PERFORMANCE BENCHMARK RESULTS ===\\n\\n\")\n",
    "            \n",
    "            # Write CPU results\n",
    "            if \"cpu\" in self.devices:\n",
    "                f.write(\"CPU PERFORMANCE:\\n\")\n",
    "                f.write(f\"Average Classification Time: {self.summary['cpu']['avg_classification_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Generation Time: {self.summary['cpu']['avg_generation_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Overall Time: {self.summary['cpu']['avg_overall_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Memory Usage: {self.summary['cpu']['avg_memory_usage']:.4f} MB\\n\\n\")\n",
    "            \n",
    "            # Write GPU results\n",
    "            if \"cuda\" in self.devices:\n",
    "                f.write(\"GPU PERFORMANCE:\\n\")\n",
    "                f.write(f\"Average Classification Time: {self.summary['cuda']['avg_classification_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Generation Time: {self.summary['cuda']['avg_generation_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Overall Time: {self.summary['cuda']['avg_overall_time']:.4f} seconds\\n\")\n",
    "                f.write(f\"Average Memory Usage: {self.summary['cuda']['avg_memory_usage']:.4f} MB\\n\\n\")\n",
    "            \n",
    "            # Write comparison\n",
    "            if \"cpu\" in self.devices and \"cuda\" in self.devices:\n",
    "                f.write(\"CPU vs GPU COMPARISON:\\n\")\n",
    "                class_speedup = self.summary['cpu']['avg_classification_time'] / self.summary['cuda']['avg_classification_time']\n",
    "                gen_speedup = self.summary['cpu']['avg_generation_time'] / self.summary['cuda']['avg_generation_time']\n",
    "                overall_speedup = self.summary['cpu']['avg_overall_time'] / self.summary['cuda']['avg_overall_time']\n",
    "                \n",
    "                f.write(f\"Classification Speed Improvement: {class_speedup:.2f}x faster on GPU\\n\")\n",
    "                f.write(f\"Generation Speed Improvement: {gen_speedup:.2f}x faster on GPU\\n\")\n",
    "                f.write(f\"Overall Speed Improvement: {overall_speedup:.2f}x faster on GPU\\n\\n\")\n",
    "                \n",
    "                f.write(\"Notes:\\n\")\n",
    "                f.write(\"- Classification is typically faster than generation\\n\")\n",
    "                f.write(\"- GPU provides greater speedup for generation than classification\\n\")\n",
    "                f.write(\"- Memory usage is higher on GPU but processing is much faster\\n\")\n",
    "            \n",
    "            f.write(\"\\n=== END OF REPORT ===\\n\")\n",
    "        \n",
    "        print(\"Performance log saved to baseline_performance_log.txt\")\n",
    "\n",
    "    def create_comparison_plots(self):\n",
    "        \"\"\"Create matplotlib comparison plots\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        \n",
    "        if \"cpu\" not in self.devices or \"cuda\" not in self.devices:\n",
    "            print(\"Both CPU and GPU results are needed for comparison plots\")\n",
    "            return\n",
    "        \n",
    "        # Set up the plots\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        \n",
    "        # 1. Classification Time Comparison\n",
    "        plt.subplot(3, 1, 1)\n",
    "        \n",
    "        labels = ['Classification', 'Generation', 'Overall']\n",
    "        cpu_times = [self.summary['cpu']['avg_classification_time'], \n",
    "                    self.summary['cpu']['avg_generation_time'], \n",
    "                    self.summary['cpu']['avg_overall_time']]\n",
    "        gpu_times = [self.summary['cuda']['avg_classification_time'],\n",
    "                    self.summary['cuda']['avg_generation_time'],\n",
    "                    self.summary['cuda']['avg_overall_time']]\n",
    "        \n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, cpu_times, width, label='CPU')\n",
    "        plt.bar(x + width/2, gpu_times, width, label='GPU')\n",
    "        \n",
    "        plt.xlabel('Task')\n",
    "        plt.ylabel('Time (seconds)')\n",
    "        plt.title('Average Time Comparison')\n",
    "        plt.xticks(x, labels)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on the bars\n",
    "        for i, v in enumerate(cpu_times):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.2f}s\", ha='center')\n",
    "        \n",
    "        for i, v in enumerate(gpu_times):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.2f}s\", ha='center')\n",
    "        \n",
    "        # 2. Speedup Chart\n",
    "        plt.subplot(3, 1, 2)\n",
    "        \n",
    "        speedup_labels = ['Classification', 'Generation', 'Overall']\n",
    "        speedups = [\n",
    "            self.summary['cpu']['avg_classification_time'] / self.summary['cuda']['avg_classification_time'],\n",
    "            self.summary['cpu']['avg_generation_time'] / self.summary['cuda']['avg_generation_time'],\n",
    "            self.summary['cpu']['avg_overall_time'] / self.summary['cuda']['avg_overall_time']\n",
    "        ]\n",
    "        \n",
    "        plt.bar(speedup_labels, speedups, color='blue')\n",
    "        plt.xlabel('Task')\n",
    "        plt.ylabel('Speedup Factor (CPU time / GPU time)')\n",
    "        plt.title('GPU Speedup Compared to CPU')\n",
    "        \n",
    "        # Add value labels on the bars\n",
    "        for i, v in enumerate(speedups):\n",
    "            plt.text(i, v + 0.1, f\"{v:.2f}x\", ha='center')\n",
    "        \n",
    "        # 3. Memory Usage\n",
    "        plt.subplot(3, 1, 3)\n",
    "        \n",
    "        memory_labels = ['CPU', 'GPU']\n",
    "        memory_usage = [self.summary['cpu']['avg_memory_usage'],\n",
    "                    self.summary['cuda']['avg_memory_usage']]\n",
    "        \n",
    "        plt.bar(memory_labels, memory_usage, color='green')\n",
    "        plt.xlabel('Device')\n",
    "        plt.ylabel('Memory Usage (MB)')\n",
    "        plt.title('Average Memory Usage')\n",
    "        \n",
    "        # Add memory usage labels\n",
    "        for i, v in enumerate(memory_usage):\n",
    "            plt.text(i, v + 0.1, f\"{v:.1f} MB\", ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('baseline_performance_comparison.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Performance comparison plots saved to baseline_performance_comparison.png\")\n",
    "\n",
    "# Move the main function OUTSIDE of the class\n",
    "def main():\n",
    "    # Define paths\n",
    "    model_paths = {\n",
    "        \"baseline_model_dir\": \"../..//Baseline_freeze_v1\",\n",
    "        \"generation_model_path\": \"../../text_generation_results_03-09-25/model\",\n",
    "        \"generation_tokenizer_path\": \"../../text_generation_results_03-09-25/tokenizer\"\n",
    "    }\n",
    "    \n",
    "    test_data_path = \"../../test.json\"\n",
    "    \n",
    "    # Verify and print paths\n",
    "    print(f\"Baseline model dir: {model_paths['baseline_model_dir']}\")\n",
    "    print(f\"Generation model path: {model_paths['generation_model_path']}\")\n",
    "    print(f\"Test data path: {test_data_path}\")\n",
    "    \n",
    "    # Number of samples to test\n",
    "    num_samples = 50  # You can adjust this number\n",
    "    \n",
    "    # Check if CUDA is available\n",
    "    device_info = \"CUDA available\" if torch.cuda.is_available() else \"CUDA not available, using CPU only\"\n",
    "    print(device_info)\n",
    "    \n",
    "    # Run the benchmark\n",
    "    benchmark = BaselinePerformanceTest(model_paths, test_data_path, num_samples)\n",
    "    benchmark.run_benchmark()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check available devices\n",
    "    device_info = \"CUDA\" if torch.cuda.is_available() else \"CPU\"\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"Running on: {device_info}\")\n",
    "    \n",
    "    # Run the benchmark\n",
    "    main()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
